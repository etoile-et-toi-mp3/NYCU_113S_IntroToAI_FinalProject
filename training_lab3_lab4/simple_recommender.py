#!/usr/bin/env python3
"""
AIé©…å‹•çš„ç©¿æ­æ¨è–¦å™¨
åŸºæ–¼å¤šç¨®è¦–è¦ºèªè¨€æ¨¡å‹é€²è¡Œæ·±åº¦åˆ†æå’Œå»ºè­°ç”Ÿæˆ
å®Œå…¨ç§»é™¤è¦å‰‡ç”Ÿæˆï¼Œæ¡ç”¨ç´”AIé©…å‹•æ–¹å¼
"""

import torch
import torch.nn.functional as F
from PIL import Image
import numpy as np
import json
import os
from sklearn.metrics.pairwise import cosine_similarity
from transformers import (
    CLIPProcessor, CLIPModel,
    Blip2Processor, Blip2ForConditionalGeneration,
    AutoProcessor, LlavaNextForConditionalGeneration,
    AutoTokenizer, AutoModelForCausalLM
)
import sys
sys.path.append('../training_lab2')
from simple_train_model import SimpleFashionRecommender, SimpleTrainingConfig
import warnings
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

warnings.filterwarnings("ignore")

class MultiModelAIRecommendationSystem:
    def __init__(self, model_path="simple_fashion_model_final_best.pth", 
                 labels_file="simple_dataset_labels.json"):
        """
        åˆå§‹åŒ–å¤šæ¨¡å‹AIæ¨è–¦ç³»çµ±
        
        Args:
            model_path: ç°¡åŒ–ç‰ˆæ¨¡å‹è·¯å¾‘
            labels_file: ç°¡åŒ–ç‰ˆæ¨™ç±¤æ–‡ä»¶
        """
        print("ğŸš€ åˆå§‹åŒ–AIé©…å‹•æ¨è–¦ç³»çµ±...")
        
        # è¨­ç½®è¨­å‚™
        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        print(f"ğŸ¯ ä½¿ç”¨è¨­å‚™: {self.device}")
        
        # è¼‰å…¥é…ç½®
        self.config = SimpleTrainingConfig()
        
        # è¼‰å…¥æ¨¡å‹
        print("ğŸ”§ è¼‰å…¥ç°¡åŒ–ç‰ˆæ¨¡å‹...")
        self.model = SimpleFashionRecommender(self.config).to(self.device)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰è¨“ç·´å¥½çš„æ¨¡å‹
        self.use_trained_model = False
        if os.path.exists(model_path):
            try:
                checkpoint = torch.load(model_path, map_location=self.device)
                self.model.load_state_dict(checkpoint['model_state_dict'])
                self.use_trained_model = True
                print(f"âœ… è¨“ç·´æ¨¡å‹è¼‰å…¥æˆåŠŸ: {model_path}")
            except Exception as e:
                print(f"âš ï¸ è¼‰å…¥è¨“ç·´æ¨¡å‹å¤±æ•—: {e}")
                print("å°‡ä½¿ç”¨é è¨“ç·´çš„ FashionCLIP ç‰¹å¾µ")
        else:
            print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            print("å°‡ä½¿ç”¨é è¨“ç·´çš„ FashionCLIP ç‰¹å¾µ")
        
        self.model.eval()
        
        # è¼‰å…¥æ•¸æ“šé›†æ¨™ç±¤
        print("ğŸ“Š è¼‰å…¥æ•¸æ“šé›†...")
        if os.path.exists(labels_file):
            with open(labels_file, 'r') as f:
                self.dataset = json.load(f)
            print(f"âœ… è¼‰å…¥ {len(self.dataset)} å€‹æ¨£æœ¬")
        else:
            print(f"âŒ æ¨™ç±¤æ–‡ä»¶ä¸å­˜åœ¨: {labels_file}")
            self.dataset = []
        
        # é è™•ç†æ•¸æ“šé›†ç‰¹å¾µ
        if self.dataset:
            self._preprocess_dataset_features()
        
        # åˆå§‹åŒ–AIæ¨¡å‹ç®¡ç†å™¨
        self.ai_models = {}
        self._init_ai_models()
        
        print("âœ… AIé©…å‹•æ¨è–¦ç³»çµ±åˆå§‹åŒ–å®Œæˆï¼")
    
    def _preprocess_dataset_features(self):
        """é è™•ç†æ•¸æ“šé›†ç‰¹å¾µçŸ©é™£"""
        print("ğŸ”„ é è™•ç†æ•¸æ“šé›†ç‰¹å¾µ...")
        
        # æå–æ‰€æœ‰ç‰¹å¾µå‘é‡
        features_list = []
        for sample in self.dataset:
            features_list.append(sample['features'])
        
        # è½‰æ›ç‚ºnumpyçŸ©é™£ä¸¦æ¨™æº–åŒ–
        self.dataset_features = np.array(features_list)
        self.dataset_features = self.dataset_features / np.linalg.norm(
            self.dataset_features, axis=1, keepdims=True
        )
        
        print(f"ğŸ“Š æ•¸æ“šé›†ç‰¹å¾µçŸ©é™£: {self.dataset_features.shape}")
        
        # å¦‚æœä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹ï¼Œéœ€è¦å°‡æ•¸æ“šé›†ç‰¹å¾µä¹Ÿé€šéæ˜ å°„å±¤
        if self.use_trained_model:
            print("ğŸ”„ å°‡æ•¸æ“šé›†ç‰¹å¾µé€šéè¨“ç·´å¥½çš„æ˜ å°„å±¤...")
            with torch.no_grad():
                dataset_features_tensor = torch.tensor(self.dataset_features, dtype=torch.float32).to(self.device)
                
                # åˆ†æ‰¹è™•ç†ä»¥é¿å…è¨˜æ†¶é«”å•é¡Œ
                batch_size = 100
                mapped_features = []
                
                for i in range(0, len(dataset_features_tensor), batch_size):
                    batch = dataset_features_tensor[i:i+batch_size]
                    outputs = self.model.forward(batch)
                    mapped_batch = outputs['fashion_embedding'].cpu().numpy()
                    mapped_features.append(mapped_batch)
                
                # åˆä½µæ‰€æœ‰æ‰¹æ¬¡
                self.dataset_features = np.vstack(mapped_features)
                # é‡æ–°æ¨™æº–åŒ–
                self.dataset_features = self.dataset_features / np.linalg.norm(
                    self.dataset_features, axis=1, keepdims=True
                )
                
                print(f"ğŸ“Š æ˜ å°„å¾Œæ•¸æ“šé›†ç‰¹å¾µçŸ©é™£: {self.dataset_features.shape}")
    
    def _init_ai_models(self):
        """åˆå§‹åŒ–å¤šå€‹AIæ¨¡å‹ï¼ˆå»¶é²è¼‰å…¥ï¼‰"""
        print("ğŸ§  æº–å‚™AIæ¨¡å‹ç®¡ç†å™¨...")
        
        # æ¨¡å‹é…ç½®
        self.model_configs = {
            'blip2': {
                'name': 'BLIP-2',
                'model_id': 'Salesforce/blip2-opt-2.7b',
                'loaded': False,
                'processor': None,
                'model': None
            },
            'llava': {
                'name': 'LLaVA-Next',
                'model_id': 'llava-hf/llava-v1.6-mistral-7b-hf',
                'loaded': False,
                'processor': None,
                'model': None
            },
            'instructblip': {
                'name': 'InstructBLIP',
                'model_id': 'Salesforce/instructblip-vicuna-7b',
                'loaded': False,
                'processor': None,
                'model': None
            }
        }
        
        print("âœ… AIæ¨¡å‹ç®¡ç†å™¨æº–å‚™å®Œæˆ")
    
    def _load_model(self, model_key):
        """å‹•æ…‹è¼‰å…¥æŒ‡å®šæ¨¡å‹"""
        if model_key not in self.model_configs:
            print(f"âŒ æœªçŸ¥æ¨¡å‹: {model_key}")
            return False
        
        config = self.model_configs[model_key]
        if config['loaded']:
            return True
        
        try:
            print(f"ğŸ”„ è¼‰å…¥ {config['name']} æ¨¡å‹...")
            
            if model_key == 'blip2':
                config['processor'] = Blip2Processor.from_pretrained(config['model_id'])
                config['model'] = Blip2ForConditionalGeneration.from_pretrained(
                    config['model_id'],
                    torch_dtype=torch.float16 if self.device.type == 'mps' else torch.float32,
                    device_map="auto" if self.device.type != 'mps' else None
                )
                if self.device.type == 'mps':
                    config['model'] = config['model'].to(self.device)
            
            elif model_key == 'llava':
                config['processor'] = AutoProcessor.from_pretrained(config['model_id'])
                config['model'] = LlavaNextForConditionalGeneration.from_pretrained(
                    config['model_id'],
                    torch_dtype=torch.float16 if self.device.type == 'mps' else torch.float32,
                    low_cpu_mem_usage=True,
                    device_map="auto" if self.device.type != 'mps' else None
                )
                if self.device.type == 'mps':
                    config['model'] = config['model'].to(self.device)
            
            elif model_key == 'instructblip':
                from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration
                config['processor'] = InstructBlipProcessor.from_pretrained(config['model_id'])
                config['model'] = InstructBlipForConditionalGeneration.from_pretrained(
                    config['model_id'],
                    torch_dtype=torch.float16 if self.device.type == 'mps' else torch.float32,
                    device_map="auto" if self.device.type != 'mps' else None
                )
                if self.device.type == 'mps':
                    config['model'] = config['model'].to(self.device)
            
            config['loaded'] = True
            print(f"âœ… {config['name']} è¼‰å…¥æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ {config['name']} è¼‰å…¥å¤±æ•—: {e}")
            return False
    
    def _cleanup_model(self, model_key):
        """æ¸…ç†æŒ‡å®šæ¨¡å‹é‡‹æ”¾è¨˜æ†¶é«”"""
        if model_key not in self.model_configs:
            return
        
        config = self.model_configs[model_key]
        if config['loaded']:
            del config['model']
            del config['processor']
            config['model'] = None
            config['processor'] = None
            config['loaded'] = False
            
            if self.device.type == 'mps':
                torch.mps.empty_cache()
            elif self.device.type == 'cuda':
                torch.cuda.empty_cache()
    
    def extract_image_features(self, image_path):
        """
        æå–åœ–ç‰‡çš„FashionCLIPç‰¹å¾µ
        
        Args:
            image_path: åœ–ç‰‡è·¯å¾‘
            
        Returns:
            np.array: æ¨™æº–åŒ–çš„ç‰¹å¾µå‘é‡
        """
        try:
            # è¼‰å…¥åœ–ç‰‡
            image = Image.open(image_path).convert('RGB')
            
            # æå– FashionCLIP ç‰¹å¾µ
            with torch.no_grad():
                clip_inputs = self.model.clip_processor(images=image, return_tensors="pt").to(self.device)
                clip_features = self.model.clip_model.get_image_features(**clip_inputs)
                clip_features = F.normalize(clip_features, p=2, dim=1)
                
                # æ ¹æ“šæ˜¯å¦ä½¿ç”¨è¨“ç·´æ¨¡å‹æ±ºå®šæ˜¯å¦é€šéæ˜ å°„å±¤
                if self.use_trained_model:
                    # é€šéè¨“ç·´å¥½çš„æ˜ å°„å±¤
                    outputs = self.model.forward(clip_features)
                    fashion_embedding = outputs['fashion_embedding'].cpu().numpy()
                else:
                    # ç›´æ¥ä½¿ç”¨åŸå§‹ FashionCLIP ç‰¹å¾µ
                    fashion_embedding = clip_features.cpu().numpy()
                
                # æ¨™æº–åŒ–
                fashion_embedding = fashion_embedding / np.linalg.norm(fashion_embedding)
                
                return fashion_embedding.flatten()
                
        except Exception as e:
            print(f"âŒ ç‰¹å¾µæå–å¤±æ•—: {e}")
            return None
    
    def find_similar_outfits(self, image_path, gender, top_k=5, style_preference=None):
        """
        æ‰¾å‡ºæœ€ç›¸ä¼¼çš„ç©¿æ­
        
        Args:
            image_path: ç”¨æˆ¶åœ–ç‰‡è·¯å¾‘
            gender: æ€§åˆ¥éæ¿¾
            top_k: è¿”å›å‰kå€‹çµæœ
            style_preference: é¢¨æ ¼åå¥½
            
        Returns:
            list: ç›¸ä¼¼ç©¿æ­åˆ—è¡¨
        """
        print(f"ğŸ” åˆ†æåœ–ç‰‡: {image_path}")
        
        # æå–æŸ¥è©¢åœ–ç‰‡ç‰¹å¾µ
        query_features = self.extract_image_features(image_path)
        if query_features is None:
            return []
        
        # éæ¿¾æ•¸æ“šé›†
        filtered_indices = []
        for i, sample in enumerate(self.dataset):
            # æ€§åˆ¥éæ¿¾
            if sample['gender'] != gender:
                continue
            
            # é¢¨æ ¼éæ¿¾ï¼ˆå¯é¸ï¼‰
            if style_preference and sample['style'] != style_preference:
                continue
                
            filtered_indices.append(i)
        
        if not filtered_indices:
            print("âŒ æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„æ¨£æœ¬")
            return []
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        filtered_features = self.dataset_features[filtered_indices]
        similarities = cosine_similarity([query_features], filtered_features)[0]
        
        # æ’åºä¸¦é¸æ“‡å‰kå€‹
        sorted_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in sorted_indices:
            original_idx = filtered_indices[idx]
            sample = self.dataset[original_idx]
            
            results.append({
                'path': sample['path'],
                'style': sample['style'],
                'gender': sample['gender'],
                'similarity': float(similarities[idx]),
                'score': float(similarities[idx] * 10),  # è½‰æ›ç‚º0-10åˆ†
                'features': sample['features']
            })
        
        print(f"âœ… æ‰¾åˆ° {len(results)} å€‹ç›¸ä¼¼ç©¿æ­")
        return results
    
    def extract_detailed_features(self, image_path):
        """
        æå–è©³ç´°çš„åœ–ç‰‡ç‰¹å¾µç”¨æ–¼æ¯”è¼ƒåˆ†æ
        
        Args:
            image_path: åœ–ç‰‡è·¯å¾‘
            
        Returns:
            dict: åŒ…å«å¤šç¨®ç‰¹å¾µçš„å­—å…¸
        """
        try:
            image = Image.open(image_path).convert('RGB')
            
            features = {
                'clip_features': None,
                'image': image,
                'path': image_path
            }
            
            # æå–CLIPç‰¹å¾µ
            with torch.no_grad():
                clip_inputs = self.model.clip_processor(images=image, return_tensors="pt").to(self.device)
                clip_features = self.model.clip_model.get_image_features(**clip_inputs)
                clip_features = F.normalize(clip_features, p=2, dim=1)
                features['clip_features'] = clip_features.cpu().numpy().flatten()
            
            return features
            
        except Exception as e:
            print(f"âŒ è©³ç´°ç‰¹å¾µæå–å¤±æ•—: {e}")
            return None
    
    def analyze_feature_differences(self, user_features, target_features):
        """
        åˆ†æç”¨æˆ¶åœ–ç‰‡èˆ‡ç›®æ¨™åœ–ç‰‡çš„ç‰¹å¾µå·®ç•°
        
        Args:
            user_features: ç”¨æˆ¶åœ–ç‰‡ç‰¹å¾µ
            target_features: ç›®æ¨™åœ–ç‰‡ç‰¹å¾µ
            
        Returns:
            dict: ç‰¹å¾µå·®ç•°åˆ†æçµæœ
        """
        try:
            user_clip = user_features['clip_features']
            target_clip = target_features['clip_features']
            
            # è¨ˆç®—æ•´é«”ç›¸ä¼¼åº¦
            overall_similarity = cosine_similarity([user_clip], [target_clip])[0][0]
            
            # è¨ˆç®—ç‰¹å¾µå‘é‡å·®ç•°
            feature_diff = target_clip - user_clip
            diff_magnitude = np.linalg.norm(feature_diff)
            
            # æ‰¾å‡ºå·®ç•°æœ€å¤§çš„ç¶­åº¦
            diff_indices = np.argsort(np.abs(feature_diff))[-20:][::-1]  # å‰20å€‹å·®ç•°æœ€å¤§çš„ç¶­åº¦
            
            analysis = {
                'overall_similarity': float(overall_similarity),
                'difference_magnitude': float(diff_magnitude),
                'key_differences': diff_indices.tolist(),
                'feature_diff_vector': feature_diff.tolist()
            }
            
            return analysis
            
        except Exception as e:
            print(f"âŒ ç‰¹å¾µå·®ç•°åˆ†æå¤±æ•—: {e}")
            return {}
    
    def generate_ai_advice_parallel(self, user_image_path, target_outfits, models=['blip2', 'llava']):
        """
        ä½¿ç”¨å¤šå€‹AIæ¨¡å‹ä¸¦è¡Œç”Ÿæˆå»ºè­°
        
        Args:
            user_image_path: ç”¨æˆ¶åœ–ç‰‡è·¯å¾‘
            target_outfits: ç›®æ¨™ç©¿æ­åˆ—è¡¨
            models: è¦ä½¿ç”¨çš„æ¨¡å‹åˆ—è¡¨
            
        Returns:
            dict: å¤šæ¨¡å‹ç”Ÿæˆçµæœ
        """
        print(f"ğŸ§  é–‹å§‹å¤šæ¨¡å‹AIåˆ†æ...")
        print(f"ğŸ¯ ä½¿ç”¨æ¨¡å‹: {[self.model_configs[m]['name'] for m in models if m in self.model_configs]}")
        
        results = {}
        
        # æå–ç”¨æˆ¶åœ–ç‰‡ç‰¹å¾µ
        user_features = self.extract_detailed_features(user_image_path)
        if not user_features:
            return {"error": "ç„¡æ³•æå–ç”¨æˆ¶åœ–ç‰‡ç‰¹å¾µ"}
        
        # å°æ¯å€‹ç›®æ¨™ç©¿æ­é€²è¡Œåˆ†æ
        for i, target_outfit in enumerate(target_outfits[:3]):  # åªåˆ†æå‰3å€‹æœ€ç›¸ä¼¼çš„
            print(f"\nğŸ“¸ åˆ†æç›®æ¨™ç©¿æ­ {i+1}: {target_outfit['style']}")
            
            # æå–ç›®æ¨™åœ–ç‰‡ç‰¹å¾µ
            target_features = self.extract_detailed_features(target_outfit['path'])
            if not target_features:
                continue
            
            # åˆ†æç‰¹å¾µå·®ç•°
            feature_analysis = self.analyze_feature_differences(user_features, target_features)
            
            # ç‚ºæ¯å€‹æ¨¡å‹ç”Ÿæˆå»ºè­°
            outfit_results = {
                'target_info': target_outfit,
                'feature_analysis': feature_analysis,
                'ai_suggestions': {}
            }
            
            # ä¸¦è¡Œè™•ç†å¤šå€‹æ¨¡å‹
            with ThreadPoolExecutor(max_workers=2) as executor:
                future_to_model = {}
                
                for model_key in models:
                    if model_key in self.model_configs:
                        future = executor.submit(
                            self._generate_single_model_advice,
                            model_key,
                            user_image_path,
                            target_outfit['path'],
                            feature_analysis
                        )
                        future_to_model[future] = model_key
                
                # æ”¶é›†çµæœ
                for future in as_completed(future_to_model):
                    model_key = future_to_model[future]
                    try:
                        advice = future.result(timeout=60)  # 60ç§’è¶…æ™‚
                        outfit_results['ai_suggestions'][model_key] = advice
                    except Exception as e:
                        print(f"âŒ {self.model_configs[model_key]['name']} ç”Ÿæˆå¤±æ•—: {e}")
                        outfit_results['ai_suggestions'][model_key] = {
                            'error': str(e),
                            'advice': 'ç”Ÿæˆå¤±æ•—ï¼Œè«‹é‡è©¦'
                        }
            
            results[f'target_{i+1}'] = outfit_results
        
        print("âœ… å¤šæ¨¡å‹AIåˆ†æå®Œæˆ")
        return results
    
    def _generate_single_model_advice(self, model_key, user_image_path, target_image_path, feature_analysis):
        """
        ä½¿ç”¨å–®å€‹æ¨¡å‹ç”Ÿæˆå»ºè­°
        
        Args:
            model_key: æ¨¡å‹éµ
            user_image_path: ç”¨æˆ¶åœ–ç‰‡è·¯å¾‘
            target_image_path: ç›®æ¨™åœ–ç‰‡è·¯å¾‘
            feature_analysis: ç‰¹å¾µåˆ†æçµæœ
            
        Returns:
            dict: å–®å€‹æ¨¡å‹çš„å»ºè­°çµæœ
        """
        try:
            # è¼‰å…¥æ¨¡å‹
            if not self._load_model(model_key):
                return {'error': 'æ¨¡å‹è¼‰å…¥å¤±æ•—'}
            
            config = self.model_configs[model_key]
            
            # è¼‰å…¥åœ–ç‰‡
            user_image = Image.open(user_image_path).convert('RGB')
            target_image = Image.open(target_image_path).convert('RGB')
            
            # æ ¹æ“šæ¨¡å‹é¡å‹ç”Ÿæˆå»ºè­°
            if model_key == 'blip2':
                advice = self._generate_blip2_advice(
                    config, user_image, target_image, feature_analysis
                )
            elif model_key == 'llava':
                advice = self._generate_llava_advice(
                    config, user_image, target_image, feature_analysis
                )
            elif model_key == 'instructblip':
                advice = self._generate_instructblip_advice(
                    config, user_image, target_image, feature_analysis
                )
            else:
                advice = {'error': 'æœªæ”¯æ´çš„æ¨¡å‹é¡å‹'}
            
            # æ¸…ç†æ¨¡å‹ï¼ˆå¯é¸ï¼Œç¯€çœè¨˜æ†¶é«”ï¼‰
            # self._cleanup_model(model_key)
            
            return advice
            
        except Exception as e:
            return {'error': f'ç”Ÿæˆå¤±æ•—: {str(e)}'}
    
    def _generate_blip2_advice(self, config, user_image, target_image, feature_analysis):
        """ä½¿ç”¨BLIP-2ç”Ÿæˆå»ºè­°"""
        try:
            similarity = feature_analysis.get('overall_similarity', 0)
            
            # æ§‹å»ºæ™ºèƒ½æç¤º
            if similarity > 0.8:
                prompt = "Describe this stylish outfit and suggest how to achieve this look:"
            elif similarity > 0.6:
                prompt = "What styling improvements would you suggest for this outfit?"
            else:
                prompt = "Analyze this fashion style and give specific advice:"
            
            inputs = config['processor'](
                images=target_image,
                text=prompt,
                return_tensors="pt"
            ).to(self.device)
            
            with torch.no_grad():
                outputs = config['model'].generate(
                    **inputs,
                    max_new_tokens=50,
                    num_beams=2,
                    temperature=0.6,
                    do_sample=True,
                    early_stopping=True,
                    repetition_penalty=1.2,
                    no_repeat_ngram_size=3
                )
            
            advice_text = config['processor'].decode(outputs[0], skip_special_tokens=True)
            
            # æ¸…ç†è¼¸å‡º
            advice_text = self._clean_blip2_output(advice_text, prompt)
            
            # é©—è­‰è¼¸å‡ºè³ªé‡
            if self._is_valid_advice(advice_text):
                confidence = 'high' if len(advice_text) > 30 else 'medium'
            else:
                # å¦‚æœè¼¸å‡ºç„¡æ•ˆï¼Œä½¿ç”¨å‚™ç”¨ç­–ç•¥
                advice_text = self._generate_blip2_fallback(similarity)
                confidence = 'low'
            
            return {
                'model': 'BLIP-2',
                'advice': advice_text,
                'similarity_score': similarity,
                'confidence': confidence
            }
            
        except Exception as e:
            return {'error': f'BLIP-2ç”Ÿæˆå¤±æ•—: {str(e)}'}
    
    def _clean_blip2_output(self, output, prompt):
        """æ”¹é€²çš„BLIP-2è¼¸å‡ºæ¸…ç†"""
        try:
            # ç§»é™¤æç¤ºæ–‡å­—
            cleaned = output.replace(prompt, "").strip()
            
            # ç§»é™¤å¸¸è¦‹çš„ç„¡ç”¨å‰ç¶´å’Œå¾Œç¶´
            unwanted_patterns = [
                "The image shows", "This image depicts", "In this image", "I can see",
                "The outfit consists of", "The person is wearing", "This person is wearing",
                "Looking at this outfit", "This outfit features", "The style is",
                "Question:", "Answer:", "Caption:", "Description:"
            ]
            
            for pattern in unwanted_patterns:
                if cleaned.lower().startswith(pattern.lower()):
                    cleaned = cleaned[len(pattern):].strip()
            
            # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œäº‚ç¢¼
            import re
            # ç§»é™¤å¤šé¤˜çš„ç¬¦è™Ÿå’Œæ•¸å­—é›œè¨Š
            cleaned = re.sub(r'[^\w\s.,!?-]', '', cleaned)
            cleaned = re.sub(r'\d+\)', '', cleaned)  # ç§»é™¤ç·¨è™Ÿ
            cleaned = re.sub(r'^[-\s]*', '', cleaned)  # ç§»é™¤é–‹é ­çš„é€£å­—ç¬¦
            
            # ç§»é™¤éçŸ­æˆ–éé•·çš„å¥å­
            sentences = cleaned.split('.')
            valid_sentences = []
            for sentence in sentences:
                sentence = sentence.strip()
                if 5 < len(sentence) < 100 and not any(char in sentence for char in ['`', '"', "'"]):
                    valid_sentences.append(sentence)
            
            if valid_sentences:
                cleaned = '. '.join(valid_sentences[:2])  # åªå–å‰å…©å€‹æœ‰æ•ˆå¥å­
                if not cleaned.endswith('.'):
                    cleaned += '.'
            else:
                return ""
            
            return cleaned.strip()
            
        except Exception:
            return ""
    
    def _is_valid_advice(self, advice):
        """é©—è­‰å»ºè­°çš„æœ‰æ•ˆæ€§"""
        if not advice or len(advice) < 10:
            return False
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«äº‚ç¢¼æˆ–ç„¡æ„ç¾©å…§å®¹
        invalid_patterns = [
            r'I have been', r'resistance of', r'first time of',
            r'^-\s*I\s*', r'^\s*[-`"\']+', r'\d+\s*[-\)]+',
            r'world$', r'time$', r'^[^a-zA-Z]*$'
        ]
        
        import re
        for pattern in invalid_patterns:
            if re.search(pattern, advice, re.IGNORECASE):
                return False
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«åˆç†çš„æ™‚å°šè©å½™
        fashion_keywords = [
            'style', 'outfit', 'wear', 'fashion', 'clothing', 'dress',
            'shirt', 'pants', 'shoes', 'accessory', 'color', 'fit',
            'layer', 'casual', 'formal', 'trendy', 'classic'
        ]
        
        advice_lower = advice.lower()
        has_fashion_content = any(keyword in advice_lower for keyword in fashion_keywords)
        
        return has_fashion_content
    
    def _generate_blip2_fallback(self, similarity):
        """BLIP-2å‚™ç”¨å»ºè­°ç”Ÿæˆ"""
        if similarity > 0.8:
            return "This outfit has a great foundation. Consider adding one statement piece to elevate the look."
        elif similarity > 0.6:
            return "Try incorporating more trendy accessories or adjusting the fit for a more polished appearance."
        else:
            return "Consider experimenting with different color combinations and layering techniques to enhance your style."
    
    def _generate_llava_advice(self, config, user_image, target_image, feature_analysis):
        """ä½¿ç”¨LLaVAç”Ÿæˆå»ºè­°"""
        try:
            similarity = feature_analysis.get('overall_similarity', 0)
            
            # LLaVAä½¿ç”¨å°è©±æ ¼å¼
            if similarity > 0.8:
                conversation = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "image": target_image},
                            {"type": "text", "text": "Look at this outfit and suggest how to achieve this style. What specific clothing items and styling choices make this look work?"}
                        ]
                    }
                ]
            else:
                conversation = [
                    {
                        "role": "user", 
                        "content": [
                            {"type": "image", "image": target_image},
                            {"type": "text", "text": "Describe this outfit style and give specific advice on how to recreate this look. Focus on key clothing pieces, colors, and styling details."}
                        ]
                    }
                ]
            
            prompt = config['processor'].apply_chat_template(conversation, add_generation_prompt=True)
            inputs = config['processor'](
                text=prompt,
                images=target_image,
                return_tensors="pt"
            ).to(self.device)
            
            with torch.no_grad():
                outputs = config['model'].generate(
                    **inputs,
                    max_new_tokens=100,
                    temperature=0.6,
                    do_sample=True
                )
            
            advice_text = config['processor'].decode(outputs[0], skip_special_tokens=True)
            
            # æ¸…ç†LLaVAè¼¸å‡º
            advice_text = self._clean_llava_output(advice_text, prompt)
            
            return {
                'model': 'LLaVA-Next',
                'advice': advice_text,
                'similarity_score': similarity,
                'confidence': 'high' if len(advice_text) > 20 else 'medium'
            }
            
        except Exception as e:
            return {'error': f'LLaVAç”Ÿæˆå¤±æ•—: {str(e)}'}
    
    def _clean_llava_output(self, output, prompt):
        """æ¸…ç†LLaVAç‰¹æ®Šè¼¸å‡ºæ ¼å¼"""
        try:
            # LLaVAçš„è¼¸å‡ºå¯èƒ½åŒ…å«ç‰¹æ®Šæ¨™è¨˜
            cleaned = output.replace(prompt, "").strip()
            
            # ç§»é™¤LLaVAç‰¹æœ‰çš„æ¨™è¨˜å’ŒæŒ‡ä»¤æ ¼å¼
            llava_markers = [
                "<|im_start|>", "<|im_end|>", "assistant\n", "user\n",
                "[INST]", "[/INST]", "<s>", "</s>", "<|endoftext|>"
            ]
            for marker in llava_markers:
                cleaned = cleaned.replace(marker, "")
            
            # ç§»é™¤é‡è¤‡çš„æŒ‡ä»¤å…§å®¹
            import re
            # ç§»é™¤é–‹é ­çš„æŒ‡ä»¤é‡è¤‡
            instruction_patterns = [
                r'^.*?Look at this outfit and suggest.*?What specific clothing.*?\?\s*',
                r'^.*?Describe this outfit style.*?Focus on key clothing.*?\.\s*',
                r'^.*?\[INST\].*?\[/INST\]\s*',
                r'Look at this outfit and suggest.*?What specific clothing.*?\?\s*',
                r'Describe this outfit style.*?Focus on key clothing.*?\.\s*'
            ]
            
            for pattern in instruction_patterns:
                cleaned = re.sub(pattern, '', cleaned, flags=re.DOTALL | re.IGNORECASE)
            
            # ç§»é™¤é–‹é ­é‡è¤‡çš„å•å¥
            question_patterns = [
                r'^.*?What specific clothing items.*?\?\s*',
                r'^.*?How to achieve this style.*?\?\s*',
                r'^.*?Focus on key clothing pieces.*?\.\s*'
            ]
            
            for pattern in question_patterns:
                cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
            
            # æå–æœ‰ç”¨çš„å»ºè­°å…§å®¹
            # é¦–å…ˆå˜—è©¦æ‰¾åˆ°å®Œæ•´çš„å¥å­
            sentences = cleaned.split('.')
            useful_sentences = []
            
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) < 10:
                    continue
                    
                # è·³éé‡è¤‡çš„æŒ‡ä»¤å…§å®¹
                if any(skip_phrase in sentence.lower() for skip_phrase in [
                    'look at this outfit', 'describe this outfit', 'what specific',
                    'focus on key', 'how to achieve', 'suggest how to'
                ]):
                    continue
                
                # ä¿ç•™æœ‰ç”¨çš„æ™‚å°šå»ºè­°
                if any(keyword in sentence.lower() for keyword in [
                    'consider', 'try', 'add', 'wear', 'choose', 'opt for', 
                    'style', 'outfit', 'clothing', 'accessory', 'layer',
                    'jacket', 'shirt', 'pants', 'shoes', 'color',
                    'versatile', 'casual', 'formal', 'trendy', 'classic'
                ]):
                    # æ¸…ç†æ ¼å¼æ¨™è¨˜
                    sentence = re.sub(r'\*\*.*?\*\*', '', sentence)  # ç§»é™¤ç²—é«”æ¨™è¨˜
                    sentence = re.sub(r'^\d+\.\s*', '', sentence)   # ç§»é™¤ç·¨è™Ÿ
                    sentence = sentence.strip()
                    
                    if len(sentence) > 15:
                        useful_sentences.append(sentence)
            
            if useful_sentences:
                # åªå–å‰2å€‹æœ€æœ‰ç”¨çš„å»ºè­°
                cleaned = '. '.join(useful_sentences[:2])
                if not cleaned.endswith('.'):
                    cleaned += '.'
            else:
                # å¦‚æœæ²’æœ‰æ‰¾åˆ°å¥½çš„å¥å­ï¼Œå˜—è©¦æå–ç¬¬ä¸€å€‹æœ‰æ„ç¾©çš„æ®µè½
                paragraphs = cleaned.split('\n')
                for paragraph in paragraphs:
                    paragraph = paragraph.strip()
                    
                    # è·³éæŒ‡ä»¤å…§å®¹
                    if any(skip_phrase in paragraph.lower() for skip_phrase in [
                        'look at this outfit', 'describe this outfit', 'what specific',
                        'focus on key', 'how to achieve'
                    ]):
                        continue
                    
                    if len(paragraph) > 30 and any(keyword in paragraph.lower() for keyword in [
                        'outfit', 'style', 'wear', 'clothing', 'fashion'
                    ]):
                        # æå–ç¬¬ä¸€å¥
                        first_sentence = paragraph.split('.')[0].strip()
                        if len(first_sentence) > 20:
                            cleaned = first_sentence + '.'
                            break
                else:
                    return ""
            
            # æœ€çµ‚æ¸…ç†
            cleaned = cleaned.strip()
            
            # ç¢ºä¿ä¸åŒ…å«é‡è¤‡çš„æŒ‡ä»¤
            if any(phrase in cleaned.lower() for phrase in [
                'look at this outfit', 'what specific clothing', 'describe this outfit'
            ]):
                return ""
                
            if len(cleaned) < 25:
                return ""
                
            return cleaned
            
        except Exception:
            return ""
    
    def _generate_instructblip_advice(self, config, user_image, target_image, feature_analysis):
        """ä½¿ç”¨InstructBLIPç”Ÿæˆå»ºè­°"""
        try:
            similarity = feature_analysis.get('overall_similarity', 0)
            
            if similarity > 0.7:
                instruction = "Provide specific styling tips to refine this outfit and make it more polished."
            else:
                instruction = "Analyze this outfit style and give detailed advice on how to achieve this look."
            
            inputs = config['processor'](
                images=target_image,
                text=instruction,
                return_tensors="pt"
            ).to(self.device)
            
            with torch.no_grad():
                outputs = config['model'].generate(
                    **inputs,
                    max_new_tokens=90,
                    num_beams=3,
                    temperature=0.7,
                    do_sample=True
                )
            
            advice_text = config['processor'].decode(outputs[0], skip_special_tokens=True)
            advice_text = self._clean_model_output(advice_text, instruction)
            
            return {
                'model': 'InstructBLIP',
                'advice': advice_text,
                'similarity_score': similarity,
                'confidence': 'high' if len(advice_text) > 20 else 'medium'
            }
            
        except Exception as e:
            return {'error': f'InstructBLIPç”Ÿæˆå¤±æ•—: {str(e)}'}
    
    def _clean_model_output(self, output, prompt):
        """æ¸…ç†æ¨¡å‹è¼¸å‡º"""
        # ç§»é™¤æç¤ºæ–‡å­—
        cleaned = output.replace(prompt, "").strip()
        
        # ç§»é™¤å¸¸è¦‹çš„ç„¡ç”¨å‰ç¶´
        prefixes_to_remove = [
            "The image shows", "This image depicts", "In this image",
            "The outfit consists of", "The person is wearing",
            "Looking at this outfit", "This outfit features"
        ]
        
        for prefix in prefixes_to_remove:
            if cleaned.startswith(prefix):
                cleaned = cleaned[len(prefix):].strip()
        
        # ç§»é™¤å¤šé¤˜çš„æ¨™é»
        cleaned = cleaned.strip(".,!?:ï¼š").strip()
        
        return cleaned if len(cleaned) > 10 else "å»ºè­°ç”Ÿæˆä¸­é‡åˆ°å•é¡Œï¼Œè«‹é‡è©¦"
    
    def analyze_and_recommend(self, image_path, gender, top_k=5, style_preference=None, 
                            models=['blip2', 'llava']):
        """
        å®Œæ•´çš„AIé©…å‹•åˆ†æå’Œæ¨è–¦æµç¨‹
        
        Args:
            image_path: ç”¨æˆ¶åœ–ç‰‡è·¯å¾‘
            gender: æ€§åˆ¥
            top_k: æ¨è–¦æ•¸é‡
            style_preference: é¢¨æ ¼åå¥½
            models: ä½¿ç”¨çš„AIæ¨¡å‹åˆ—è¡¨
            
        Returns:
            dict: å®Œæ•´çš„AIåˆ†æçµæœ
        """
        print(f"\nğŸ¯ é–‹å§‹AIé©…å‹•ç©¿æ­åˆ†æ...")
        print(f"ğŸ“ åœ–ç‰‡: {image_path}")
        print(f"ğŸ‘¤ æ€§åˆ¥: {gender}")
        print(f"ğŸ¨ é¢¨æ ¼åå¥½: {style_preference or 'ç„¡é™åˆ¶'}")
        print(f"ğŸ§  AIæ¨¡å‹: {models}")
        
        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            return {"error": f"åœ–ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}"}
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šæ‰¾å‡ºç›¸ä¼¼ç©¿æ­
            similar_outfits = self.find_similar_outfits(
                image_path, gender, top_k, style_preference
            )
            
            if not similar_outfits:
                return {
                    "error": "æ²’æœ‰æ‰¾åˆ°ç›¸ä¼¼çš„ç©¿æ­",
                    "suggestion": "è«‹å˜—è©¦èª¿æ•´æ€§åˆ¥æˆ–é¢¨æ ¼åå¥½è¨­ç½®"
                }
            
            # ç¬¬äºŒæ­¥ï¼šAIå¤šæ¨¡å‹åˆ†æ
            ai_analysis = self.generate_ai_advice_parallel(
                image_path, similar_outfits, models
            )
            
            # ç¬¬ä¸‰æ­¥ï¼šç¶œåˆçµæœ
            result = {
                "status": "success",
                "input_image": image_path,
                "gender": gender,
                "style_preference": style_preference,
                "similar_outfits": similar_outfits,
                "ai_analysis": ai_analysis,
                "models_used": models,
                "summary": {
                    "best_match": similar_outfits[0] if similar_outfits else None,
                    "total_recommendations": len(similar_outfits),
                    "ai_models_count": len(models)
                }
            }
            
            print(f"âœ… AIåˆ†æå®Œæˆï¼")
            print(f"ğŸ† æœ€ä½³åŒ¹é…: {result['summary']['best_match']['style']} ({result['summary']['best_match']['score']:.1f}åˆ†)")
            print(f"ğŸ§  AIæ¨¡å‹æ•¸é‡: {result['summary']['ai_models_count']}")
            
            return result
            
        except Exception as e:
            print(f"âŒ åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {"error": f"åˆ†æå¤±æ•—: {str(e)}"}
    
    def print_ai_recommendations(self, results):
        """
        ç¾åŒ–æ‰“å°AIæ¨è–¦çµæœ
        
        Args:
            results: AIåˆ†æçµæœ
        """
        if "error" in results:
            print(f"âŒ éŒ¯èª¤: {results['error']}")
            return
        
        print(f"\n{'='*80}")
        print(f"ğŸ¤– AIé©…å‹•ç©¿æ­æ¨è–¦çµæœ")
        print(f"{'='*80}")
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"ğŸ“¸ åˆ†æåœ–ç‰‡: {results['input_image']}")
        print(f"ğŸ‘¤ æ€§åˆ¥: {results['gender']}")
        print(f"ğŸ¨ é¢¨æ ¼åå¥½: {results['style_preference'] or 'ç„¡é™åˆ¶'}")
        print(f"ğŸ§  ä½¿ç”¨çš„AIæ¨¡å‹: {', '.join(results['models_used'])}")
        
        # æœ€ä½³åŒ¹é…
        best_match = results['summary']['best_match']
        print(f"\nğŸ¥‡ æœ€ä½³åŒ¹é…:")
        print(f"  é¢¨æ ¼: {best_match['style']}")
        print(f"  ç›¸ä¼¼åº¦: {best_match['similarity']:.3f}")
        print(f"  è©•åˆ†: {best_match['score']:.1f}/10")
        print(f"  åœ–ç‰‡: {best_match['path']}")
        
        # AIåˆ†æçµæœ
        ai_analysis = results['ai_analysis']
        print(f"\nğŸ§  AIå¤šæ¨¡å‹åˆ†æçµæœ:")
        
        for target_key, target_data in ai_analysis.items():
            if 'error' in target_data:
                continue
                
            target_info = target_data['target_info']
            feature_analysis = target_data['feature_analysis']
            ai_suggestions = target_data['ai_suggestions']
            
            print(f"\nğŸ“¸ {target_key.upper()}: {target_info['style']} é¢¨æ ¼")
            print(f"  ç›¸ä¼¼åº¦: {feature_analysis.get('overall_similarity', 0):.3f}")
            print(f"  å·®ç•°ç¨‹åº¦: {feature_analysis.get('difference_magnitude', 0):.3f}")
            
            # é¡¯ç¤ºå„AIæ¨¡å‹çš„å»ºè­°
            for model_key, suggestion in ai_suggestions.items():
                model_name = suggestion.get('model', model_key.upper())
                confidence = suggestion.get('confidence', 'unknown')
                
                print(f"\n  ğŸ¤– {model_name} å»ºè­° (ä¿¡å¿ƒåº¦: {confidence}):")
                if 'error' in suggestion:
                    print(f"    âŒ {suggestion['error']}")
                else:
                    advice = suggestion.get('advice', 'ç„¡å»ºè­°')
                    print(f"    ğŸ’¡ {advice}")
        
        # æ¨¡å‹æ¯”è¼ƒç¸½çµ
        print(f"\nğŸ“Š æ¨¡å‹å»ºè­°æ¯”è¼ƒ:")
        self._print_model_comparison(ai_analysis)
        
        print(f"\n{'='*80}")
    
    def _print_model_comparison(self, ai_analysis):
        """æ‰“å°æ¨¡å‹å»ºè­°æ¯”è¼ƒ"""
        model_advice_count = {}
        model_errors = {}
        
        for target_data in ai_analysis.values():
            if 'ai_suggestions' in target_data:
                for model_key, suggestion in target_data['ai_suggestions'].items():
                    model_name = suggestion.get('model', model_key.upper())
                    
                    if model_name not in model_advice_count:
                        model_advice_count[model_name] = 0
                        model_errors[model_name] = 0
                    
                    if 'error' in suggestion:
                        model_errors[model_name] += 1
                    else:
                        model_advice_count[model_name] += 1
        
        for model_name in model_advice_count:
            success_count = model_advice_count[model_name]
            error_count = model_errors[model_name]
            total = success_count + error_count
            success_rate = (success_count / total * 100) if total > 0 else 0
            
            print(f"  {model_name}: æˆåŠŸ {success_count}/{total} ({success_rate:.1f}%)")


def main():
    """ä¸»å‡½æ•¸ - AIé©…å‹•å‘½ä»¤è¡Œç•Œé¢"""
    import argparse
    
    parser = argparse.ArgumentParser(description='AIé©…å‹•ç©¿æ­æ¨è–¦ç³»çµ±')
    parser.add_argument('--image', type=str, required=True, help='ç”¨æˆ¶åœ–ç‰‡è·¯å¾‘')
    parser.add_argument('--gender', type=str, required=True, choices=['MEN', 'WOMEN'], help='æ€§åˆ¥')
    parser.add_argument('--top_k', type=int, default=5, help='æ¨è–¦æ•¸é‡')
    parser.add_argument('--style', type=str, help='é¢¨æ ¼åå¥½')
    parser.add_argument('--model', type=str, default='simple_fashion_model_final_best.pth', help='æ¨¡å‹è·¯å¾‘')
    parser.add_argument('--labels', type=str, default='simple_dataset_labels.json', help='æ¨™ç±¤æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--ai_models', type=str, nargs='+', default=['blip2', 'llava'], 
                       help='ä½¿ç”¨çš„AIæ¨¡å‹ (å¯é¸: blip2, llava, instructblip)')
    parser.add_argument('--save', type=str, help='ä¿å­˜çµæœåˆ°JSONæ–‡ä»¶')
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–ç³»çµ±
        system = MultiModelAIRecommendationSystem(args.model, args.labels)
        
        # é€²è¡ŒAIåˆ†æ
        results = system.analyze_and_recommend(
            args.image, 
            args.gender, 
            args.top_k, 
            args.style,
            args.ai_models
        )
        
        # é¡¯ç¤ºçµæœ
        system.print_ai_recommendations(results)
        
        # ä¿å­˜çµæœ
        if args.save:
            with open(args.save, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {args.save}")
            
    except Exception as e:
        print(f"âŒ ç³»çµ±éŒ¯èª¤: {e}")


if __name__ == "__main__":
    main() 