import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
import torchvision.models as models
import numpy as np
import cv2
from PIL import Image
import os
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from collections import defaultdict
import json
import time
import gc
import psutil
import multiprocessing as mp
import sys

import torch
import torch.multiprocessing as mp
import os

# NVIDIA CUDA ç‰¹å®šå„ªåŒ–è¨­ç½®
def setup_cuda_optimization():
    """è¨­ç½® NVIDIA CUDA ç‰¹å®šçš„å„ªåŒ–"""
    # è¨­ç½®å¤šé€²ç¨‹å•Ÿå‹•æ–¹æ³•ç‚ºspawnï¼ˆé©ç”¨æ–¼å¤šå¹³å°ï¼ŒåŒ…æ‹¬ Windows/Linuxï¼‰
    if mp.get_start_method(allow_none=True) != 'spawn':
        mp.set_start_method('spawn', force=True)

    # æª¢æŸ¥æ˜¯å¦æœ‰ CUDA æ”¯æŒ
    if torch.cuda.is_available():
        print(f"âœ… æª¢æ¸¬åˆ° CUDA æ”¯æŒ: {torch.cuda.get_device_name(0)}")
        device = torch.device("cuda")
        
        # CUDA æ€§èƒ½å„ªåŒ–é¸é …
        torch.backends.cudnn.benchmark = True  # é©ç”¨æ–¼è¼¸å…¥å°ºå¯¸å›ºå®šçš„æ¨¡å‹
        torch.backends.cudnn.deterministic = False  # æé«˜é€Ÿåº¦ä½†çµæœéå®Œå…¨å¯é‡ç¾
    else:
        print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
        device = torch.device("cpu")

    # æ ¹æ“š CPU æ ¸å¿ƒæ•¸è¨­ç½® PyTorch ç·šç¨‹æ•¸
    torch.set_num_threads(min(8, mp.cpu_count()))

    return device

# ================== è¨˜æ†¶é«”å„ªåŒ–çš„æ•¸æ“šé›† ==================

class OptimizedOutfitDataset(Dataset):
    def __init__(self, root_dir, transform=None, mode='train', max_samples_per_class=500):
        self.root_dir = root_dir
        self.transform = transform
        self.mode = mode
        self.max_samples_per_class = max_samples_per_class
        
        # å®šç¾©é¢¨æ ¼é¡åˆ¥
        self.style_categories = [
            'Artsy', 'Athleisure', 'BRITISH', 'CASUAL', 'GOTH', 'Japanese',
            'Kawaii', 'Korean', 'MINIMALIST', 'Preppy', 'STREET', 'Streetwear', 
            'Vintage', 'Y2K'
        ]
        
        self.gender_categories = ['MEN', 'WOMEN']
        
        # å»ºç«‹é¡åˆ¥åˆ°ç´¢å¼•çš„æ˜ å°„
        self.style_to_idx = {style: idx for idx, style in enumerate(self.style_categories)}
        self.gender_to_idx = {gender: idx for idx, gender in enumerate(self.gender_categories)}
        
        # è¼‰å…¥åœ–ç‰‡è·¯å¾‘å’Œæ¨™ç±¤ï¼ˆé™åˆ¶æ•¸é‡ä»¥ç¯€çœè¨˜æ†¶é«”ï¼‰
        self.samples = []
        self._load_samples()
        
        print(f"ğŸ“Š è¼‰å…¥äº† {len(self.samples)} å€‹æ¨£æœ¬")
        
    def _load_samples(self):
        class_counts = defaultdict(int)
        
        for style in self.style_categories:
            for gender in self.gender_categories:
                folder_name = f"{style}_{gender}"
                folder_path = os.path.join(self.root_dir, folder_name)
                
                if os.path.exists(folder_path):
                    img_files = [f for f in os.listdir(folder_path) 
                               if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                    
                    # é™åˆ¶æ¯å€‹é¡åˆ¥çš„æ¨£æœ¬æ•¸é‡
                    img_files = img_files[:self.max_samples_per_class]
                    
                    for img_name in img_files:
                        img_path = os.path.join(folder_path, img_name)
                        
                        # å¿«é€Ÿæª¢æŸ¥æ–‡ä»¶å¤§å°ï¼ˆè·³ééå°çš„æ–‡ä»¶ï¼‰
                        if os.path.getsize(img_path) < 1024:  # å°æ–¼1KB
                            continue
                            
                        if self._is_valid_image(img_path):
                            self.samples.append({
                                'path': img_path,
                                'style': style,
                                'gender': gender,
                                'style_idx': self.style_to_idx[style],
                                'gender_idx': self.gender_to_idx[gender]
                            })
                            class_counts[f"{style}_{gender}"] += 1
        
        print("ğŸ“ˆ å„é¡åˆ¥æ¨£æœ¬æ•¸é‡:")
        for class_name, count in sorted(class_counts.items()):
            print(f"  {class_name}: {count}")
    
    def _is_valid_image(self, img_path):
        """å¿«é€Ÿæª¢æŸ¥åœ–ç‰‡æ˜¯å¦æœ‰æ•ˆ"""
        try:
            with Image.open(img_path) as img:
                img.verify()  # é©—è­‰åœ–ç‰‡å®Œæ•´æ€§
                return True
        except Exception:
            return False
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # è¼‰å…¥åœ–ç‰‡ä¸¦è™•ç†ç•°å¸¸
        try:
            image = Image.open(sample['path']).convert('RGB')
            
            # æª¢æŸ¥åœ–ç‰‡å°ºå¯¸
            if min(image.size) < 32:  # å¤ªå°çš„åœ–ç‰‡
                raise ValueError("åœ–ç‰‡å¤ªå°")
                
        except Exception as e:
            print(f"âš ï¸ è¼‰å…¥åœ–ç‰‡å¤±æ•—: {sample['path']}")
            # å‰µå»ºä¸€å€‹é è¨­çš„å½©è‰²åœ–ç‰‡
            image = Image.new('RGB', (224, 224), color=(128, 128, 128))
        
        if self.transform:
            image = self.transform(image)
            
        return {
            'image': image,
            'style_idx': sample['style_idx'],
            'gender_idx': sample['gender_idx'],
            'style': sample['style'],
            'gender': sample['gender'],
            'path': sample['path']
        }

# ================== è¼•é‡åŒ–æ¨¡å‹ ==================

class LightweightStyleClassifier(nn.Module):
    def __init__(self, num_styles=12, num_genders=2, feature_dim=1024):
        super(LightweightStyleClassifier, self).__init__()
        
        # ä½¿ç”¨æ›´è¼•é‡çš„backbone
        try:
            # å˜—è©¦ä½¿ç”¨MobileNetV3
            self.backbone = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)
            self.backbone.classifier = nn.Identity()
            backbone_features = 960
        except:
            # å‚™é¸æ–¹æ¡ˆï¼šä½¿ç”¨ResNet18
            self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
            self.backbone.fc = nn.Identity()
            backbone_features = 512
            
        self.feature_dim = feature_dim
        
        # ç‰¹å¾µæŠ•å½±å±¤
        self.feature_projection = nn.Sequential(
            nn.Linear(backbone_features, feature_dim),
            nn.BatchNorm1d(feature_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # é¢¨æ ¼åˆ†é¡é ­ï¼ˆç°¡åŒ–ï¼‰
        self.style_classifier = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_styles)
        )
        
        # æ€§åˆ¥åˆ†é¡é ­ï¼ˆç°¡åŒ–ï¼‰
        self.gender_classifier = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_genders)
        )
        
        # æŠ•å½±é ­ï¼ˆç”¨æ–¼å°æ¯”å­¸ç¿’ï¼‰
        self.projection_head = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128)
        )
        
        # åˆå§‹åŒ–æ¬Šé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æ¬Šé‡"""
        for module in [self.feature_projection, self.style_classifier, 
                      self.gender_classifier, self.projection_head]:
            for m in module.modules():
                if isinstance(m, nn.Linear):
                    nn.init.xavier_uniform_(m.weight)
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
                elif isinstance(m, nn.BatchNorm1d):
                    nn.init.constant_(m.weight, 1)
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # ç‰¹å¾µæå–
        backbone_features = self.backbone(x)
        features = self.feature_projection(backbone_features)
        
        # åˆ†é¡é æ¸¬
        style_logits = self.style_classifier(features)
        gender_logits = self.gender_classifier(features)
        
        # æŠ•å½±ç‰¹å¾µ
        projected_features = self.projection_head(features)
        projected_features = F.normalize(projected_features, dim=1)
        
        return {
            'features': features,
            'projected_features': projected_features,
            'style_logits': style_logits,
            'gender_logits': gender_logits
        }

# ================== è¨˜æ†¶é«”å„ªåŒ–çš„å°æ¯”å­¸ç¿’ ==================

class MemoryEfficientContrastiveLoss(nn.Module):
    def __init__(self, temperature=0.07):
        super(MemoryEfficientContrastiveLoss, self).__init__()
        self.temperature = temperature
    
    def forward(self, features, labels):
        batch_size = features.shape[0]
        
        if batch_size < 2:
            return torch.tensor(0.0, device=features.device)
        
        # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£ï¼ˆåˆ†å¡Šè¨ˆç®—ä»¥ç¯€çœè¨˜æ†¶é«”ï¼‰
        chunk_size = min(32, batch_size)
        similarity_matrix = torch.zeros(batch_size, batch_size, device=features.device)
        
        for i in range(0, batch_size, chunk_size):
            end_i = min(i + chunk_size, batch_size)
            for j in range(0, batch_size, chunk_size):
                end_j = min(j + chunk_size, batch_size)
                
                chunk_sim = torch.matmul(features[i:end_i], features[j:end_j].T) / self.temperature
                similarity_matrix[i:end_i, j:end_j] = chunk_sim
        
        # å»ºç«‹æ­£æ¨£æœ¬mask
        labels = labels.contiguous().view(-1, 1)
        mask = torch.eq(labels, labels.T).float().to(features.device)
        mask = mask.fill_diagonal_(0)
        
        if mask.sum() == 0:
            return torch.tensor(0.0, device=features.device)
        
        # è¨ˆç®—å°æ¯”æå¤±
        similarity_matrix = similarity_matrix - similarity_matrix.max(dim=1, keepdim=True)[0].detach()
        exp_sim = torch.exp(similarity_matrix)
        sum_exp_sim = torch.sum(exp_sim, dim=1, keepdim=True)
        
        log_prob = similarity_matrix - torch.log(sum_exp_sim + 1e-8)
        mean_log_prob_pos = torch.sum(mask * log_prob, dim=1) / (torch.sum(mask, dim=1) + 1e-8)
        
        valid_items = torch.sum(mask, dim=1) > 0
        if valid_items.sum() == 0:
            return torch.tensor(0.0, device=features.device)
            
        loss = -mean_log_prob_pos[valid_items].mean()
        
        if torch.isnan(loss):
            return torch.tensor(0.0, device=features.device)
            
        return loss

# ================== Cuda å„ªåŒ–çš„è¨“ç·´å™¨ ==================

class CudaOptimizedTrainer:
    def __init__(self, model, device):
        self.model = model
        self.device = device
        self.contrastive_loss = MemoryEfficientContrastiveLoss()
        
        # è¨˜æ†¶é«”ç›£æ§
        self.process = psutil.Process()
        self.scaler = torch.amp.GradScaler(device='cuda')  # ç”¨æ–¼AMP
        
    def get_memory_usage(self):
        """ç²å–ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨é‡"""
        memory_info = self.process.memory_info()
        return memory_info.rss / 1024 / 1024  # MB
    
    def train_epoch(self, dataloader, optimizer, epoch, max_memory_mb=8192):
        self.model.train()
        total_loss = 0
        num_batches = 0
        
        print(f"ğŸš€ é–‹å§‹è¨“ç·´ Epoch {epoch}")
        start_time = time.time()
        
        for batch_idx, batch in enumerate(dataloader):
            try:
                # è¨˜æ†¶é«”æª¢æŸ¥
                current_memory = self.get_memory_usage()
                if current_memory > max_memory_mb:
                    print(f"âš ï¸ è¨˜æ†¶é«”ä½¿ç”¨éé«˜ ({current_memory:.1f}MB)ï¼ŒåŸ·è¡Œåƒåœ¾å›æ”¶")
                    gc.collect()
                    if self.device.type == 'cuda':
                        torch.cuda.empty_cache()
                
                images = batch['image'].to(self.device, non_blocking=True)
                style_labels = batch['style_idx'].to(self.device, non_blocking=True)
                gender_labels = batch['gender_idx'].to(self.device, non_blocking=True)
                
                # æª¢æŸ¥è¼¸å…¥æœ‰æ•ˆæ€§
                if torch.isnan(images).any() or torch.isinf(images).any():
                    print(f"âš ï¸ è·³ébatch {batch_idx}: è¼¸å…¥åŒ…å«NaNæˆ–Inf")
                    continue
                
                optimizer.zero_grad()
                
                # å‰å‘å‚³æ’­ + AMP
                with torch.amp.autocast(device_type='cuda'):  # âœ… AMP enabled on CUDA
                    outputs = self.model(images)

                    style_loss = F.cross_entropy(outputs['style_logits'], style_labels)
                    gender_loss = F.cross_entropy(outputs['gender_logits'], gender_labels)
                    contrastive_loss = self.contrastive_loss(outputs['projected_features'], style_labels)

                    if torch.isnan(style_loss) or torch.isnan(gender_loss) or torch.isnan(contrastive_loss):
                        print(f"âš ï¸ è·³ébatch {batch_idx}: æå¤±åŒ…å«NaN")
                        continue

                    total_batch_loss = style_loss + 0.2 * gender_loss + 0.05 * contrastive_loss
           
                if torch.isnan(total_batch_loss):
                    print(f"âš ï¸ è·³ébatch {batch_idx}: ç¸½æå¤±ç‚ºNaN")
                    continue
                
                # AMP æ¢¯åº¦åå‘å‚³æ’­èˆ‡ç¸®æ”¾
                self.scaler.scale(total_batch_loss).backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.scaler.step(optimizer)
                self.scaler.update()
                
                total_loss += total_batch_loss.item()
                num_batches += 1
                
                # é€²åº¦å ±å‘Š
                if batch_idx % 10 == 0:
                    elapsed = time.time() - start_time
                    memory_usage = self.get_memory_usage()
                    print(f'ğŸ“Š Epoch {epoch}, Batch {batch_idx}/{len(dataloader)}, '
                          f'Loss: {total_batch_loss.item():.4f}, '
                          f'Memory: {memory_usage:.1f}MB, '
                          f'Time: {elapsed:.1f}s')
                
                # å®šæœŸæ¸…ç†è¨˜æ†¶é«”
                if batch_idx % 50 == 0:
                    gc.collect()
            
            except Exception as e:
                print(f"âŒ è™•ç†batch {batch_idx}æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                continue
        
        if num_batches == 0:
            return 0.0
        
        avg_loss = total_loss / num_batches
        epoch_time = time.time() - start_time
        print(f"âœ… Epoch {epoch} å®Œæˆï¼Œå¹³å‡æå¤±: {avg_loss:.4f}ï¼Œè€—æ™‚: {epoch_time:.1f}s")
        
        return avg_loss

# ================== ä¸»ç¨‹å¼ ==================

def main(config, resume):
    print("ğŸŸ© Cuda å„ªåŒ–è¨“ç·´é–‹å§‹")
    
    device = setup_cuda_optimization()
    
    # æ•¸æ“šå¢å¼·
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(p=config.HORIZONTAL_FLIP_PROB),
        transforms.ColorJitter(
            brightness=config.COLOR_JITTER_BRIGHTNESS,
            contrast=config.COLOR_JITTER_CONTRAST
        ),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    # å»ºç«‹æ•¸æ“šé›†
    print("ğŸ“‚ è¼‰å…¥æ•¸æ“šé›†...")
    train_dataset = OptimizedOutfitDataset(
        config.DATA_ROOT,
        transform=train_transform,
        max_samples_per_class=config.MAX_SAMPLES_PER_CLASS
    )
    
    # Cuda å„ªåŒ–çš„DataLoaderè¨­ç½®
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=True,
        num_workers=config.NUM_WORKERS,
        pin_memory=config.PIN_MEMORY,
        drop_last=config.DROP_LAST,
        persistent_workers=config.PERSISTENT_WORKERS
    )
    
    print(f"ğŸ“Š æ•¸æ“šé›†å¤§å°: {len(train_dataset)}")
    print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {device}")
    
    # åˆå§‹åŒ–è¼•é‡åŒ–æ¨¡å‹
    model = LightweightStyleClassifier(
        num_styles=config.NUM_STYLES,
        num_genders=config.NUM_GENDERS,
        feature_dim=config.FEATURE_DIM
    )
    model.to(device)
    
    # è¨ˆç®—æ¨¡å‹åƒæ•¸æ•¸é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“ˆ æ¨¡å‹åƒæ•¸: {total_params:,} (å¯è¨“ç·´: {trainable_params:,})")
    
    # åˆå§‹åŒ–è¨“ç·´å™¨
    trainer = CudaOptimizedTrainer(model, device)
    
    # å„ªåŒ–å™¨è¨­ç½®
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.LEARNING_RATE,
        weight_decay=config.WEIGHT_DECAY,
        betas=config.OPTIMIZER_BETAS
    )
    
    # å­¸ç¿’ç‡èª¿åº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=config.NUM_EPOCHS,
        eta_min=config.LEARNING_RATE * config.SCHEDULER_ETA_MIN_RATIO
    )
    
    # è¨“ç·´å¾ªç’°
    print("ğŸš€ é–‹å§‹è¨“ç·´...")
    best_loss = float('inf')
    start_epoch = 0
    
    if resume and os.path.exists(resume):
        print(f"ğŸ”„ å¾æª¢æŸ¥é»æ¢å¾©: {resume}")
        checkpoint = torch.load(resume, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        best_loss = checkpoint.get('loss', float('inf'))
    
    os.makedirs(config.TRAINED_MODEL_FOLDER, exist_ok=True)
    os.makedirs(config.CHECKPOINT_FOLDER, exist_ok=True)
    
    # è¨“ç·´å¾ªç’°
    try:
        for epoch in range(start_epoch, config.NUM_EPOCHS):
            print(f"\n{'='*50}")
            print(f"ğŸ“… Epoch {epoch+1}/{config.NUM_EPOCHS}")
            print(f"{'='*50}")
            
            # è¨“ç·´ä¸€å€‹epoch
            avg_loss = trainer.train_epoch(
                train_loader, optimizer, epoch, config.MAX_MEMORY_MB
            )
            scheduler.step()
            
            current_lr = optimizer.param_groups[0]['lr']
            memory_usage = trainer.get_memory_usage()
            
            print(f"ğŸ“Š Epoch {epoch+1} çµæœ:")
            print(f"  å¹³å‡æå¤±: {avg_loss:.4f}")
            print(f"  å­¸ç¿’ç‡: {current_lr:.6f}")
            print(f"  è¨˜æ†¶é«”ä½¿ç”¨: {memory_usage:.1f}MB")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_loss < best_loss and avg_loss > 0:
                best_loss = avg_loss
                torch.save(model.state_dict(), config.BEST_MODEL_PATH)
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œæå¤±: {best_loss:.4f}")
            
            # ä¿å­˜æª¢æŸ¥é»
            if (epoch + 1) % config.SAVE_CHECKPOINT_EVERY == 0:
                # å‰µå»ºå¯åºåˆ—åŒ–çš„é…ç½®å­—å…¸
                config_dict = {}
                for key, value in config.__dict__.items():
                    if isinstance(value, (int, float, str, bool, list, tuple)):
                        config_dict[key] = value
                
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'loss': avg_loss,
                    'config': config_dict
                }
                checkpoint_path = f"{config.CHECKPOINT_PREFIX}{epoch+1}.pth"
                torch.save(checkpoint, checkpoint_path)
                print(f"ğŸ’¾ ä¿å­˜æª¢æŸ¥é»: {checkpoint_path}")
            
            # è¨˜æ†¶é«”æ¸…ç†
            gc.collect()
    
    except KeyboardInterrupt:
        print("\nâš ï¸ è¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·")
        # ä¿å­˜ç•¶å‰ç‹€æ…‹
        config_dict = {}
        for key, value in config.__dict__.items():
            if isinstance(value, (int, float, str, bool, list, tuple)):
                config_dict[key] = value
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'loss': avg_loss if 'avg_loss' in locals() else float('inf'),
            'config': config_dict
        }
        torch.save(checkpoint, config.INTERRUPT_PATH)
        print(f"ğŸ’¾ å·²ä¿å­˜ä¸­æ–·æª¢æŸ¥é»: {config.INTERRUPT_PATH}")
    
    except Exception as e:
        print(f"âŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        sys.exit(1)
    
    # ä¿å­˜æœ€çµ‚æ¨¡å‹
    torch.save(model.state_dict(), config.FINAL_MODEL_PATH)
    
    print("\nğŸ‰ è¨“ç·´å®Œæˆï¼")
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  - {config.BEST_MODEL_PATH} (æœ€ä½³æ¨¡å‹)")
    print(f"  - {config.FINAL_MODEL_PATH} (æœ€çµ‚æ¨¡å‹)")
    print(f"  - {config.CHECKPOINT_PREFIX}*.pth (æª¢æŸ¥é»)")

if __name__ == "__main__":
    main() 