#!/usr/bin/env python3
"""
æ™‚å°šAIç„¡ç›£ç£å­¸ç¿’æ¸¬è©¦ç³»çµ±
ä½¿ç”¨è¨“ç·´å¥½çš„ç„¡ç›£ç£æ¨¡å‹é€²è¡Œç›¸ä¼¼åº¦æ¨è–¦
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import torchvision.models as models
import timm
from transformers import ViTModel, ViTConfig
from PIL import Image
import numpy as np
import os
import json
import pickle
from datetime import datetime
from sklearn.neighbors import NearestNeighbors
import argparse
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# ==================== Macå„ªåŒ–è¨­ç½® ====================

def setup_mac_optimization():
    """è¨­ç½®Macå„ªåŒ–"""
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ğŸ ä½¿ç”¨ Metal Performance Shaders (MPS)")
    else:
        device = torch.device("cpu")
        print("ğŸ’» ä½¿ç”¨ CPU")
    return device

# ==================== æ¨¡å‹æ¶æ§‹å®šç¾©ï¼ˆèˆ‡è¨“ç·´æ™‚ç›¸åŒï¼‰ ====================

class FashionBackbone(nn.Module):
    """å¯é…ç½®çš„Fashion Backboneï¼ˆèˆ‡è¨“ç·´æ™‚ç›¸åŒï¼‰"""
    def __init__(self, backbone_type='resnet50', pretrained=True):
        super(FashionBackbone, self).__init__()
        self.backbone_type = backbone_type
        
        if backbone_type == 'mobilenet':
            self.backbone = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1 if pretrained else None)
            self.backbone.classifier = nn.Identity()
            self.feature_dim = 960
            
        elif backbone_type == 'resnet18':
            self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)
            self.backbone.fc = nn.Identity()
            self.feature_dim = 512
            
        elif backbone_type == 'resnet50':
            self.backbone = timm.create_model("resnet50", pretrained=pretrained, num_classes=0)
            self.feature_dim = 2048
            
        elif backbone_type == 'efficientnet_b0':
            self.backbone = timm.create_model('efficientnet_b0', pretrained=pretrained, num_classes=0)
            self.feature_dim = 1280
            
        elif backbone_type == 'efficientnet_b2':
            self.backbone = timm.create_model('efficientnet_b2', pretrained=pretrained, num_classes=0)
            self.feature_dim = 1408
            
        elif backbone_type == 'vit_tiny':
            if pretrained:
                self.backbone = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=0)
            else:
                config = ViTConfig(hidden_size=192, num_hidden_layers=12, num_attention_heads=3)
                self.backbone = ViTModel(config)
            self.feature_dim = 192
            
        elif backbone_type == 'vit_small':
            if pretrained:
                self.backbone = timm.create_model('vit_small_patch16_224', pretrained=True, num_classes=0)
            else:
                config = ViTConfig(hidden_size=384, num_hidden_layers=12, num_attention_heads=6)
                self.backbone = ViTModel(config)
            self.feature_dim = 384
            
        elif backbone_type == 'fashion_resnet':
            self.backbone = self._create_fashion_resnet(pretrained)
            self.feature_dim = 512
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„backboneé¡å‹: {backbone_type}")
    
    def _create_fashion_resnet(self, pretrained):
        """å‰µå»ºé‡å°æ™‚å°šåœ–ç‰‡å„ªåŒ–çš„ResNet"""
        base_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)
        base_model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        base_model.maxpool = nn.Identity()
        base_model.fc = nn.Identity()
        return base_model
    
    def forward(self, x):
        if 'vit' in self.backbone_type and hasattr(self.backbone, 'last_hidden_state'):
            outputs = self.backbone(x)
            if hasattr(outputs, 'last_hidden_state'):
                return outputs.last_hidden_state[:, 0, :]
            else:
                return outputs
        else:
            return self.backbone(x)
    
    def get_feature_dim(self):
        return self.feature_dim

class UnsupervisedStyleModel(nn.Module):
    """ç„¡ç›£ç£é¢¨æ ¼æ¨¡å‹ï¼ˆèˆ‡è¨“ç·´æ™‚ç›¸åŒï¼‰"""
    def __init__(self, backbone_type='resnet50', feature_dim=128):
        super(UnsupervisedStyleModel, self).__init__()
        
        self.backbone = FashionBackbone(
            backbone_type=backbone_type, 
            pretrained=True
        )
        
        backbone_features = self.backbone.get_feature_dim()
        self.feature_dim = feature_dim
        
        # æŠ•å½±é ­ - èˆ‡è¨“ç·´æ™‚æ¶æ§‹å®Œå…¨ä¸€è‡´
        self.projection = nn.Sequential(
            nn.Linear(backbone_features, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, feature_dim),
            nn.BatchNorm1d(feature_dim)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, (nn.BatchNorm1d, nn.LayerNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
        
    def forward(self, x):
        backbone_features = self.backbone(x)
        features = self.projection(backbone_features)
        return features

# ==================== æ¨¡å‹è¼‰å…¥å·¥å…· ====================

def load_trained_model(model_path, device, backbone_type='resnet50'):
    """è¼‰å…¥è¨“ç·´å¥½çš„ç„¡ç›£ç£æ¨¡å‹"""
    print(f"ğŸ“‚ è¼‰å…¥æ¨¡å‹: {model_path}")
    
    # å‰µå»ºæ¨¡å‹
    model = UnsupervisedStyleModel(
        backbone_type=backbone_type,
        feature_dim=128
    )
    
    try:
        # è¼‰å…¥æ¬Šé‡
        checkpoint = torch.load(model_path, map_location=device)
        
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"âœ… è¼‰å…¥checkpointï¼Œepoch: {checkpoint.get('epoch', 'æœªçŸ¥')}")
        else:
            model.load_state_dict(checkpoint)
            print("âœ… è¼‰å…¥state_dict")
        
        model.to(device)
        model.eval()
        
        return model
        
    except Exception as e:
        print(f"âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
        print("\nğŸ” æª¢æŸ¥æ¨¡å‹æª”æ¡ˆè³‡è¨Šï¼š")
        try:
            checkpoint = torch.load(model_path, map_location='cpu')
            if isinstance(checkpoint, dict):
                print(f"   Checkpoint keys: {list(checkpoint.keys())}")
                if 'model_state_dict' in checkpoint:
                    print(f"   Model keys (first 5): {list(checkpoint['model_state_dict'].keys())[:5]}")
            else:
                print(f"   State dict keys (first 5): {list(checkpoint.keys())[:5]}")
        except Exception as inspect_error:
            print(f"   ç„¡æ³•æª¢æŸ¥æª”æ¡ˆ: {inspect_error}")
        raise

# ==================== æ¨ç†åŠŸèƒ½ ====================

def extract_image_features(model, image_path, device):
    """æå–å–®å¼µåœ–ç‰‡çš„ç‰¹å¾µ"""
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    try:
        # è¼‰å…¥å’Œé è™•ç†åœ–ç‰‡
        image = Image.open(image_path).convert('RGB')
        input_tensor = transform(image).unsqueeze(0).to(device)
        
        # æ¨ç†
        with torch.no_grad():
            features = model(input_tensor)
            features = F.normalize(features, dim=1)  # æ­¸ä¸€åŒ–ç‰¹å¾µ
            
            result = {
                'features': features.cpu().numpy()[0],
                'image_path': image_path
            }
            
            return result
            
    except Exception as e:
        print(f"âŒ ç‰¹å¾µæå–å¤±æ•—: {e}")
        raise

def extract_features_from_dataset_labels(labels_path):
    """å¾dataset_labels.jsonè¼‰å…¥ç‰¹å¾µæ•¸æ“šåº«"""
    print(f"ğŸ“ è¼‰å…¥ç‰¹å¾µæ•¸æ“šåº«: {labels_path}")
    
    try:
        with open(labels_path, 'r') as f:
            dataset_labels = json.load(f)
        
        feature_database = {
            'features': [],
            'image_paths': []
        }
        
        for item in dataset_labels:
            feature_database['features'].append(item['unsupervised_features'])
            feature_database['image_paths'].append(item['path'])
        
        feature_database['features'] = np.array(feature_database['features'])
        
        print(f"âœ… è¼‰å…¥äº† {len(feature_database['features'])} å€‹ç‰¹å¾µå‘é‡")
        return feature_database
        
    except Exception as e:
        print(f"âŒ è¼‰å…¥ç‰¹å¾µæ•¸æ“šåº«å¤±æ•—: {e}")
        raise

def find_similar_images(user_features, feature_database, top_k=10):
    """æ‰¾åˆ°æœ€ç›¸ä¼¼çš„åœ–ç‰‡"""
    print(f"ğŸ” æœç´¢æœ€ç›¸ä¼¼çš„ {top_k} å¼µåœ–ç‰‡...")
    
    # æ­¸ä¸€åŒ–ç‰¹å¾µ
    user_features_norm = user_features['features'].reshape(1, -1)
    user_features_norm = user_features_norm / np.linalg.norm(user_features_norm)
    
    database_features_norm = feature_database['features']
    database_features_norm = database_features_norm / np.linalg.norm(database_features_norm, axis=1, keepdims=True)
    
    # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
    similarities = np.dot(user_features_norm, database_features_norm.T)[0]
    
    # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„
    top_indices = similarities.argsort()[-top_k:][::-1]
    
    similar_images = []
    for i, idx in enumerate(top_indices):
        similar_info = {
            'rank': i + 1,
            'similarity_score': float(similarities[idx]),
            'image_path': feature_database['image_paths'][idx],
            'score': float(similarities[idx] * 10)  # è½‰ç‚º0-10åˆ†
        }
        
        similar_images.append(similar_info)
    
    return similar_images

def create_recommendation_report(user_features, similar_images, output_dir="./recommendations"):
    """å‰µå»ºæ¨è–¦å ±å‘Š"""
    os.makedirs(output_dir, exist_ok=True)
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'user_image': user_features['image_path'],
        'similar_images': similar_images,
        'analysis': {
            'avg_similarity': float(np.mean([img['similarity_score'] for img in similar_images])),
            'max_similarity': float(max([img['similarity_score'] for img in similar_images])),
            'min_similarity': float(min([img['similarity_score'] for img in similar_images]))
        }
    }
    
    # ä¿å­˜å ±å‘Š
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = os.path.join(output_dir, f"unsupervised_recommendation_report_{timestamp}.json")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“Š æ¨è–¦å ±å‘Šå·²ä¿å­˜: {report_path}")
    
    return report, report_path

def print_analysis_results(user_features, similar_images):
    """æ‰“å°åˆ†æçµæœ"""
    print(f"\n{'='*80}")
    print(f"ğŸ¨ ç„¡ç›£ç£ç›¸ä¼¼åº¦åˆ†æçµæœ")
    print(f"{'='*80}")
    
    # ç”¨æˆ¶åœ–ç‰‡ä¿¡æ¯
    print(f"ğŸ“· åˆ†æåœ–ç‰‡: {user_features['image_path']}")
    
    # ç›¸ä¼¼åœ–ç‰‡
    print(f"\nğŸ” æœ€ç›¸ä¼¼çš„ç©¿æ­ (å‰{min(len(similar_images), 10)}å):")
    for img in similar_images[:10]:
        print(f"  {img['rank']}. {os.path.basename(img['image_path'])}")
        print(f"     ç›¸ä¼¼åº¦: {img['similarity_score']:.4f}")
        print(f"     è©•åˆ†: {img['score']:.2f}/10")
    
    # çµ±è¨ˆåˆ†æ
    similarities = [img['similarity_score'] for img in similar_images]
    print(f"\nğŸ“Š ç›¸ä¼¼åº¦çµ±è¨ˆ:")
    print(f"  â€¢ å¹³å‡ç›¸ä¼¼åº¦: {np.mean(similarities):.4f}")
    print(f"  â€¢ æœ€é«˜ç›¸ä¼¼åº¦: {np.max(similarities):.4f}")
    print(f"  â€¢ æœ€ä½ç›¸ä¼¼åº¦: {np.min(similarities):.4f}")
    print(f"  â€¢ æ¨™æº–å·®: {np.std(similarities):.4f}")
    
    # è·¯å¾‘åˆ†æ
    path_distribution = defaultdict(int)
    for img in similar_images:
        # æå–é¢¨æ ¼ä¿¡æ¯ï¼ˆå¦‚æœè·¯å¾‘åŒ…å«é¢¨æ ¼ä¿¡æ¯ï¼‰
        path_parts = img['image_path'].split('/')
        if len(path_parts) > 1:
            folder_name = path_parts[-2]  # å‡è¨­å€’æ•¸ç¬¬äºŒå€‹æ˜¯é¢¨æ ¼æ–‡ä»¶å¤¾
            path_distribution[folder_name] += 1
    
    if path_distribution:
        print(f"\nğŸ­ ç›¸ä¼¼åœ–ç‰‡é¢¨æ ¼åˆ†ä½ˆ:")
        for style, count in sorted(path_distribution.items(), key=lambda x: x[1], reverse=True):
            print(f"  â€¢ {style}: {count} å¼µ")
    
    print(f"\n{'='*80}")

# ==================== ä¸»ç¨‹åº ====================

def main():
    parser = argparse.ArgumentParser(description='æ™‚å°šç„¡ç›£ç£å­¸ç¿’æ¸¬è©¦ç³»çµ±')
    parser.add_argument('--model', type=str, required=True,
                       help='è¨“ç·´å¥½çš„ç„¡ç›£ç£æ¨¡å‹è·¯å¾‘')
    parser.add_argument('--image', type=str, required=True,
                       help='è¦åˆ†æçš„åœ–ç‰‡è·¯å¾‘')
    parser.add_argument('--labels', type=str, default='dataset_labels.json',
                       help='ç‰¹å¾µæ•¸æ“šåº«JSONæ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--backbone', type=str, default='resnet50',
                       choices=['mobilenet', 'resnet18', 'resnet50', 'efficientnet_b0', 
                               'efficientnet_b2', 'vit_tiny', 'vit_small', 'fashion_resnet'],
                       help='Backboneæ¶æ§‹é¡å‹')
    parser.add_argument('--output-dir', type=str, default='./recommendations',
                       help='çµæœè¼¸å‡ºç›®éŒ„')
    parser.add_argument('--top-k', type=int, default=10,
                       help='è¿”å›æœ€ç›¸ä¼¼çš„Kå¼µåœ–ç‰‡')
    
    args = parser.parse_args()
    
    print("ğŸ¤– æ™‚å°šç„¡ç›£ç£å­¸ç¿’æ¸¬è©¦ç³»çµ±")
    print("=" * 50)
    
    # æª¢æŸ¥æ–‡ä»¶
    if not os.path.exists(args.model):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model}")
        return
    
    if not os.path.exists(args.image):
        print(f"âŒ åœ–ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {args.image}")
        return
    
    if not os.path.exists(args.labels):
        print(f"âŒ ç‰¹å¾µæ•¸æ“šåº«æ–‡ä»¶ä¸å­˜åœ¨: {args.labels}")
        return
    
    try:
        # è¨­ç½®è¨­å‚™
        device = setup_mac_optimization()
        
        # è¼‰å…¥æ¨¡å‹
        model = load_trained_model(args.model, device, args.backbone)
        
        # æå–ç”¨æˆ¶åœ–ç‰‡ç‰¹å¾µ
        print(f"ğŸ” åˆ†æåœ–ç‰‡: {args.image}")
        user_features = extract_image_features(model, args.image, device)
        
        # è¼‰å…¥ç‰¹å¾µæ•¸æ“šåº«
        feature_database = extract_features_from_dataset_labels(args.labels)
        
        # æœç´¢ç›¸ä¼¼åœ–ç‰‡
        similar_images = find_similar_images(user_features, feature_database, args.top_k)
        
        # ç”Ÿæˆå ±å‘Š
        report, report_path = create_recommendation_report(
            user_features, similar_images, args.output_dir
        )
        
        # é¡¯ç¤ºçµæœ
        print_analysis_results(user_features, similar_images)
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼å ±å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
    except Exception as e:
        print(f"âŒ ç³»çµ±é‹è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 