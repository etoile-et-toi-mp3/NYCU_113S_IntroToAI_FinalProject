#!/usr/bin/env python3
"""
æ™‚å°šAIç„¡ç›£ç£å­¸ç¿’æ‰¹é‡è©•ä¼°ç³»çµ±
æ­£ç¢ºçš„æº–ç¢ºç‡è¨ˆç®—ï¼šé æ¸¬æ­£ç¢ºçš„åœ–ç‰‡æ•¸é‡ / (220 Ã— 5) = æ­£ç¢ºæ•¸ / 1100
"""

import subprocess
import json
import os
import argparse
from datetime import datetime
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import csv

def load_test_data_with_labels(testdata_dir):
    """è¼‰å…¥æ¸¬è©¦æ•¸æ“šä¸¦ç”Ÿæˆæ­£ç¢ºç­”æ¡ˆ"""
    print(f"ğŸ“‚ è¼‰å…¥æ¸¬è©¦æ•¸æ“š: {testdata_dir}")
    
    image_files = []
    for file in os.listdir(testdata_dir):
        if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):
            image_files.append(file)
    
    image_files.sort()
    print(f"ğŸ“Š æ‰¾åˆ° {len(image_files)} å¼µæ¸¬è©¦åœ–ç‰‡")
    
    test_data = []
    categories = set()
    
    for img_file in image_files:
        img_path = os.path.join(testdata_dir, img_file)
        filename_without_ext = os.path.splitext(img_file)[0]
        
        # æå–é¡åˆ¥åç¨±
        category = None
        for i in range(len(filename_without_ext), 0, -1):
            if not filename_without_ext[i-1:].isdigit():
                category = filename_without_ext[:i]
                break
        
        if category is None:
            category = filename_without_ext
        
        categories.add(category)
        test_data.append({
            'image_path': img_path,
            'filename': img_file,
            'category': category
        })
    
    categories = sorted(list(categories))
    print(f"ğŸ“‹ ç™¼ç¾ {len(categories)} å€‹é¡åˆ¥: {categories}")
    
    # çµ±è¨ˆæ¯å€‹é¡åˆ¥çš„åœ–ç‰‡æ•¸é‡
    category_counts = defaultdict(int)
    for item in test_data:
        category_counts[item['category']] += 1
    
    print("ğŸ“ˆ å„é¡åˆ¥åœ–ç‰‡æ•¸é‡:")
    for category in categories:
        print(f"  {category}: {category_counts[category]} å¼µ")
    
    return test_data, categories

def run_unsupervised_test(model_path, image_path, labels_path, backbone_type, top_k, output_dir):
    """èª¿ç”¨ unsupervised_test.py é€²è¡Œå–®å¼µåœ–ç‰‡æ¸¬è©¦"""
    cmd = [
        'python', 'unsupervised_test.py',
        '--model', model_path,
        '--image', image_path,
        '--labels', labels_path,
        '--backbone', backbone_type,
        '--top-k', str(top_k),
        '--output-dir', output_dir
    ]
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
        if result.returncode == 0:
            return True, result.stdout, result.stderr
        else:
            return False, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        return False, "", "Timeout"
    except Exception as e:
        return False, "", str(e)

def find_latest_report(output_dir):
    """æ‰¾åˆ°æœ€æ–°ç”Ÿæˆçš„æ¨è–¦å ±å‘Š"""
    report_files = []
    for file in os.listdir(output_dir):
        if file.startswith('unsupervised_recommendation_report_') and file.endswith('.json'):
            report_files.append(file)
    
    if not report_files:
        return None
    
    report_files.sort(reverse=True)
    return os.path.join(output_dir, report_files[0])

def find_model_files(models_dir, backbones):
    """è‡ªå‹•æ‰¾åˆ°å„backboneçš„æ¨¡å‹æª”æ¡ˆå’Œç‰¹å¾µæ•¸æ“šåº«"""
    model_configs = {}
    
    print(f"ğŸ“ åœ¨ç›®éŒ„ä¸­å°‹æ‰¾æ¨¡å‹å’Œç‰¹å¾µæª”æ¡ˆ: {models_dir}")
    
    all_files = os.listdir(models_dir)
    
    for backbone in backbones:
        model_file = None
        labels_file = None
        
        # å°‹æ‰¾æ¨¡å‹æª”æ¡ˆ
        for file in all_files:
            if f"best_model_{backbone}_" in file and file.endswith('.pth'):
                model_file = os.path.join(models_dir, file)
                break
        
        # å°‹æ‰¾å°æ‡‰çš„ç‰¹å¾µæ•¸æ“šåº«æª”æ¡ˆ
        possible_labels_files = [
            f"{backbone}_dataset_labels.json",
            f"dataset_labels_{backbone}.json",
            "dataset_labels.json"
        ]
        
        for labels_filename in possible_labels_files:
            if labels_filename in all_files:
                labels_file = os.path.join(models_dir, labels_filename)
                break
        
        if model_file and labels_file:
            model_configs[backbone] = {
                'model_path': model_file,
                'labels_path': labels_file
            }
            print(f"  âœ… {backbone}:")
            print(f"     æ¨¡å‹: {os.path.basename(model_file)}")
            print(f"     ç‰¹å¾µ: {os.path.basename(labels_file)}")
        else:
            missing = []
            if not model_file:
                missing.append("æ¨¡å‹æª”æ¡ˆ")
            if not labels_file:
                missing.append("ç‰¹å¾µæª”æ¡ˆ")
            print(f"  âŒ {backbone}: æ‰¾ä¸åˆ° {', '.join(missing)}")
    
    return model_configs

def evaluate_single_backbone(backbone_type, model_path, labels_path, test_data, top_k, temp_output_dir):
    """è©•ä¼°å–®å€‹backbone - ä½¿ç”¨æ­£ç¢ºçš„æº–ç¢ºç‡è¨ˆç®—æ–¹å¼"""
    print(f"\n{'='*60}")
    print(f"ğŸ”§ è©•ä¼° Backbone: {backbone_type}")
    print(f"ğŸ“‚ æ¨¡å‹: {os.path.basename(model_path)}")
    print(f"ğŸ“ ç‰¹å¾µåº«: {os.path.basename(labels_path)}")
    print(f"{'='*60}")
    
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨: {model_path}")
        return None
    
    if not os.path.exists(labels_path):
        print(f"âŒ ç‰¹å¾µæ•¸æ“šåº«ä¸å­˜åœ¨: {labels_path}")
        return None
    
    results = {
        'backbone': backbone_type,
        'model_path': model_path,
        'labels_path': labels_path,
        'total_test_images': len(test_data),
        'total_predictions': 0,  # ç¸½é æ¸¬æ¬¡æ•¸ (æ‡‰è©²æ˜¯ test_images * top_k)
        'correct_predictions': 0,  # æ­£ç¢ºé æ¸¬æ¬¡æ•¸
        'failed_tests': 0,
        'category_results': defaultdict(lambda: {
            'total_predictions': 0,  # è©²é¡åˆ¥çš„ç¸½é æ¸¬æ¬¡æ•¸
            'correct_predictions': 0,  # è©²é¡åˆ¥çš„æ­£ç¢ºé æ¸¬æ¬¡æ•¸
            'test_images': 0  # è©²é¡åˆ¥çš„æ¸¬è©¦åœ–ç‰‡æ•¸
        }),
        'detailed_results': []
    }
    
    # ç‚ºæ¯å¼µæ¸¬è©¦åœ–ç‰‡é€²è¡Œé æ¸¬
    for i, test_item in enumerate(test_data):
        print(f"  æ¸¬è©¦ {i+1}/{len(test_data)}: {test_item['filename']}")
        
        # èª¿ç”¨ unsupervised_test.py
        success, stdout, stderr = run_unsupervised_test(
            model_path, test_item['image_path'], labels_path, 
            backbone_type, top_k, temp_output_dir
        )
        
        if not success:
            print(f"    âŒ æ¸¬è©¦å¤±æ•—: {stderr}")
            results['failed_tests'] += 1
            continue
        
        # è®€å–ç”Ÿæˆçš„å ±å‘Š
        report_path = find_latest_report(temp_output_dir)
        if not report_path:
            print(f"    âŒ æ‰¾ä¸åˆ°å ±å‘Šæª”æ¡ˆ")
            results['failed_tests'] += 1
            continue
        
        try:
            with open(report_path, 'r', encoding='utf-8') as f:
                report = json.load(f)
            
            # åˆ†æçµæœ
            similar_images = report.get('similar_images', [])
            test_category = test_item['category']
            
            # æ–°çš„è¨ˆç®—æ–¹å¼ï¼šè¨ˆç®—Top-Kä¸­æ¯å¼µåœ–ç‰‡
            similar_categories = []
            correct_count = 0  # é€™å¼µæ¸¬è©¦åœ–ç‰‡è²¢ç»çš„æ­£ç¢ºé æ¸¬æ•¸
            
            # ç¢ºä¿æˆ‘å€‘è™•ç†å®Œæ•´çš„top_kå€‹çµæœ
            processed_count = 0
            for sim_img in similar_images:
                if processed_count >= top_k:
                    break
                    
                # å¾è·¯å¾‘ä¸­æå–é¡åˆ¥
                img_path = sim_img.get('image_path', '')
                path_parts = img_path.split('/')
                if len(path_parts) >= 2:
                    folder_name = path_parts[-2]  # è³‡æ–™å¤¾åç¨±å°±æ˜¯é¡åˆ¥
                    similar_categories.append(folder_name)
                    
                    # æ¯å¼µç›¸ä¼¼åœ–ç‰‡éƒ½æ˜¯ä¸€æ¬¡é æ¸¬
                    results['total_predictions'] += 1
                    results['category_results'][test_category]['total_predictions'] += 1
                    processed_count += 1
                    
                    # å¦‚æœé æ¸¬æ­£ç¢º
                    if folder_name == test_category:
                        correct_count += 1
                        results['correct_predictions'] += 1
                        results['category_results'][test_category]['correct_predictions'] += 1
            
            # å¦‚æœè¿”å›çš„ç›¸ä¼¼åœ–ç‰‡å°‘æ–¼top_kï¼Œè£œè¶³è¨ˆæ•¸
            if processed_count < top_k:
                missing_predictions = top_k - processed_count
                results['total_predictions'] += missing_predictions
                results['category_results'][test_category]['total_predictions'] += missing_predictions
                print(f"    âš ï¸ åªè¿”å›äº† {processed_count}/{top_k} å¼µç›¸ä¼¼åœ–ç‰‡")
            
            # çµ±è¨ˆæ¸¬è©¦åœ–ç‰‡æ•¸
            results['category_results'][test_category]['test_images'] += 1
            
            # é¡¯ç¤ºçµæœ
            if correct_count > 0:
                print(f"    âœ… {correct_count}/{top_k} æ­£ç¢º (Top-{top_k}: {similar_categories})")
            else:
                print(f"    âŒ 0/{top_k} æ­£ç¢º (Top-{top_k}: {similar_categories})")
            
            # è©³ç´°çµæœ
            results['detailed_results'].append({
                'test_image': test_item['filename'],
                'test_category': test_category,
                'similar_categories': similar_categories,
                'correct_count': correct_count,
                'total_predictions': len(similar_categories),
                'similarities': [img.get('similarity_score', 0) for img in similar_images]
            })
            
            # æ¸…ç†è‡¨æ™‚å ±å‘Š
            os.remove(report_path)
            
        except Exception as e:
            print(f"    âŒ è§£æå ±å‘Šå¤±æ•—: {e}")
            results['failed_tests'] += 1
    
    # è¨ˆç®—æº–ç¢ºç‡
    if results['total_predictions'] > 0:
        results['accuracy'] = results['correct_predictions'] / results['total_predictions']
    else:
        results['accuracy'] = 0.0
    
    # è¨ˆç®—å„é¡åˆ¥æº–ç¢ºç‡
    results['category_accuracies'] = {}
    for category, stats in results['category_results'].items():
        if stats['total_predictions'] > 0:
            results['category_accuracies'][category] = stats['correct_predictions'] / stats['total_predictions']
        else:
            results['category_accuracies'][category] = 0.0
    
    print(f"ğŸ¯ {backbone_type} è©•ä¼°å®Œæˆ:")
    print(f"  ç¸½æº–ç¢ºç‡: {results['accuracy']:.4f} ({results['correct_predictions']}/{results['total_predictions']})")
    print(f"  æ¸¬è©¦åœ–ç‰‡: {results['total_test_images']}, å¤±æ•—: {results['failed_tests']}")
    
    return results

def generate_comprehensive_csv_reports_and_visualizations(all_results, output_dir, top_k):
    """ç”Ÿæˆè©³ç´°çš„CSVå ±å‘Šå’Œè¦–è¦ºåŒ–åœ–è¡¨ - ä½¿ç”¨æ­£ç¢ºçš„è¨ˆç®—æ–¹å¼"""
    print(f"\nğŸ“Š ç”ŸæˆCSVå ±å‘Šå’Œè¦–è¦ºåŒ–...")
    
    # éæ¿¾æœ‰æ•ˆçµæœ
    valid_results = [r for r in all_results if r is not None]
    if not valid_results:
        print("âŒ æ²’æœ‰æœ‰æ•ˆçš„è©•ä¼°çµæœ")
        return None
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # æ”¶é›†æ‰€æœ‰é¡åˆ¥
    all_categories = set()
    for result in valid_results:
        all_categories.update(result['category_accuracies'].keys())
    all_categories = sorted(list(all_categories))
    
    print("ğŸ“ ç”Ÿæˆè©³ç´°çµ±è¨ˆå ±å‘Š...")
    
    # === 1. è©³ç´°é æ¸¬çµæœ CSV ===
    detailed_data = []
    for result in valid_results:
        for category in all_categories:
            if category in result['category_results']:
                stats = result['category_results'][category]
                accuracy = result['category_accuracies'].get(category, 0.0)
                
                detailed_data.append({
                    'Backbone': result['backbone'],
                    'Category': category,
                    'Test_Images': stats['test_images'],
                    'Total_Predictions': stats['total_predictions'],
                    'Correct_Predictions': stats['correct_predictions'],
                    'Accuracy': accuracy,
                    'Percentage': f"{accuracy*100:.2f}%",
                    'Expected_Predictions': stats['test_images'] * top_k,
                    'Prediction_Rate': stats['total_predictions'] / (stats['test_images'] * top_k) if stats['test_images'] > 0 else 0
                })
    
    detailed_df = pd.DataFrame(detailed_data)
    detailed_csv_path = os.path.join(output_dir, f'detailed_predictions_{timestamp}.csv')
    detailed_df.to_csv(detailed_csv_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ“„ è©³ç´°é æ¸¬çµæœ: {detailed_csv_path}")
    
    # === 2. Backboneæ•´é«”çµ±è¨ˆ CSV ===
    backbone_summary = []
    for result in valid_results:
        expected_predictions = (result['total_test_images'] - result['failed_tests']) * top_k
        
        backbone_summary.append({
            'Backbone': result['backbone'],
            'Overall_Accuracy': result['accuracy'],
            'Correct_Predictions': result['correct_predictions'],
            'Total_Predictions': result['total_predictions'],
            'Expected_Predictions': expected_predictions,
            'Test_Images': result['total_test_images'],
            'Failed_Tests': result['failed_tests'],
            'Success_Rate': result['total_predictions'] / expected_predictions if expected_predictions > 0 else 0,
            'Percentage': f"{result['accuracy']*100:.2f}%"
        })
    
    backbone_summary.sort(key=lambda x: x['Overall_Accuracy'], reverse=True)
    backbone_df = pd.DataFrame(backbone_summary)
    backbone_csv_path = os.path.join(output_dir, f'backbone_summary_{timestamp}.csv')
    backbone_df.to_csv(backbone_csv_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ“„ Backboneæ•´é«”çµ±è¨ˆ: {backbone_csv_path}")
    
    # === 3. é¡åˆ¥æ•´é«”çµ±è¨ˆ CSV ===
    category_summary = []
    for category in all_categories:
        total_correct = 0
        total_predictions = 0
        total_test_images = 0
        backbone_accuracies = []
        
        for result in valid_results:
            if category in result['category_results']:
                stats = result['category_results'][category]
                total_correct += stats['correct_predictions']
                total_predictions += stats['total_predictions']
                total_test_images += stats['test_images']
                
                if stats['total_predictions'] > 0:
                    backbone_accuracies.append(stats['correct_predictions'] / stats['total_predictions'])
        
        overall_accuracy = total_correct / total_predictions if total_predictions > 0 else 0.0
        avg_backbone_accuracy = np.mean(backbone_accuracies) if backbone_accuracies else 0.0
        std_backbone_accuracy = np.std(backbone_accuracies) if len(backbone_accuracies) > 1 else 0.0
        expected_predictions = total_test_images * top_k
        
        category_summary.append({
            'Category': category,
            'Overall_Accuracy': overall_accuracy,
            'Total_Correct': total_correct,
            'Total_Predictions': total_predictions,
            'Expected_Predictions': expected_predictions,
            'Test_Images': total_test_images,
            'Avg_Backbone_Accuracy': avg_backbone_accuracy,
            'Std_Backbone_Accuracy': std_backbone_accuracy,
            'Tested_Backbones': len(backbone_accuracies),
            'Percentage': f"{overall_accuracy*100:.2f}%",
            'Prediction_Rate': total_predictions / expected_predictions if expected_predictions > 0 else 0
        })
    
    category_summary.sort(key=lambda x: x['Overall_Accuracy'], reverse=True)
    category_df = pd.DataFrame(category_summary)
    category_csv_path = os.path.join(output_dir, f'category_summary_{timestamp}.csv')
    category_df.to_csv(category_csv_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ“„ é¡åˆ¥æ•´é«”çµ±è¨ˆ: {category_csv_path}")
    
    # === 4. ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨ ===
    print("ğŸ¨ ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨...")
    
    # è¨­ç½®åœ–è¡¨æ¨£å¼
    plt.style.use('default')
    sns.set_palette("husl")
    
    # åœ–è¡¨1: ç¶œåˆè©•ä¼°çµæœ (2x2)
    fig, axes = plt.subplots(2, 2, figsize=(20, 16))
    fig.suptitle(f'Unsupervised Learning Evaluation Results (Top-{top_k})\næ–°è¨ˆç®—æ–¹å¼: æ­£ç¢ºé æ¸¬æ•¸ / ç¸½é æ¸¬æ•¸ (ç¸½é æ¸¬æ•¸ = åœ–ç‰‡æ•¸ Ã— {top_k})', 
                 fontsize=16, fontweight='bold')
    
    # å­åœ–1: Backboneæº–ç¢ºç‡æ¯”è¼ƒ
    backbones = [item['Backbone'] for item in backbone_summary]
    accuracies = [item['Overall_Accuracy'] for item in backbone_summary]
    
    bars1 = axes[0,0].bar(backbones, accuracies, color='steelblue', alpha=0.8, edgecolor='navy', linewidth=1)
    axes[0,0].set_title('Backbone Overall Accuracy Comparison', fontsize=14, fontweight='bold')
    axes[0,0].set_xlabel('Backbone Architecture', fontsize=12)
    axes[0,0].set_ylabel('Accuracy (Correct/Total Predictions)', fontsize=12)
    axes[0,0].set_ylim(0, max(accuracies) * 1.1 if accuracies else 1)
    axes[0,0].tick_params(axis='x', rotation=45)
    axes[0,0].grid(True, alpha=0.3)
    
    for bar, acc in zip(bars1, accuracies):
        axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(accuracies)*0.01, 
                      f'{acc:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    # å­åœ–2: é¡åˆ¥æº–ç¢ºç‡åˆ†å¸ƒ (å‰15å)
    top_categories = category_summary[:15]
    categories = [item['Category'] for item in top_categories]
    cat_accuracies = [item['Overall_Accuracy'] for item in top_categories]
    
    bars2 = axes[0,1].bar(categories, cat_accuracies, color='forestgreen', alpha=0.8, edgecolor='darkgreen', linewidth=1)
    axes[0,1].set_title('Category Overall Accuracy (Top 15)', fontsize=14, fontweight='bold')
    axes[0,1].set_xlabel('Category', fontsize=12)
    axes[0,1].set_ylabel('Accuracy', fontsize=12)
    axes[0,1].set_ylim(0, max(cat_accuracies) * 1.1 if cat_accuracies else 1)
    axes[0,1].tick_params(axis='x', rotation=45)
    axes[0,1].grid(True, alpha=0.3)
    
    for bar, acc in zip(bars2, cat_accuracies):
        axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(cat_accuracies)*0.01, 
                      f'{acc:.2f}', ha='center', va='bottom', fontsize=9)
    
    # å­åœ–3: é æ¸¬æ•¸é‡çµ±è¨ˆ
    prediction_data = []
    for item in backbone_summary:
        prediction_data.append({
            'Backbone': item['Backbone'],
            'Correct': item['Correct_Predictions'],
            'Incorrect': item['Total_Predictions'] - item['Correct_Predictions']
        })
    
    pred_df = pd.DataFrame(prediction_data)
    pred_df.set_index('Backbone')[['Correct', 'Incorrect']].plot(kind='bar', stacked=True, ax=axes[1,0], 
                                                                  color=['lightgreen', 'lightcoral'])
    axes[1,0].set_title('Prediction Count Breakdown by Backbone', fontsize=14, fontweight='bold')
    axes[1,0].set_xlabel('Backbone', fontsize=12)
    axes[1,0].set_ylabel('Number of Predictions', fontsize=12)
    axes[1,0].tick_params(axis='x', rotation=45)
    axes[1,0].legend(['Correct', 'Incorrect'])
    axes[1,0].grid(True, alpha=0.3)
    
    # å­åœ–4: æº–ç¢ºç‡vsé æ¸¬ç‡æ•£é»åœ–
    accuracy_vs_rate = []
    for result in valid_results:
        for category in all_categories:
            if category in result['category_results']:
                stats = result['category_results'][category]
                if stats['test_images'] > 0:
                    accuracy = stats['correct_predictions'] / stats['total_predictions'] if stats['total_predictions'] > 0 else 0
                    prediction_rate = stats['total_predictions'] / (stats['test_images'] * top_k)
                    accuracy_vs_rate.append({
                        'Accuracy': accuracy,
                        'Prediction_Rate': prediction_rate,
                        'Backbone': result['backbone'],
                        'Category': category
                    })
    
    if accuracy_vs_rate:
        rate_df = pd.DataFrame(accuracy_vs_rate)
        for backbone in backbones:
            backbone_data = rate_df[rate_df['Backbone'] == backbone]
            axes[1,1].scatter(backbone_data['Prediction_Rate'], backbone_data['Accuracy'], 
                            label=backbone, alpha=0.7, s=50)
        
        axes[1,1].set_title('Accuracy vs Prediction Rate', fontsize=14, fontweight='bold')
        axes[1,1].set_xlabel('Prediction Rate (Actual/Expected Predictions)', fontsize=12)
        axes[1,1].set_ylabel('Accuracy', fontsize=12)
        axes[1,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
        axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plot_path1 = os.path.join(output_dir, f'comprehensive_evaluation_{timestamp}.png')
    plt.savefig(plot_path1, dpi=300, bbox_inches='tight')
    plt.close()
    
    # åœ–è¡¨2: è©³ç´°ç†±åŠ›åœ–åˆ†æ
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 10))
    fig.suptitle(f'Detailed Accuracy Analysis (Top-{top_k})', fontsize=16, fontweight='bold')
    
    # å·¦åœ–: Backbone vs Category ç†±åŠ›åœ–
    heatmap_data = detailed_df.pivot(index='Category', columns='Backbone', values='Accuracy')
    
    sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdYlBu_r', 
                ax=ax1, cbar_kws={'label': 'Accuracy'}, linewidths=0.5)
    ax1.set_title('Backbone vs Category Accuracy Heatmap', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Backbone', fontsize=12)
    ax1.set_ylabel('Category', fontsize=12)
    
    # å³åœ–: é æ¸¬æ•¸é‡ç†±åŠ›åœ–
    prediction_heatmap = detailed_df.pivot(index='Category', columns='Backbone', values='Total_Predictions')
    
    sns.heatmap(prediction_heatmap, annot=True, fmt='d', cmap='Blues', 
                ax=ax2, cbar_kws={'label': 'Total Predictions'}, linewidths=0.5)
    ax2.set_title('Total Predictions Count Heatmap', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Backbone', fontsize=12)
    ax2.set_ylabel('Category', fontsize=12)
    
    plt.tight_layout()
    plot_path2 = os.path.join(output_dir, f'detailed_heatmap_analysis_{timestamp}.png')
    plt.savefig(plot_path2, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ¨ è¦–è¦ºåŒ–åœ–è¡¨å·²ç”Ÿæˆ:")
    print(f"  ğŸ“Š ç¶œåˆè©•ä¼°çµæœ: {plot_path1}")
    print(f"  ğŸ”¥ è©³ç´°ç†±åŠ›åœ–åˆ†æ: {plot_path2}")
    
    return {
        'detailed_csv': detailed_csv_path,
        'backbone_csv': backbone_csv_path,
        'category_csv': category_csv_path,
        'plot1': plot_path1,
        'plot2': plot_path2,
        'backbone_summary': backbone_summary,
        'category_summary': category_summary
    }

def create_comprehensive_log(all_results, output_dir, top_k):
    """å‰µå»ºè©³ç´°çš„æ—¥èªŒæ–‡ä»¶ - ä½¿ç”¨æ­£ç¢ºçš„è¨ˆç®—æ–¹å¼"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_path = os.path.join(output_dir, f'evaluation_log_{timestamp}.txt')
    
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("ç„¡ç›£ç£å­¸ç¿’ Backbone è©•ä¼°è©³ç´°æ—¥èªŒ\n")
        f.write("æ­£ç¢ºè¨ˆç®—æ–¹å¼: æº–ç¢ºç‡ = æ­£ç¢ºé æ¸¬æ•¸ / ç¸½é æ¸¬æ•¸\n")
        f.write(f"ç¸½é æ¸¬æ•¸ = æ¸¬è©¦åœ–ç‰‡æ•¸ Ã— Top-{top_k}\n")
        f.write("=" * 80 + "\n")
        f.write(f"è©•ä¼°æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Top-K è¨­å®š: {top_k}\n")
        f.write(f"è©•ä¼°çš„ Backbone æ•¸é‡: {len([r for r in all_results if r is not None])}\n")
        f.write("\n")
        
        valid_results = [r for r in all_results if r is not None]
        
        if not valid_results:
            f.write("âŒ æ²’æœ‰æœ‰æ•ˆçš„è©•ä¼°çµæœ\n")
            return log_path
        
        # 1. Backbone æ•´é«”æº–ç¢ºç‡æ’å
        f.write("ğŸ† BACKBONE æ•´é«”æº–ç¢ºç‡æ’å:\n")
        f.write("-" * 80 + "\n")
        f.write("   æ’å  Backbone               æº–ç¢ºç‡    æ­£ç¢ºé æ¸¬/ç¸½é æ¸¬     æ¸¬è©¦åœ–ç‰‡  å¤±æ•—\n")
        f.write("-" * 80 + "\n")
        
        backbone_summary = []
        for result in valid_results:
            backbone_summary.append({
                'backbone': result['backbone'],
                'accuracy': result['accuracy'],
                'correct': result['correct_predictions'],
                'total': result['total_predictions'],
                'test_images': result['total_test_images'],
                'failed': result['failed_tests']
            })
        
        backbone_summary.sort(key=lambda x: x['accuracy'], reverse=True)
        
        for i, item in enumerate(backbone_summary, 1):
            f.write(f"   {i:2d}.   {item['backbone']:20s}   {item['accuracy']:7.4f}   "
                   f"{item['correct']:4d}/{item['total']:4d}        {item['test_images']:3d}      {item['failed']:2d}\n")
        
        f.write("\n")
        
        # 2. é¡åˆ¥æ•´é«”æº–ç¢ºç‡æ’å
        f.write("ğŸ“Š é¡åˆ¥æ•´é«”æº–ç¢ºç‡æ’å:\n")
        f.write("-" * 80 + "\n")
        f.write("   æ’å  é¡åˆ¥                   æº–ç¢ºç‡    æ­£ç¢ºé æ¸¬/ç¸½é æ¸¬     æ¸¬è©¦åœ–ç‰‡\n")
        f.write("-" * 80 + "\n")
        
        # æ”¶é›†æ‰€æœ‰é¡åˆ¥
        all_categories = set()
        for result in valid_results:
            all_categories.update(result['category_accuracies'].keys())
        all_categories = sorted(list(all_categories))
        
        category_summary = []
        for category in all_categories:
            total_correct = 0
            total_predictions = 0
            total_test_images = 0
            
            for result in valid_results:
                if category in result['category_results']:
                    stats = result['category_results'][category]
                    total_correct += stats['correct_predictions']
                    total_predictions += stats['total_predictions']
                    total_test_images += stats['test_images']
            
            overall_accuracy = total_correct / total_predictions if total_predictions > 0 else 0.0
            
            category_summary.append({
                'category': category,
                'overall_accuracy': overall_accuracy,
                'total_correct': total_correct,
                'total_predictions': total_predictions,
                'total_test_images': total_test_images
            })
        
        category_summary.sort(key=lambda x: x['overall_accuracy'], reverse=True)
        
        for i, item in enumerate(category_summary, 1):
            f.write(f"   {i:2d}.   {item['category']:20s}   {item['overall_accuracy']:7.4f}   "
                   f"{item['total_correct']:4d}/{item['total_predictions']:4d}        {item['total_test_images']:3d}\n")
        
        f.write("\n")
        
        # 3. æ¯å€‹ Backbone çš„è©³ç´°é¡åˆ¥è¡¨ç¾
        f.write("ğŸ“‹ æ¯å€‹ BACKBONE çš„è©³ç´°é¡åˆ¥è¡¨ç¾:\n")
        f.write("=" * 80 + "\n")
        
        for result in backbone_summary:
            backbone = result['backbone']
            backbone_result = next(r for r in valid_results if r['backbone'] == backbone)
            
            f.write(f"\nğŸ”§ {backbone.upper()}:\n")
            f.write("-" * 50 + "\n")
            f.write(f"  æ•´é«”æº–ç¢ºç‡: {result['accuracy']:.4f} ({result['correct']}/{result['total']})\n")
            f.write(f"  æ¸¬è©¦åœ–ç‰‡: {result['test_images']}, å¤±æ•—æ¸¬è©¦: {result['failed']}\n")
            f.write(f"  é æœŸç¸½é æ¸¬æ•¸: {(result['test_images'] - result['failed']) * top_k}\n")
            f.write("\n  å„é¡åˆ¥è©³ç´°è¡¨ç¾:\n")
            f.write("    é¡åˆ¥                  æº–ç¢ºç‡    æ­£ç¢º/ç¸½é æ¸¬  æ¸¬è©¦åœ–ç‰‡\n")
            f.write("    " + "-" * 55 + "\n")
            
            # é¡åˆ¥è¡¨ç¾æ’åº
            category_perfs = []
            for category in all_categories:
                if category in backbone_result['category_results']:
                    stats = backbone_result['category_results'][category]
                    accuracy = backbone_result['category_accuracies'][category]
                    category_perfs.append((category, accuracy, stats['correct_predictions'], 
                                         stats['total_predictions'], stats['test_images']))
            
            category_perfs.sort(key=lambda x: x[1], reverse=True)
            
            for category, accuracy, correct, total, test_imgs in category_perfs:
                f.write(f"    {category:20s}  {accuracy:7.4f}   {correct:3d}/{total:3d}      {test_imgs:2d}\n")
        
        # 4. çµ±è¨ˆç¸½çµ
        f.write("\n" + "=" * 80 + "\n")
        f.write("ğŸ“ˆ çµ±è¨ˆç¸½çµ:\n")
        f.write("-" * 50 + "\n")
        
        all_accuracies = [item['accuracy'] for item in backbone_summary]
        total_test_images = sum(item['test_images'] for item in backbone_summary) // len(backbone_summary)
        total_expected_predictions = total_test_images * top_k * len(backbone_summary)
        total_actual_predictions = sum(item['total'] for item in backbone_summary)
        total_correct_predictions = sum(item['correct'] for item in backbone_summary)
        
        f.write(f"  æ¸¬è©¦è¨­å®š:\n")
        f.write(f"    Top-K: {top_k}\n")
        f.write(f"    æ¸¬è©¦åœ–ç‰‡æ•¸: {total_test_images}\n")
        f.write(f"    è©•ä¼°çš„ Backbone æ•¸: {len(backbone_summary)}\n")
        f.write(f"    é æœŸç¸½é æ¸¬æ•¸: {total_expected_predictions}\n")
        f.write(f"    å¯¦éš›ç¸½é æ¸¬æ•¸: {total_actual_predictions}\n")
        f.write(f"    ç¸½æ­£ç¢ºé æ¸¬æ•¸: {total_correct_predictions}\n")
        f.write(f"\n")
        
        f.write(f"  Backbone è¡¨ç¾çµ±è¨ˆ:\n")
        f.write(f"    æœ€é«˜æº–ç¢ºç‡: {max(all_accuracies):.4f} ({backbone_summary[0]['backbone']})\n")
        f.write(f"    æœ€ä½æº–ç¢ºç‡: {min(all_accuracies):.4f} ({backbone_summary[-1]['backbone']})\n")
        f.write(f"    å¹³å‡æº–ç¢ºç‡: {np.mean(all_accuracies):.4f}\n")
        f.write(f"    æ¨™æº–å·®: {np.std(all_accuracies):.4f}\n")
        f.write(f"\n")
        
        all_cat_accuracies = [item['overall_accuracy'] for item in category_summary]
        f.write(f"  é¡åˆ¥è¡¨ç¾çµ±è¨ˆ:\n")
        f.write(f"    æœ€æ˜“åˆ†é¡åˆ¥: {category_summary[0]['category']} ({category_summary[0]['overall_accuracy']:.4f})\n")
        f.write(f"    æœ€é›£åˆ†é¡åˆ¥: {category_summary[-1]['category']} ({category_summary[-1]['overall_accuracy']:.4f})\n")
        f.write(f"    é¡åˆ¥å¹³å‡æº–ç¢ºç‡: {np.mean(all_cat_accuracies):.4f}\n")
        f.write(f"    é¡åˆ¥æº–ç¢ºç‡æ¨™æº–å·®: {np.std(all_cat_accuracies):.4f}\n")
        
        f.write(f"\n")
        f.write("=" * 80 + "\n")
        f.write("è¨ˆç®—æ–¹å¼èªªæ˜:\n")
        f.write("- æº–ç¢ºç‡ = æ­£ç¢ºé æ¸¬æ•¸ / ç¸½é æ¸¬æ•¸\n")
        f.write(f"- æ¯å¼µæ¸¬è©¦åœ–ç‰‡ç”¢ç”Ÿ {top_k} æ¬¡é æ¸¬\n")
        f.write("- æ¯æ¬¡é æ¸¬æ­£ç¢ºèˆ‡å¦ç¨ç«‹è¨ˆç®—\n")
        f.write("- ç¸½é æ¸¬æ•¸ = æ¸¬è©¦åœ–ç‰‡æ•¸ Ã— Top-K\n")
        f.write("=" * 80 + "\n")
        f.write("æ—¥èªŒç”Ÿæˆå®Œæˆ\n")
        f.write("=" * 80 + "\n")
    
    print(f"ğŸ“ è©³ç´°æ—¥èªŒå·²ç”Ÿæˆ: {log_path}")
    return log_path

def main():
    parser = argparse.ArgumentParser(description='ç„¡ç›£ç£å­¸ç¿’Backboneæ‰¹é‡è©•ä¼°ç³»çµ±ï¼ˆæ­£ç¢ºè¨ˆç®—ç‰ˆï¼‰')
    parser.add_argument('--testdata', type=str, required=True,
                       help='æ¸¬è©¦æ•¸æ“šç›®éŒ„è·¯å¾‘')
    parser.add_argument('--models-dir', type=str, required=True,
                       help='æ¨¡å‹å’Œç‰¹å¾µæ•¸æ“šåº«ç›®éŒ„è·¯å¾‘')
    parser.add_argument('--backbones', type=str, nargs='+', 
                       default=['mobilenet', 'resnet18', 'resnet50', 'efficientnet_b0', 
                               'efficientnet_b2', 'vit_tiny', 'vit_small', 'fashion_resnet'],
                       help='è¦è©•ä¼°çš„backboneåˆ—è¡¨')
    parser.add_argument('--top-k', type=int, default=5,
                       help='Top-Kç›¸ä¼¼åº¦æœå°‹')
    parser.add_argument('--output-dir', type=str, default='./correct_evaluation_results',
                       help='çµæœè¼¸å‡ºç›®éŒ„')
    
    args = parser.parse_args()
    
    print("ğŸš€ ç„¡ç›£ç£å­¸ç¿’Backboneæ‰¹é‡è©•ä¼°ç³»çµ±ï¼ˆæ­£ç¢ºè¨ˆç®—ç‰ˆï¼‰")
    print("   æº–ç¢ºç‡ = æ­£ç¢ºé æ¸¬æ•¸ / ç¸½é æ¸¬æ•¸ (ç¸½é æ¸¬æ•¸ = æ¸¬è©¦åœ–ç‰‡æ•¸ Ã— Top-K)")
    print("=" * 70)
    
    # æª¢æŸ¥å¿…è¦æª”æ¡ˆ
    if not os.path.exists('unsupervised_test.py'):
        print("âŒ æ‰¾ä¸åˆ° unsupervised_test.pyï¼Œè«‹ç¢ºä¿è©²æª”æ¡ˆåœ¨åŒä¸€ç›®éŒ„ä¸‹")
        return
    
    if not os.path.exists(args.testdata):
        print(f"âŒ æ¸¬è©¦æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: {args.testdata}")
        return
    
    if not os.path.exists(args.models_dir):
        print(f"âŒ æ¨¡å‹ç›®éŒ„ä¸å­˜åœ¨: {args.models_dir}")
        return
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    os.makedirs(args.output_dir, exist_ok=True)
    temp_output_dir = os.path.join(args.output_dir, 'temp_reports')
    os.makedirs(temp_output_dir, exist_ok=True)
    
    try:
        # è¼‰å…¥æ¸¬è©¦æ•¸æ“š
        test_data, categories = load_test_data_with_labels(args.testdata)
        
        # è‡ªå‹•æ‰¾åˆ°æ¨¡å‹æª”æ¡ˆ
        model_configs = find_model_files(args.models_dir, args.backbones)
        
        if not model_configs:
            print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æ¨¡å‹é…ç½®")
            return
        
        print(f"\nğŸ“Š è©•ä¼°è¨­ç½®:")
        print(f"  æ¸¬è©¦åœ–ç‰‡æ•¸é‡: {len(test_data)}")
        print(f"  é¡åˆ¥æ•¸é‡: {len(categories)}")
        print(f"  Top-K: {args.top_k}")
        print(f"  é æœŸç¸½é æ¸¬æ•¸ (æ¯å€‹backbone): {len(test_data) * args.top_k}")
        print(f"  æ‰¾åˆ°çš„æ¨¡å‹: {list(model_configs.keys())}")
        
        # è©•ä¼°æ¯å€‹backbone
        all_results = []
        for backbone in args.backbones:
            if backbone in model_configs:
                config = model_configs[backbone]
                result = evaluate_single_backbone(
                    backbone, config['model_path'], config['labels_path'],
                    test_data, args.top_k, temp_output_dir
                )
                all_results.append(result)
            else:
                print(f"\nâš ï¸ è·³é {backbone}: æ‰¾ä¸åˆ°æ¨¡å‹æˆ–ç‰¹å¾µæª”æ¡ˆ")
                all_results.append(None)
        
        # ç”Ÿæˆç¶œåˆå ±å‘Šå’Œè¦–è¦ºåŒ–
        summary_report = generate_comprehensive_csv_reports_and_visualizations(
            all_results, args.output_dir, args.top_k
        )
        
        # ç”Ÿæˆè©³ç´°æ—¥èªŒ
        log_path = create_comprehensive_log(all_results, args.output_dir, args.top_k)
        
        # æ¸…ç†è‡¨æ™‚ç›®éŒ„
        import shutil
        shutil.rmtree(temp_output_dir)
        
        # é¡¯ç¤ºæœ€çµ‚æ’å
        valid_results = [r for r in all_results if r is not None]
        if valid_results:
            print(f"\nğŸ† æœ€çµ‚ Backbone æ’å (Top-{args.top_k} ç›¸ä¼¼åº¦æœå°‹):")
            print("   (æº–ç¢ºç‡ = æ­£ç¢ºé æ¸¬æ•¸ / ç¸½é æ¸¬æ•¸)")
            print("-" * 60)
            
            backbone_rankings = []
            for result in valid_results:
                backbone_rankings.append({
                    'backbone': result['backbone'],
                    'accuracy': result['accuracy'],
                    'correct': result['correct_predictions'],
                    'total': result['total_predictions']
                })
            
            backbone_rankings.sort(key=lambda x: x['accuracy'], reverse=True)
            
            for i, item in enumerate(backbone_rankings, 1):
                print(f"  {i}. {item['backbone']:20s}: {item['accuracy']:.4f} "
                      f"({item['correct']:4d}/{item['total']:4d})")
        
        print(f"\nâœ… è©•ä¼°å®Œæˆï¼çµæœå·²ä¿å­˜åˆ°: {args.output_dir}")
        print(f"ğŸ“Š CSVå ±å‘Šã€è¦–è¦ºåŒ–åœ–è¡¨å’Œè©³ç´°æ—¥èªŒå·²ç”Ÿæˆ")
        
    except Exception as e:
        print(f"âŒ è©•ä¼°å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()