#!/usr/bin/env python3
"""
ç°¡åŒ–ç‰ˆç©¿æ­æ¨è–¦æ¨¡å‹è¨“ç·´ç³»çµ±
åªä½¿ç”¨ FashionCLIP æå–ç´”ç²¹çš„æ™‚å°šèªç¾©ç‰¹å¾µ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import os
import json
import numpy as np
from transformers import CLIPProcessor, CLIPModel
import psutil
import multiprocessing as mp
import gc
import argparse
import warnings
import time
from sklearn.metrics.pairwise import cosine_similarity

warnings.filterwarnings("ignore")

# ==================== Mac å„ªåŒ–è¨­ç½® ====================

def setup_mac_optimization():
    if mp.get_start_method(allow_none=True) != 'spawn':
        mp.set_start_method('spawn', force=True)
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        print("âœ… æª¢æ¸¬åˆ° Metal Performance Shaders (MPS) æ”¯æŒ")
        device = torch.device("mps")
        os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
        os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
        torch.set_num_threads(12)
        print(f"ğŸš€ M4 Pro å„ªåŒ–è¨­ç½®å·²å•Ÿç”¨")
    else:
        print("âš ï¸ MPS ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
        device = torch.device("cpu")
        torch.set_num_threads(min(8, mp.cpu_count()))
    
    return device

# ==================== ç°¡åŒ–é…ç½® ====================

class SimpleTrainingConfig:
    def __init__(self, config_type="balanced"):
        self.config_type = config_type
        self.style_categories = [
            'Artsy', 'Athleisure', 'BRITISH', 'CASUAL', 'GOTH',
            'Japanese', 'Kawaii', 'Korean', 'Preppy', 'STREET', 'Vintage'
        ]
        self.gender_categories = ['MEN', 'WOMEN']
        self.style_to_idx = {style: idx for idx, style in enumerate(self.style_categories)}
        self.gender_to_idx = {gender: idx for idx, gender in enumerate(self.gender_categories)}
        
        # ç°¡åŒ–çš„é…ç½®é¸é …
        if config_type == "minimal":
            self.batch_size = 8
            self.num_epochs = 5
            self.learning_rate = 0.001
            self.max_samples_per_class = 200
        elif config_type == "performance":
            self.batch_size = 32
            self.num_epochs = 15
            self.learning_rate = 0.0005
            self.max_samples_per_class = 2000
        else:  # balanced
            self.batch_size = 16
            self.num_epochs = 10
            self.learning_rate = 0.001
            self.max_samples_per_class = 800

# ==================== ç°¡åŒ–çš„ç‰¹å¾µæå– ====================

def is_valid_image(image):
    """æª¢æŸ¥åœ–ç‰‡æ˜¯å¦æœ‰æ•ˆ"""
    try:
        image.verify()
        image = image.copy()  # é‡æ–°è¼‰å…¥ï¼Œå› ç‚ºverify()æœƒæ¶ˆè€—åœ–ç‰‡
        pixels = np.array(image)
        if pixels.size == 0 or np.all(pixels == 0) or np.all(pixels == 255):
            return False
        return True
    except:
        return False

def generate_simple_dataset_labels(root_dir, config, device="mps"):
    """
    ç°¡åŒ–ç‰ˆæ•¸æ“šé›†æ¨™ç±¤ç”Ÿæˆ - åªä½¿ç”¨ FashionCLIP
    """
    print("ğŸ”§ è¼‰å…¥ FashionCLIP æ¨¡å‹...")
    clip_processor = CLIPProcessor.from_pretrained("patrickjohncyh/fashion-clip")
    clip_model = CLIPModel.from_pretrained("patrickjohncyh/fashion-clip").to(device)
    clip_model.eval()
    
    dataset_labels = []
    processed_count = 0
    
    print("ğŸ“Š é–‹å§‹æå–ç´” FashionCLIP ç‰¹å¾µ...")
    
    for style in config.style_categories:
        for gender in config.gender_categories:
            folder = os.path.join(root_dir, f"{style}_{gender}")
            if os.path.exists(folder):
                print(f"ğŸ“ è™•ç†è³‡æ–™å¤¾: {style}_{gender}")
                
                # é™åˆ¶æ¯å€‹é¡åˆ¥çš„æ¨£æœ¬æ•¸é‡
                image_files = [f for f in os.listdir(folder) 
                             if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]
                image_files = image_files[:config.max_samples_per_class]
                
                for img_name in image_files:
                    img_path = os.path.join(folder, img_name)
                    try:
                        # è¼‰å…¥ä¸¦é©—è­‰åœ–ç‰‡
                        image = Image.open(img_path).convert('RGB')
                        
                        if not is_valid_image(image.copy()):
                            continue
                        
                        # åªæå– FashionCLIP ç‰¹å¾µ - é€™æ˜¯é—œéµæ”¹é€²ï¼
                        clip_inputs = clip_processor(images=image, return_tensors="pt").to(device)
                        with torch.no_grad():
                            features = clip_model.get_image_features(**clip_inputs)
                            # æ¨™æº–åŒ–ç‰¹å¾µå‘é‡
                            features = F.normalize(features, p=2, dim=1)
                        
                        # ç¢ºä¿ç‰¹å¾µå‘é‡ç¶­åº¦æ­£ç¢º
                        if features.shape[1] != 512:
                            print(f"âš ï¸ ç‰¹å¾µå‘é‡ç¶­åº¦ç•°å¸¸: {features.shape}")
                            continue
                        
                        # åªä¿å­˜å¿…è¦ä¿¡æ¯
                        dataset_labels.append({
                            "path": img_path,
                            "style": style,
                            "gender": gender,
                            "style_idx": config.style_to_idx[style],
                            "gender_idx": config.gender_to_idx[gender],
                            "features": features.cpu().numpy().flatten().tolist()
                        })
                        
                        processed_count += 1
                        if processed_count % 100 == 0:
                            print(f"  å·²è™•ç† {processed_count} å¼µåœ–ç‰‡...")
                            
                    except Exception as e:
                        print(f"âš ï¸ è™•ç†åœ–ç‰‡å¤±æ•—: {img_path}, {e}")
                        continue
    
    print(f"âœ… æˆåŠŸè™•ç† {len(dataset_labels)} å¼µåœ–ç‰‡")
    
    # ä¿å­˜ç°¡åŒ–çš„æ¨™ç±¤æ–‡ä»¶
    output_file = "simple_dataset_labels.json"
    with open(output_file, "w") as f:
        json.dump(dataset_labels, f, indent=2)
    
    print(f"ğŸ’¾ ç°¡åŒ–æ¨™ç±¤å·²ä¿å­˜åˆ°: {output_file}")
    return dataset_labels

# ==================== ç°¡åŒ–çš„æ•¸æ“šé›†é¡ ====================

class SimpleOutfitDataset(Dataset):
    def __init__(self, labels_file="simple_dataset_labels.json"):
        with open(labels_file, "r") as f:
            self.labels = json.load(f)
        
        print(f"ğŸ“Š è¼‰å…¥ {len(self.labels)} å€‹æ¨£æœ¬")
        
        # ç”Ÿæˆæ›´æœ‰æ„ç¾©çš„æ¨£æœ¬å°
        self.pairs = []
        self._generate_meaningful_pairs()
        
        print(f"ğŸ”— ç”Ÿæˆ {len(self.pairs)} å€‹è¨“ç·´å°")
    
    def _generate_meaningful_pairs(self):
        """ç”Ÿæˆæ›´æœ‰æ„ç¾©çš„æ­£è² æ¨£æœ¬å°"""
        
        # æŒ‰æ€§åˆ¥å’Œé¢¨æ ¼çµ„ç¹”æ•¸æ“š
        organized_data = {}
        for sample in self.labels:
            key = f"{sample['gender']}_{sample['style']}"
            if key not in organized_data:
                organized_data[key] = []
            organized_data[key].append(sample)
        
        # ç”Ÿæˆæ­£æ¨£æœ¬ï¼šåŒæ€§åˆ¥åŒé¢¨æ ¼
        for key, samples in organized_data.items():
            if len(samples) >= 2:
                for i in range(len(samples)):
                    for j in range(i+1, min(i+6, len(samples))):  # é™åˆ¶æ­£æ¨£æœ¬æ•¸é‡
                        self.pairs.append((samples[i], samples[j], 1.0))
        
        # ç”Ÿæˆè² æ¨£æœ¬ï¼šåŒæ€§åˆ¥ä¸åŒé¢¨æ ¼
        gender_groups = {'MEN': [], 'WOMEN': []}
        for sample in self.labels:
            gender_groups[sample['gender']].append(sample)
        
        for gender, samples in gender_groups.items():
            np.random.shuffle(samples)
            for i in range(0, min(len(samples)-1, 1000), 2):  # é™åˆ¶è² æ¨£æœ¬æ•¸é‡
                if i+1 < len(samples) and samples[i]['style'] != samples[i+1]['style']:
                    self.pairs.append((samples[i], samples[i+1], 0.0))
    
    def __len__(self):
        return len(self.pairs)
    
    def __getitem__(self, idx):
        sample1, sample2, label = self.pairs[idx]
        
        features1 = torch.tensor(sample1['features'], dtype=torch.float32)
        features2 = torch.tensor(sample2['features'], dtype=torch.float32)
        
        return {
            'features1': features1,
            'features2': features2,
            'style1': sample1['style_idx'],
            'style2': sample2['style_idx'],
            'gender1': sample1['gender_idx'],
            'gender2': sample2['gender_idx'],
            'label': torch.tensor(label, dtype=torch.float32),
            'path1': sample1['path'],
            'path2': sample2['path']
        }

# ==================== ç°¡åŒ–çš„æ¨¡å‹ ====================

class SimpleFashionRecommender(nn.Module):
    def __init__(self, config):
        super(SimpleFashionRecommender, self).__init__()
        self.config = config
        
        # åªè¼‰å…¥ FashionCLIP
        self.clip_processor = CLIPProcessor.from_pretrained("patrickjohncyh/fashion-clip")
        self.clip_model = CLIPModel.from_pretrained("patrickjohncyh/fashion-clip")
        
        # å‡çµ CLIP åƒæ•¸
        for param in self.clip_model.parameters():
            param.requires_grad = False
        
        # ç°¡å–®çš„æ˜ å°„å±¤ - å­¸ç¿’æ™‚å°šæ­é…çš„èªç¾©é—œä¿‚
        self.fashion_projector = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 64)  # æœ€çµ‚çš„æ™‚å°šèªç¾©åµŒå…¥
        )
        
        # é¢¨æ ¼åˆ†é¡é ­ï¼ˆè¼”åŠ©ä»»å‹™ï¼‰
        self.style_classifier = nn.Linear(64, len(config.style_categories))
        
    def forward(self, features):
        """
        å‰å‘å‚³æ’­
        Args:
            features: é æå–çš„ FashionCLIP ç‰¹å¾µ [batch_size, 512]
        """
        # æ˜ å°„åˆ°æ™‚å°šèªç¾©ç©ºé–“
        fashion_embedding = self.fashion_projector(features)
        
        # é¢¨æ ¼é æ¸¬ï¼ˆè¼”åŠ©ä»»å‹™ï¼‰
        style_logits = self.style_classifier(fashion_embedding)
        
        return {
            'fashion_embedding': fashion_embedding,
            'style_logits': style_logits
        }
    
    def extract_image_features(self, image, device):
        """æå–å–®å¼µåœ–ç‰‡çš„ç‰¹å¾µ"""
        self.eval()
        with torch.no_grad():
            clip_inputs = self.clip_processor(images=image, return_tensors="pt").to(device)
            clip_features = self.clip_model.get_image_features(**clip_inputs)
            clip_features = F.normalize(clip_features, p=2, dim=1)
            
            outputs = self.forward(clip_features)
            return outputs['fashion_embedding'].cpu().numpy()

# ==================== ç°¡åŒ–çš„è¨“ç·´å™¨ ====================

class SimpleFashionTrainer:
    def __init__(self, model, config, device):
        self.model = model
        self.config = config
        self.device = device
        
        # åªè¨“ç·´æˆ‘å€‘çš„æ˜ å°„å±¤
        trainable_params = list(model.fashion_projector.parameters()) + \
                          list(model.style_classifier.parameters())
        
        self.optimizer = torch.optim.Adam(trainable_params, lr=config.learning_rate)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.7)
        self.best_loss = float('inf')
    
    def contrastive_loss(self, emb1, emb2, label, margin=1.0):
        """å°æ¯”å­¸ç¿’æå¤±"""
        distance = F.pairwise_distance(emb1, emb2, keepdim=True)
        loss = label * torch.pow(distance, 2) + \
               (1 - label) * torch.pow(torch.clamp(margin - distance, min=0.0), 2)
        return loss.mean()
    
    def style_classification_loss(self, style_logits, style_labels):
        """é¢¨æ ¼åˆ†é¡æå¤±ï¼ˆè¼”åŠ©ä»»å‹™ï¼‰"""
        return F.cross_entropy(style_logits, style_labels)
    
    def train_epoch(self, dataloader, epoch):
        self.model.train()
        total_loss = 0.0
        total_contrastive_loss = 0.0
        total_style_loss = 0.0
        num_batches = 0
        
        for batch_idx, batch in enumerate(dataloader):
            try:
                self.optimizer.zero_grad()
                
                features1 = batch['features1'].to(self.device)
                features2 = batch['features2'].to(self.device)
                style1 = batch['style1'].to(self.device)
                style2 = batch['style2'].to(self.device)
                label = batch['label'].to(self.device)
                
                # å‰å‘å‚³æ’­
                outputs1 = self.model(features1)
                outputs2 = self.model(features2)
                
                emb1 = outputs1['fashion_embedding']
                emb2 = outputs2['fashion_embedding']
                style_logits1 = outputs1['style_logits']
                style_logits2 = outputs2['style_logits']
                
                # å°æ¯”å­¸ç¿’æå¤± - ä¸»è¦ä»»å‹™
                contrastive_loss = self.contrastive_loss(emb1, emb2, label)
                
                # é¢¨æ ¼åˆ†é¡æå¤± - è¼”åŠ©ä»»å‹™
                style_loss = (self.style_classification_loss(style_logits1, style1) + 
                            self.style_classification_loss(style_logits2, style2)) / 2
                
                # ç¸½æå¤±
                total_batch_loss = contrastive_loss + 0.3 * style_loss
                
                # åå‘å‚³æ’­
                total_batch_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()
                
                total_loss += total_batch_loss.item()
                total_contrastive_loss += contrastive_loss.item()
                total_style_loss += style_loss.item()
                num_batches += 1
                
                if batch_idx % 20 == 0:
                    print(f"  æ‰¹æ¬¡ {batch_idx}/{len(dataloader)}")
                    print(f"    å°æ¯”æå¤±: {contrastive_loss.item():.4f}")
                    print(f"    é¢¨æ ¼æå¤±: {style_loss.item():.4f}")
                
                # è¨˜æ†¶é«”ç®¡ç†
                if self.device.type == 'mps':
                    torch.mps.empty_cache()
                
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡ {batch_idx} è¨“ç·´å¤±æ•—: {e}")
                continue
        
        avg_loss = total_loss / max(num_batches, 1)
        avg_contrastive = total_contrastive_loss / max(num_batches, 1)
        avg_style = total_style_loss / max(num_batches, 1)
        
        return {
            'total_loss': avg_loss,
            'contrastive_loss': avg_contrastive,
            'style_loss': avg_style
        }
    
    def save_checkpoint(self, epoch, losses, save_path):
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'losses': losses,
            'config': self.config.__dict__
        }
        torch.save(checkpoint, save_path)
        print(f"ğŸ’¾ æª¢æŸ¥é»å·²ä¿å­˜: {save_path}")
        
        if losses['total_loss'] < self.best_loss:
            self.best_loss = losses['total_loss']
            best_path = save_path.replace('.pth', '_best.pth')
            torch.save(checkpoint, best_path)
            print(f"ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_path}")

# ==================== ä¸»è¨“ç·´å‡½æ•¸ ====================

def train_simple_model(data_root, config_type="balanced", resume_from=None):
    """
    ç°¡åŒ–ç‰ˆæ¨¡å‹è¨“ç·´
    """
    device = setup_mac_optimization()
    config = SimpleTrainingConfig(config_type)
    
    print(f"ğŸš€ é–‹å§‹ç°¡åŒ–ç‰ˆç©¿æ­æ¨è–¦æ¨¡å‹è¨“ç·´")
    print(f"ğŸ“Š é…ç½®: {config_type}")
    print(f"ğŸ¯ è¨­å‚™: {device}")
    print(f"ğŸ”§ ç‰¹è‰²: ç´” FashionCLIP èªç¾©ç‰¹å¾µ")
    
    # æª¢æŸ¥ä¸¦ç”Ÿæˆç°¡åŒ–çš„æ•¸æ“šé›†æ¨™ç±¤
    labels_file = "simple_dataset_labels.json"
    should_regenerate = False
    
    if os.path.exists(labels_file):
        # æª¢æŸ¥ç¾æœ‰æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        try:
            with open(labels_file, 'r') as f:
                existing_labels = json.load(f)
            
            if not existing_labels or len(existing_labels) == 0:
                print("âš ï¸ ç¾æœ‰æ¨™ç±¤æ–‡ä»¶ç‚ºç©ºï¼Œéœ€è¦é‡æ–°ç”Ÿæˆ")
                should_regenerate = True
            else:
                # æª¢æŸ¥æ¨™ç±¤æ˜¯å¦åŒ¹é…ç•¶å‰é¢¨æ ¼é¡åˆ¥
                existing_styles = set(label.get('style', '') for label in existing_labels)
                expected_styles = set(config.style_categories)
                
                if not existing_styles.intersection(expected_styles):
                    print("âš ï¸ ç¾æœ‰æ¨™ç±¤èˆ‡ç•¶å‰é¢¨æ ¼é¡åˆ¥ä¸åŒ¹é…ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆ")
                    print(f"   ç¾æœ‰é¢¨æ ¼: {existing_styles}")
                    print(f"   æœŸæœ›é¢¨æ ¼: {expected_styles}")
                    should_regenerate = True
                else:
                    print(f"âœ… ä½¿ç”¨ç¾æœ‰ç°¡åŒ–æ•¸æ“šé›†æ¨™ç±¤ ({len(existing_labels)} å€‹æ¨£æœ¬)")
                    
        except (json.JSONDecodeError, KeyError) as e:
            print(f"âš ï¸ ç¾æœ‰æ¨™ç±¤æ–‡ä»¶æ ¼å¼éŒ¯èª¤: {e}")
            should_regenerate = True
    else:
        should_regenerate = True
    
    if should_regenerate:
        print("ğŸ“ ç”Ÿæˆç°¡åŒ–æ•¸æ“šé›†æ¨™ç±¤...")
        # åˆªé™¤èˆŠæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if os.path.exists(labels_file):
            os.remove(labels_file)
        generate_simple_dataset_labels(data_root, config, device)
    
    # å‰µå»ºæ•¸æ“šé›†å’Œæ•¸æ“šè¼‰å…¥å™¨
    try:
        dataset = SimpleOutfitDataset()
        
        if len(dataset) == 0:
            print("âŒ æ•¸æ“šé›†ç‚ºç©ºï¼è«‹æª¢æŸ¥ï¼š")
            print(f"   1. æ•¸æ“šé›†è·¯å¾‘æ˜¯å¦æ­£ç¢º: {data_root}")
            print(f"   2. è³‡æ–™å¤¾å‘½åæ˜¯å¦æ­£ç¢º (ä¾‹: Artsy_MEN, Artsy_WOMEN)")
            print(f"   3. åœ–ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
            return
            
        print(f"ğŸ“¦ æ•¸æ“šé›†å¤§å°: {len(dataset)} å€‹æ¨£æœ¬å°")
        
        dataloader = DataLoader(
            dataset, 
            batch_size=config.batch_size, 
            shuffle=True, 
            num_workers=0,  # ç°¡åŒ–ç‚ºå–®ç·šç¨‹
            pin_memory=True if device.type == 'mps' else False
        )
        
    except Exception as e:
        print(f"âŒ å‰µå»ºæ•¸æ“šè¼‰å…¥å™¨å¤±æ•—: {e}")
        return
    
    # å‰µå»ºæ¨¡å‹å’Œè¨“ç·´å™¨
    model = SimpleFashionRecommender(config).to(device)
    trainer = SimpleFashionTrainer(model, config, device)
    
    start_epoch = 0
    if resume_from and os.path.exists(resume_from):
        print(f"ğŸ”„ å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´: {resume_from}")
        checkpoint = torch.load(resume_from, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        print(f"âœ… æ¢å¾©å®Œæˆï¼Œå¾ç¬¬ {start_epoch} è¼ªé–‹å§‹")
    
    # é–‹å§‹è¨“ç·´
    print(f"ğŸ¯ é–‹å§‹è¨“ç·´ {config.num_epochs} è¼ª...")
    for epoch in range(start_epoch, config.num_epochs):
        print(f"\n=== ç¬¬ {epoch+1}/{config.num_epochs} è¼ª ===")
        start_time = time.time()
        
        losses = trainer.train_epoch(dataloader, epoch)
        trainer.scheduler.step()
        
        epoch_time = time.time() - start_time
        print(f"âœ… ç¬¬ {epoch+1} è¼ªå®Œæˆ")
        print(f"ğŸ“Š ç¸½æå¤±: {losses['total_loss']:.4f}")
        print(f"ğŸ”— å°æ¯”æå¤±: {losses['contrastive_loss']:.4f}")
        print(f"ğŸ¨ é¢¨æ ¼æå¤±: {losses['style_loss']:.4f}")
        print(f"â±ï¸  ç”¨æ™‚: {epoch_time:.2f}ç§’")
        
        # ä¿å­˜æª¢æŸ¥é»
        if (epoch + 1) % 3 == 0:
            save_path = f"simple_fashion_model_epoch_{epoch+1}.pth"
            trainer.save_checkpoint(epoch, losses, save_path)
    
    # ä¿å­˜æœ€çµ‚æ¨¡å‹
    final_path = "simple_fashion_model_final.pth"
    trainer.save_checkpoint(config.num_epochs-1, losses, final_path)
    
    print(f"\nğŸ‰ ç°¡åŒ–ç‰ˆè¨“ç·´å®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜: {final_path}")
    print(f"ğŸ† æœ€ä½³æ¨¡å‹: simple_fashion_model_final_best.pth")
    print(f"âœ¨ ç‰¹è‰²: ç´”æ·¨çš„ FashionCLIP èªç¾©ç‰¹å¾µç©ºé–“")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ç°¡åŒ–ç‰ˆç©¿æ­æ¨è–¦æ¨¡å‹è¨“ç·´')
    parser.add_argument('--data_root', type=str, required=True, help='æ•¸æ“šé›†æ ¹ç›®éŒ„')
    parser.add_argument('--config', type=str, default='balanced', 
                       choices=['minimal', 'balanced', 'performance'], help='è¨“ç·´é…ç½®')
    parser.add_argument('--resume', type=str, help='æ¢å¾©è¨“ç·´çš„æª¢æŸ¥é»è·¯å¾‘')
    
    args = parser.parse_args()
    train_simple_model(args.data_root, args.config, args.resume) 