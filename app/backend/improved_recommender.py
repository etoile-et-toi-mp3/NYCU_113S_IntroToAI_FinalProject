#!/usr/bin/env python3
"""
æ”¹é€²ç‰ˆç©¿æ­æ¨è–¦å™¨
æ”¯æ´å¤šæ¨¡æ…‹æ¨¡å‹æ¯”è¼ƒç”¨æˆ¶åœ–ç‰‡èˆ‡æ¨è–¦åœ–ç‰‡ï¼Œç”Ÿæˆçµæ§‹åŒ–æ”¹é€²å»ºè­°
"""

import torch
import torch.nn.functional as F
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import json
import os
import argparse
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import PCA
from transformers import (
    CLIPProcessor, CLIPModel,
    AutoProcessor, LlavaNextForConditionalGeneration,
    Blip2Processor, Blip2ForConditionalGeneration,
    InstructBlipProcessor, InstructBlipForConditionalGeneration
)
from simple_train_model import SimpleFashionRecommender, SimpleTrainingConfig
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import textwrap
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class ImprovedFashionRecommendationSystem:
    def __init__(self, model_path="simple_fashion_model_final.pth", 
                 labels_file="simple_dataset_labels.json"):
        """
        æ”¹é€²ç‰ˆæ¨è–¦ç³»çµ±åˆå§‹åŒ–
        """
        logging.info("ğŸš€ åˆå§‹åŒ–æ”¹é€²ç‰ˆæ¨è–¦ç³»çµ±...")
        
        # è¨­ç½®è¨­å‚™
        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        logging.info(f"ğŸ¯ ä½¿ç”¨è¨­å‚™: {self.device}")
        
        # è¼‰å…¥é…ç½®
        self.config = SimpleTrainingConfig()
        
        # è¨­ç½®å­—é«”
        self._setup_fonts()
        
        # è¼‰å…¥æ¨¡å‹
        logging.info("ğŸ”§ è¼‰å…¥æ¨è–¦æ¨¡å‹...")
        self.model = SimpleFashionRecommender(self.config).to(self.device)
        self.use_trained_model = False
        
        if os.path.exists(model_path):
            try:
                checkpoint = torch.load(model_path, map_location=self.device)
                self.model.load_state_dict(checkpoint['model_state_dict'])
                self.use_trained_model = True
                logging.info("âœ… è¨“ç·´æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            except Exception as e:
                logging.error(f"âš ï¸ è¼‰å…¥è¨“ç·´æ¨¡å‹å¤±æ•—: {e}")
        
        self.model.eval()
        
        # è¼‰å…¥æ•¸æ“šé›†
        logging.info("ğŸ“Š è¼‰å…¥æ•¸æ“šé›†...")
        with open(labels_file, 'r') as f:
            self.dataset = json.load(f)
        logging.info(f"âœ… è¼‰å…¥ {len(self.dataset)} å€‹æ¨£æœ¬")
        
        # é è™•ç†å¤šç¨®ç‰¹å¾µ
        self._preprocess_multi_features()
        
        # åˆå§‹åŒ–å¤šæ¨¡æ…‹æ¨¡å‹
        self.loaded_ai_models = {}
        self._init_ai_model_configs()
        
        # åˆå§‹åŒ–è©³ç´°çš„fashion-clipç‰¹å¾µæç¤ºè©
        self._init_fashion_features()
        
        logging.info("âœ… æ”¹é€²ç‰ˆæ¨è–¦ç³»çµ±åˆå§‹åŒ–å®Œæˆï¼")
    
    def _init_fashion_features(self):
        """åˆå§‹åŒ–è©³ç´°çš„fashion-clipç‰¹å¾µæç¤ºè©"""
        self.fashion_features = {
            'colors': [
                'red clothing', 'blue clothing', 'black clothing', 'white clothing', 
                'gray clothing', 'brown clothing', 'green clothing', 'yellow clothing',
                'pink clothing', 'purple clothing', 'orange clothing', 'beige clothing'
            ],
            'clothing_types': [
                'dress', 't-shirt', 'shirt', 'blouse', 'tank top', 'sweater', 'cardigan',
                'pants', 'jeans', 'skirt', 'shorts', 'leggings', 'jacket', 'coat',
                'blazer', 'hoodie', 'polo shirt', 'vest'
            ],
            'styles': [
                'casual style', 'formal style', 'bohemian style', 'street style',
                'vintage style', 'minimalist style', 'romantic style', 'sporty style',
                'business casual', 'elegant style', 'edgy style', 'preppy style'
            ],
            'patterns': [
                'solid color', 'striped pattern', 'floral pattern', 'plaid pattern',
                'polka dots', 'abstract pattern', 'geometric pattern', 'animal print'
            ],
            'accessories': [
                'handbag', 'backpack', 'hat', 'cap', 'scarf', 'belt', 'watch',
                'sunglasses', 'jewelry', 'necklace', 'earrings', 'bracelet'
            ],
            'shoes': [
                'sneakers', 'boots', 'high heels', 'flats', 'sandals', 'loafers',
                'pumps', 'athletic shoes', 'dress shoes', 'casual shoes'
            ],
            'materials': [
                'cotton fabric', 'denim material', 'silk fabric', 'wool material',
                'leather material', 'synthetic fabric', 'linen fabric', 'knit fabric'
            ],
            'fit': [
                'tight fitting', 'loose fitting', 'oversized clothing', 'fitted clothing',
                'relaxed fit', 'slim fit', 'regular fit', 'cropped clothing'
            ]
        }
    
    def _setup_fonts(self):
        """è¨­ç½®æ”¯æŒä¸­æ–‡çš„å­—é«”"""
        try:
            available_fonts = [f.name for f in fm.fontManager.ttflist]
            preferred_fonts = ['Arial Unicode MS', 'PingFang SC', 'DejaVu Sans']
            
            selected_font = None
            for font in preferred_fonts:
                if font in available_fonts:
                    selected_font = font
                    break
            
            if selected_font:
                plt.rcParams['font.sans-serif'] = [selected_font]
            else:
                plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
            
            plt.rcParams['axes.unicode_minus'] = False
        except Exception as e:
            logging.error(f"âš ï¸ å­—é«”è¨­ç½®å¤±æ•—: {e}")
    
    def _init_ai_model_configs(self):
        """åˆå§‹åŒ–å¤šæ¨¡æ…‹æ¨¡å‹é…ç½®"""
        logging.info("ğŸ§  åˆå§‹åŒ–å¤šæ¨¡æ…‹æ¨¡å‹é…ç½®...")
        
        self.model_configs = {
            'rule_based': {
                'name': 'è¦å‰‡ç³»çµ±',
                'loaded': True
            },
            'clip': {
                'name': 'FashionCLIP',
                'loaded': True
            },
            'llava': {
                'name': 'è¦–è¦ºèªè¨€æ¨¡å‹ (LLaVA)',
                'model_id': 'llava-hf/llava-v1.6-mistral-7b-hf',
                'loaded': False,
                'processor': None,
                'model': None
            },
            'blip2': {
                'name': 'åœ–åƒæè¿°æ¨¡å‹ (BLIP-2)',
                'model_id': 'Salesforce/blip2-opt-2.7b',
                'loaded': False,
                'processor': None,
                'model': None
            },
            'instructblip': {
                'name': 'æŒ‡ä»¤åœ–åƒæ¨¡å‹ (InstructBLIP)',
                'model_id': 'Salesforce/instructblip-vicuna-7b',
                'loaded': False,
                'processor': None,
                'model': None
            }
        }
        
        logging.info("âœ… å¤šæ¨¡æ…‹æ¨¡å‹é…ç½®å®Œæˆ")
    
    def _load_ai_models(self, model_keys):
        """è¼‰å…¥æŒ‡å®šçš„å¤šæ¨¡æ…‹æ¨¡å‹"""
        logging.info(f"ğŸ”„ è¼‰å…¥å¤šæ¨¡æ…‹æ¨¡å‹: {', '.join(model_keys)}")
        
        for model_key in model_keys:
            if model_key in self.loaded_ai_models:
                logging.info(f"âœ… {model_key.upper()} å·²è¼‰å…¥ï¼Œè·³é")
                continue
                
            if model_key not in self.model_configs:
                logging.error(f"âŒ æœªçŸ¥æ¨¡å‹: {model_key}")
                continue
            
            config = self.model_configs[model_key]
            
            try:
                logging.info(f"ğŸ”„ è¼‰å…¥ {config['name']} æ¨¡å‹...")
                
                if model_key == 'llava':
                    config['processor'] = AutoProcessor.from_pretrained(config['model_id'], use_fast=True)
                    config['model'] = LlavaNextForConditionalGeneration.from_pretrained(
                        config['model_id'],
                        torch_dtype=torch.float16 if self.device.type == 'mps' else torch.float32,
                        low_cpu_mem_usage=True
                    ).to(self.device)
                
                elif model_key == 'blip2':
                    config['processor'] = Blip2Processor.from_pretrained(config['model_id'], use_fast=True)
                    config['model'] = Blip2ForConditionalGeneration.from_pretrained(
                        config['model_id'],
                        torch_dtype=torch.float16 if self.device.type == 'mps' else torch.float32
                    ).to(self.device)
                
                elif model_key == 'instructblip':
                    config['processor'] = InstructBlipProcessor.from_pretrained(config['model_id'], use_fast=True)
                    config['model'] = InstructBlipForConditionalGeneration.from_pretrained(
                        config['model_id'],
                        torch_dtype=torch.float16 if self.device.type == 'mps' else torch.float32
                    ).to(self.device)
                
                if model_key not in ['rule_based', 'clip']:
                    config['loaded'] = True
                    self.loaded_ai_models[model_key] = config
                    logging.info(f"âœ… {config['name']} è¼‰å…¥æˆåŠŸ")
                
            except Exception as e:
                logging.error(f"âŒ è¼‰å…¥ {config['name']} å¤±æ•—: {e}")
                continue
        
        logging.info(f"âœ… å…±è¼‰å…¥ {len(self.loaded_ai_models)} å€‹å¤šæ¨¡æ…‹æ¨¡å‹")
    
    def _preprocess_multi_features(self):
        """é è™•ç†å¤šç¨®ç‰¹å¾µä»¥æ”¹å–„æ¨è–¦è³ªé‡"""
        logging.info("ğŸ”„ é è™•ç†å¤šç´šç‰¹å¾µ...")
        
        original_features = [sample['features'] for sample in self.dataset]
        self.original_features = np.array(original_features)
        self.original_features = self.original_features / np.linalg.norm(
            self.original_features, axis=1, keepdims=True
        )
        logging.info(f"ğŸ“Š åŸå§‹ç‰¹å¾µçŸ©é™£: {self.original_features.shape}")
        
        self.mapped_features = None
        if self.use_trained_model:
            logging.info("ğŸ”„ è¨ˆç®—æ˜ å°„ç‰¹å¾µ...")
            with torch.no_grad():
                original_tensor = torch.tensor(self.original_features, dtype=torch.float32).to(self.device)
                mapped_features = []
                batch_size = 100
                
                for i in range(0, len(original_tensor), batch_size):
                    batch = original_tensor[i:i+batch_size]
                    outputs = self.model.forward(batch)
                    mapped_batch = outputs['fashion_embedding'].cpu().numpy()
                    mapped_features.append(mapped_batch)
                
                self.mapped_features = np.vstack(mapped_features)
                self.mapped_features = self.mapped_features / np.linalg.norm(
                    self.mapped_features, axis=1, keepdims=True
                )
                logging.info(f"ğŸ“Š æ˜ å°„ç‰¹å¾µçŸ©é™£: {self.mapped_features.shape}")
        
        logging.info("ğŸ”„ è¨ˆç®—PCAé™ç¶­ç‰¹å¾µ...")
        self.pca = PCA(n_components=128)
        self.pca_features = self.pca.fit_transform(self.original_features)
        self.pca_features = self.pca_features / np.linalg.norm(
            self.pca_features, axis=1, keepdims=True
        )
        logging.info(f"ğŸ“Š PCAç‰¹å¾µçŸ©é™£: {self.pca_features.shape}")
    
    def extract_image_features(self, image_path):
        """æå–æŸ¥è©¢åœ–ç‰‡çš„å¤šç¨®ç‰¹å¾µ"""
        try:
            image = Image.open(image_path).convert('RGB')
            
            with torch.no_grad():
                clip_inputs = self.model.clip_processor(images=image, return_tensors="pt").to(self.device)
                clip_features = self.model.clip_model.get_image_features(**clip_inputs)
                clip_features = F.normalize(clip_features, p=2, dim=1)
                
                original_features = clip_features.cpu().numpy().flatten()
                original_features = original_features / np.linalg.norm(original_features)
                
                mapped_features = None
                if self.use_trained_model:
                    outputs = self.model.forward(clip_features)
                    mapped_features = outputs['fashion_embedding'].cpu().numpy().flatten()
                    mapped_features = mapped_features / np.linalg.norm(mapped_features)
                
                pca_features = self.pca.transform([original_features])[0]
                pca_features = pca_features / np.linalg.norm(pca_features)
                
                return {
                    'original': original_features,
                    'mapped': mapped_features,
                    'pca': pca_features
                }
        except Exception as e:
            logging.error(f"âŒ ç‰¹å¾µæå–å¤±æ•—: {e}")
            return None
    
    def extract_outfit_attributes(self, image_path):
        """æå–åœ–ç‰‡çš„æœè£å±¬æ€§"""
        try:
            image = Image.open(image_path).convert('RGB')
            
            # ä½¿ç”¨ FashionCLIP æå–å±¬æ€§
            text_prompts = [
                "red dress", "blue t-shirt", "black pants", "white shirt", "jeans",
                "casual style", "formal style", "bohemian style", "street style",
                "black shoes", "sneakers", "scarf", "hat", "long sleeves", "floral pattern"
            ]
            clip_inputs = self.model.clip_processor(
                text=text_prompts, images=image, return_tensors="pt", padding=True
            ).to(self.device)
            with torch.no_grad():
                outputs = self.model.clip_model(**clip_inputs)
                logits_per_image = outputs.logits_per_image
                probs = logits_per_image.softmax(dim=1)[0]
            
            clip_attrs = {prompt: float(prob) for prompt, prob in zip(text_prompts, probs)}
            
            # ä½¿ç”¨ BLIP-2 ç”Ÿæˆæè¿°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            description = ""
            if 'blip2' in self.loaded_ai_models:
                config = self.loaded_ai_models['blip2']
                inputs = config['processor'](
                    images=image,
                    text="Describe the outfit in this image, including clothing types, colors, and style.",
                    return_tensors="pt"
                ).to(self.device)
                with torch.no_grad():
                    outputs = config['model'].generate(
                        **inputs,
                        max_new_tokens=50,
                        num_beams=4
                    )
                description = config['processor'].decode(outputs[0], skip_special_tokens=True)
            
            # è§£ææè¿°
            parsed_attrs = {}
            if description:
                description_lower = description.lower()
                colors = ["red", "blue", "black", "white", "gray"]
                clothing_types = ["dress", "t-shirt", "pants", "shirt", "jeans"]
                styles = ["casual", "formal", "bohemian", "street"]
                
                for color in colors:
                    if color in description_lower:
                        parsed_attrs["color"] = color
                        break
                for clothing in clothing_types:
                    if clothing in description_lower:
                        parsed_attrs["clothing_type"] = clothing
                        break
                for style in styles:
                    if style in description_lower:
                        parsed_attrs["style"] = style
                        break
            
            return {
                "clip_attributes": clip_attrs,
                "description": description,
                "parsed_attributes": parsed_attrs
            }
        except Exception as e:
            logging.error(f"âŒ å±¬æ€§æå–å¤±æ•—: {e}")
            return {"clip_attributes": {}, "description": "", "parsed_attributes": {}}
    
    def find_similar_outfits_improved(self, image_path, gender, top_k=4, style_preference=None, 
                                    similarity_weights={'original': 0.5, 'pca': 0.3, 'mapped': 0.2}):
        """æ”¹é€²ç‰ˆç›¸ä¼¼ç©¿æ­æŸ¥æ‰¾ï¼Œå›ºå®šè¿”å›4å¼µæ¨è–¦"""
        logging.info(f"ğŸ” æ”¹é€²ç‰ˆåœ–ç‰‡åˆ†æ: {image_path}")
        
        query_features = self.extract_image_features(image_path)
        if query_features is None:
            return []
        
        filtered_indices = []
        for i, sample in enumerate(self.dataset):
            if sample['gender'] != gender:
                continue
            if style_preference and sample['style'] != style_preference:
                continue
            filtered_indices.append(i)
        
        if not filtered_indices:
            logging.error("âŒ æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„æ¨£æœ¬")
            return []
        
        similarities = self._calculate_multi_similarity(
            query_features, filtered_indices, similarity_weights
        )
        
        sorted_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in sorted_indices:
            original_idx = filtered_indices[idx]
            sample = self.dataset[original_idx]
            
            detailed_sim = self._calculate_detailed_similarity(
                query_features, original_idx
            )
            
            results.append({
                'path': sample['path'],
                'style': sample['style'],
                'gender': sample['gender'],
                'similarity': float(similarities[idx]),
                'score': float(similarities[idx] * 10),
                'detailed_similarity': detailed_sim,
                'features': sample['features'],
                'clothing_types': sample.get('clothing_types', []),
                'colors': sample.get('colors', [])
            })
        
        logging.info(f"âœ… æ‰¾åˆ° {len(results)} å€‹ç›¸ä¼¼ç©¿æ­")
        return results
    
    def _calculate_multi_similarity(self, query_features, filtered_indices, weights):
        """è¨ˆç®—å¤šç´šç‰¹å¾µèåˆç›¸ä¼¼åº¦"""
        final_similarities = np.zeros(len(filtered_indices))
        
        if 'original' in weights and weights['original'] > 0:
            filtered_original = self.original_features[filtered_indices]
            original_sim = cosine_similarity([query_features['original']], filtered_original)[0]
            final_similarities += weights['original'] * original_sim
        
        if 'pca' in weights and weights['pca'] > 0:
            filtered_pca = self.pca_features[filtered_indices]
            pca_sim = cosine_similarity([query_features['pca']], filtered_pca)[0]
            final_similarities += weights['pca'] * pca_sim
        
        if 'mapped' in weights and weights['mapped'] > 0 and self.mapped_features is not None:
            filtered_mapped = self.mapped_features[filtered_indices]
            mapped_sim = cosine_similarity([query_features['mapped']], filtered_mapped)[0]
            final_similarities += weights['mapped'] * mapped_sim
        
        return final_similarities
    
    def _calculate_detailed_similarity(self, query_features, sample_idx):
        """è¨ˆç®—è©³ç´°ç›¸ä¼¼åº¦åˆ†æ"""
        sample_original = self.original_features[sample_idx]
        sample_pca = self.pca_features[sample_idx]
        
        detailed = {
            'visual_similarity': float(cosine_similarity([query_features['original']], [sample_original])[0][0]),
            'main_component_similarity': float(cosine_similarity([query_features['pca']], [sample_pca])[0][0])
        }
        
        if self.mapped_features is not None:
            sample_mapped = self.mapped_features[sample_idx]
            detailed['style_similarity'] = float(cosine_similarity([query_features['mapped']], [sample_mapped])[0][0])
        
        return detailed
    
    def translate_to_chinese(self, text):
        """å°‡è‹±æ–‡å»ºè­°ç¿»è­¯ç‚ºä¸­æ–‡"""
        translations = {
            "red": "ç´…è‰²",
            "blue": "è—è‰²",
            "black": "é»‘è‰²",
            "white": "ç™½è‰²",
            "gray": "ç°è‰²",
            "dress": "é€£è¡£è£™",
            "t-shirt": "Tæ¤",
            "pants": "è¤²å­",
            "jeans": "ç‰›ä»”è¤²",
            "shirt": "è¥¯è¡«",
            "casual": "ä¼‘é–’",
            "bohemian": "æ³¢è¥¿ç±³äº",
            "formal": "æ­£å¼",
            "street": "è¡—é ­",
            "shoes": "é‹å­",
            "sneakers": "é‹å‹•é‹",
            "scarf": "åœå·¾",
            "hat": "å¸½å­",
            "accessories": "é…é£¾",
            "color": "é¡è‰²",
            "style": "é¢¨æ ¼",
            "replace": "æ›¿æ›",
            "add": "æ·»åŠ ",
            "remove": "ç§»é™¤",
            "adjust": "èª¿æ•´",
            "long sleeves": "é•·è¢–",
            "floral pattern": "èŠ±å‰åœ–æ¡ˆ"
        }
        
        for en, zh in translations.items():
            text = re.sub(r'\b' + en + r'\b', zh, text, flags=re.IGNORECASE)
        
        text = re.sub(r'(\d+)\.', r'\1ï¼š', text)
        return text
    
    def _clean_text_output(self, text):
        """æ¸…ç†AIè¼¸å‡ºæ–‡å­—ï¼Œä¿ç•™çµæ§‹åŒ–æ ¼å¼"""
        if not text or len(text.strip()) < 5:
            return "å»ºè­°èª¿æ•´é—œéµå…ƒç´ ä»¥åŒ¹é…ç›®æ¨™é¢¨æ ¼"
        
        text = re.sub(r'[.]{2,}', '.', text)
        text = re.sub(r'[ ]{2,}', ' ', text)
        text = re.sub(r'^.*?describe.*?:', '', text, flags=re.IGNORECASE)
        text = re.sub(r'^.*?question.*?:', '', text, flags=re.IGNORECASE)
        
        text = text.strip()
        sentences = text.split('. ')
        formatted_text = []
        
        for sentence in sentences:
            if sentence.strip():
                sentence = re.sub(r'(\d+)\.', r'\1:', sentence)
                formatted_text.append(sentence.strip())
        
        return '. '.join(formatted_text) + '.' if formatted_text else "å»ºè­°èª¿æ•´é—œéµå…ƒç´ ä»¥åŒ¹é…ç›®æ¨™é¢¨æ ¼"
    
    def generate_comparison_advice(self, user_image_path, recommendation, ai_models):
        """ç‚ºå–®ä¸€æ¨è–¦ç”Ÿæˆå¤šæ¨¡æ…‹æ¯”è¼ƒå»ºè­°"""
        logging.info(f"ğŸ¤– ç‚ºæ¨è–¦ {recommendation['style']} ç”Ÿæˆæ¯”è¼ƒå»ºè­°...")
        
        advice_results = {
            'target_style': recommendation['style'],
            'similarity': recommendation['similarity'],
            'ai_advice': {}
        }
        
        with ThreadPoolExecutor(max_workers=len(ai_models)) as executor:
            future_to_model = {
                executor.submit(self._generate_model_advice, model_key, user_image_path, recommendation): model_key
                for model_key in ai_models if model_key in self.loaded_ai_models or model_key in ['rule_based', 'clip']
            }
            
            for future in as_completed(future_to_model):
                model_key = future_to_model[future]
                try:
                    advice = future.result()
                    advice_results['ai_advice'][model_key] = advice
                except Exception as e:
                    advice_results['ai_advice'][model_key] = f"{self.model_configs[model_key]['name']} åˆ†æå¤±æ•—: {str(e)}"
        
        return advice_results
    
    def _generate_model_advice(self, model_key, user_image_path, recommendation):
        """æ ¹æ“šæ¨¡å‹é¡å‹ç”Ÿæˆå»ºè­°"""
        if model_key == 'rule_based':
            return self._generate_rule_based_advice(user_image_path, recommendation)
        elif model_key == 'clip':
            return self._generate_clip_advice(user_image_path, recommendation)
        elif model_key == 'llava':
            return self._generate_llava_advice(user_image_path, recommendation)
        elif model_key == 'blip2':
            return self._generate_blip2_advice(user_image_path, recommendation)
        elif model_key == 'instructblip':
            return self._generate_instructblip_advice(user_image_path, recommendation)
        else:
            return "ä¸æ”¯æŒçš„æ¨¡å‹é¡å‹"
    
    def _generate_rule_based_advice(self, user_image_path, recommendation):
        """ç”Ÿæˆè¦å‰‡ç³»çµ±å»ºè­°"""
        try:
            similarity = recommendation['similarity']
            style = recommendation['style']
            colors = recommendation.get('colors', [])
            clothing_types = recommendation.get('clothing_types', [])
            
            advice_parts = []
            
            # æ·»åŠ ç›¸ä¼¼åº¦è©•ä¼°
            if similarity > 0.8:
                advice_parts.append(f"âœ¨ æ‚¨çš„ç©¿æ­èˆ‡{style}é¢¨æ ¼éå¸¸æ¥è¿‘ï¼")
                advice_parts.append("å»ºè­°é€²è¡Œå¾®èª¿ï¼š")
            elif similarity > 0.6:
                advice_parts.append(f"ğŸ‘ æ‚¨çš„ç©¿æ­èˆ‡{style}é¢¨æ ¼æœ‰ä¸€å®šç›¸ä¼¼æ€§")
                advice_parts.append("å»ºè­°é€²è¡Œä»¥ä¸‹èª¿æ•´ï¼š")
            else:
                advice_parts.append(f"ğŸ¯ è¦é”åˆ°{style}é¢¨æ ¼ï¼Œéœ€è¦è¼ƒå¤§å¹…åº¦èª¿æ•´ï¼š")
            
            # é¢¨æ ¼ç‰¹å®šå»ºè­°
            style_advice = {
                'CASUAL': [
                    "â€¢ é¸æ“‡èˆ’é©è‡ªç„¶çš„å‰ªè£",
                    "â€¢ é¡è‰²ä»¥åŸºæœ¬è‰²ç‚ºä¸»ï¼šç™½ã€é»‘ã€è—ã€ç°",
                    "â€¢ æè³ªé¸æ“‡æ£‰è³ªã€ç‰›ä»”ç­‰",
                    "â€¢ é¿å…éæ–¼æ­£å¼æˆ–è¯éº—çš„å…ƒç´ "
                ],
                'BOHEMIAN': [
                    "â€¢ é¸æ“‡è¼•ç›ˆã€æµå‹•çš„æè³ª",
                    "â€¢ èå…¥èŠ±å‰åœ–æ¡ˆæˆ–æ°‘æ—å…ƒç´ ", 
                    "â€¢ é…é£¾ï¼šé•·é …éŠã€å¯¬é‚Šå¸½",
                    "â€¢ é¡è‰²ä»¥å¤§åœ°è‰²æˆ–é®®è±”è‰²ç‚ºä¸»"
                ],
                'STREET': [
                    "â€¢ åŠ å…¥è¡—é ­å…ƒç´ ï¼šå¯¬é¬†å‰ªè£ã€å±¤æ¬¡æ­é…",
                    "â€¢ å˜—è©¦oversizedä¸Šè¡£æˆ–å¸½T",
                    "â€¢ é‹å­é¸æ“‡é‹å‹•é‹æˆ–è¡—é ­é¢¨é´å­",
                    "â€¢ é…é£¾ï¼šæ£’çƒå¸½ã€èƒŒåŒ…"
                ],
                'FORMAL': [
                    "â€¢ é¸æ“‡ä¿®èº«å‰ªè£çš„è¥¿è£æˆ–ç¦®æœ",
                    "â€¢ é¡è‰²ä»¥é»‘ã€ç™½ã€æ·±è—ç‚ºä¸»",
                    "â€¢ é…é£¾ï¼šé ˜å¸¶ã€è¢–æ‰£ç­‰æ­£å¼å…ƒç´ ",
                    "â€¢ é¿å…ä¼‘é–’å…ƒç´ "
                ],
                'ARTSY': [
                    "â€¢ å˜—è©¦ä¸å°ç¨±æˆ–ç‰¹æ®Šå‰ªè£è¨­è¨ˆ",
                    "â€¢ èå…¥è—è¡“æ„Ÿçš„åœ–æ¡ˆæˆ–è‰²å½©",
                    "â€¢ é…é£¾é¸æ“‡æœ‰è¨­è¨ˆæ„Ÿçš„å–®å“",
                    "â€¢ æ•¢æ–¼å˜—è©¦ç¨ç‰¹çš„æ­é…çµ„åˆ"
                ]
            }
            
            if style in style_advice:
                advice_parts.extend(style_advice[style])
            else:
                # é€šç”¨å»ºè­°
                advice_parts.extend([
                    "â€¢ æ³¨æ„æ•´é«”æ­é…çš„å”èª¿æ€§",
                    "â€¢ é¸æ“‡é©åˆçš„é¡è‰²çµ„åˆ",
                    "â€¢ è€ƒæ…®å ´åˆçš„é©åˆ‡æ€§"
                ])
            
            # æ·»åŠ å…·é«”çš„é¡è‰²å’Œæœè£å»ºè­°
            if colors:
                advice_parts.append(f"ğŸ¨ åƒè€ƒè‰²å½©ï¼š{', '.join(colors[:3])}")
            
            if clothing_types:
                advice_parts.append(f"ğŸ‘” æ¨è–¦æœè£é¡å‹ï¼š{', '.join(clothing_types[:3])}")
            
            # æ·»åŠ ç›¸ä¼¼åº¦è©•åˆ†
            score = int(similarity * 100)
            advice_parts.append(f"ğŸ“Š ç›®å‰åŒ¹é…åº¦ï¼š{score}%")
            
            return "\n".join(advice_parts)
            
        except Exception as e:
            logging.error(f"è¦å‰‡ç³»çµ±å»ºè­°ç”Ÿæˆå¤±æ•—: {e}")
            return f"è¦å‰‡ç³»çµ±åˆ†æå¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦"
    
    def _generate_clip_advice(self, user_image_path, recommendation):
        """ç”Ÿæˆæ”¹é€²ç‰ˆ FashionCLIP åŸºæ–¼ç‰¹å¾µçš„ rule_based æ¯”è¼ƒå»ºè­°"""
        try:
            logging.info("ğŸ” é–‹å§‹è©³ç´°çš„FashionCLIPç‰¹å¾µæ¯”è¼ƒ...")
            
            # æå–ç”¨æˆ¶å’Œç›®æ¨™åœ–ç‰‡çš„è©³ç´°ç‰¹å¾µ
            user_features = self.extract_detailed_fashion_features(user_image_path)
            target_features = self.extract_detailed_fashion_features(recommendation['path'])
            
            if not user_features or not target_features:
                return "FashionCLIP ç‰¹å¾µæå–å¤±æ•—"
            
            style = recommendation['style']
            similarity = recommendation['similarity']
            
            advice_parts = []
            advice_parts.append(f"ğŸ¯ FashionCLIP è©³ç´°ç‰¹å¾µæ¯”è¼ƒåˆ†æ (ç›®æ¨™é¢¨æ ¼: {style})")
            
            # åˆ†ææ¯å€‹ç‰¹å¾µé¡åˆ¥çš„å·®ç•°
            for category in self.fashion_features.keys():
                if category in user_features and category in target_features:
                    user_feature = user_features[category]
                    target_feature = target_features[category]
                    
                    user_top = user_feature['top_feature']
                    target_top = target_feature['top_feature']
                    user_score = user_feature['score']
                    target_score = target_feature['score']
                    
                    category_advice = self._analyze_feature_category(
                        category, user_top, target_top, user_score, target_score, user_feature, target_feature
                    )
                    
                    if category_advice:
                        advice_parts.append(category_advice)
            
            # æ·»åŠ æ•´é«”å»ºè­°
            overall_advice = self._generate_overall_clip_advice(user_features, target_features, similarity, style)
            advice_parts.append(overall_advice)
            
            return "\n".join(advice_parts)
            
        except Exception as e:
            return f"FashionCLIP è©³ç´°åˆ†æå¤±æ•—: {str(e)}"

    def _analyze_feature_category(self, category, user_top, target_top, user_score, target_score, user_feature, target_feature):
        """åˆ†æç‰¹å®šç‰¹å¾µé¡åˆ¥çš„å·®ç•°ä¸¦ç”Ÿæˆå»ºè­°"""
        category_names = {
            'colors': 'é¡è‰²',
            'clothing_types': 'æœè£é¡å‹', 
            'styles': 'é¢¨æ ¼',
            'patterns': 'åœ–æ¡ˆ',
            'accessories': 'é…é£¾',
            'shoes': 'é‹é¡',
            'materials': 'æè³ª',
            'fit': 'ç‰ˆå‹'
        }
        
        category_name = category_names.get(category, category)
        
        # å¦‚æœç‰¹å¾µç›¸åŒï¼Œçµ¦äºˆæ­£é¢åé¥‹
        if user_top == target_top:
            if user_score > 0.6 and target_score > 0.6:
                return f"âœ… {category_name}: å¾ˆå¥½ï¼æ‚¨çš„{user_top}èˆ‡ç›®æ¨™å®Œå…¨åŒ¹é… (ç›¸ä¼¼åº¦: {user_score:.2f})"
            else:
                return f"âš¡ {category_name}: æ–¹å‘æ­£ç¢ºä½†å¯ä»¥æ›´çªå‡º{user_top}ç‰¹å¾µ"
        
        # å¦‚æœç‰¹å¾µä¸åŒï¼Œæä¾›å…·é«”å»ºè­°
        advice = f"ğŸ”„ {category_name}: "
        
        if category == 'colors':
            advice += f"æ‚¨ç›®å‰çš„{user_top}å¯ä»¥è€ƒæ…®èª¿æ•´ç‚º{target_top}ä¾†æ›´ç¬¦åˆç›®æ¨™é¢¨æ ¼"
        elif category == 'clothing_types':
            advice += f"å»ºè­°å°‡{user_top}æ›¿æ›ç‚º{target_top}ä»¥é”åˆ°æ›´å¥½çš„æ•ˆæœ"
        elif category == 'styles':
            advice += f"å¾{user_top}è½‰å‘{target_top}ï¼Œæ³¨é‡ç›¸æ‡‰çš„å‰ªè£å’Œæ­é…"
        elif category == 'patterns':
            advice += f"åœ–æ¡ˆå¾{user_top}æ”¹ç‚º{target_top}æœƒæ›´åŠ åˆé©"
        elif category == 'accessories':
            advice += f"é…é£¾æ–¹é¢å»ºè­°åŠ å…¥{target_top}å…ƒç´ "
        elif category == 'shoes':
            advice += f"é‹é¡å»ºè­°å¾{user_top}æ›æˆ{target_top}"
        elif category == 'materials':
            advice += f"æè³ªå¯ä»¥é¸æ“‡{target_top}è€Œé{user_top}"
        elif category == 'fit':
            advice += f"ç‰ˆå‹å»ºè­°èª¿æ•´ç‚º{target_top}è€Œé{user_top}"
        
        # æ·»åŠ ä¿¡å¿ƒåº¦ä¿¡æ¯
        if target_score > 0.7:
            advice += f" (ç›®æ¨™ç‰¹å¾µæ˜é¡¯: {target_score:.2f})"
        elif target_score > 0.5:
            advice += f" (ç›®æ¨™ç‰¹å¾µä¸­ç­‰: {target_score:.2f})"
        else:
            advice += f" (ç›®æ¨™ç‰¹å¾µè¼ƒå¼±: {target_score:.2f})"
            
        return advice

    def _generate_overall_clip_advice(self, user_features, target_features, similarity, style):
        """ç”Ÿæˆæ•´é«”çš„FashionCLIPå»ºè­°"""
        advice = f"\nğŸ“‹ æ•´é«”å»ºè­° (ç›¸ä¼¼åº¦: {similarity:.3f}):\n"
        
        # è¨ˆç®—å„é¡åˆ¥çš„åŒ¹é…åº¦
        matches = 0
        total_categories = 0
        priority_suggestions = []
        
        for category in self.fashion_features.keys():
            if category in user_features and category in target_features:
                total_categories += 1
                if user_features[category]['top_feature'] == target_features[category]['top_feature']:
                    matches += 1
                else:
                    # æ ¹æ“šç›®æ¨™ç‰¹å¾µçš„ä¿¡å¿ƒåº¦æ±ºå®šå„ªå…ˆç´š
                    target_score = target_features[category]['score']
                    if target_score > 0.6:
                        priority_suggestions.append((category, target_score))
        
        match_rate = matches / total_categories if total_categories > 0 else 0
        
        if match_rate > 0.7:
            advice += "ğŸ‰ æ‚¨çš„ç©¿æ­å·²ç¶“éå¸¸æ¥è¿‘ç›®æ¨™é¢¨æ ¼ï¼åªéœ€è¦ç´°å¾®èª¿æ•´"
        elif match_rate > 0.5:
            advice += "ğŸ‘ åŸºæœ¬æ–¹å‘æ­£ç¢ºï¼Œé‡é»æ”¹é€²ä»¥ä¸‹å¹¾å€‹æ–¹é¢"
        else:
            advice += "ğŸ¯ éœ€è¦è¼ƒå¤§å¹…åº¦çš„èª¿æ•´ä¾†é”åˆ°ç›®æ¨™é¢¨æ ¼"
        
        # æŒ‰å„ªå…ˆç´šæ’åºå»ºè­°
        priority_suggestions.sort(key=lambda x: x[1], reverse=True)
        
        if priority_suggestions:
            advice += "\nğŸ”¥ å„ªå…ˆæ”¹é€²é …ç›®ï¼š"
            for i, (category, score) in enumerate(priority_suggestions[:3], 1):
                category_names = {
                    'colors': 'é¡è‰²æ­é…',
                    'clothing_types': 'æœè£é¸æ“‡', 
                    'styles': 'æ•´é«”é¢¨æ ¼',
                    'patterns': 'åœ–æ¡ˆè¨­è¨ˆ',
                    'accessories': 'é…é£¾é¸æ“‡',
                    'shoes': 'é‹é¡æ­é…',
                    'materials': 'æè³ªé¸æ“‡',
                    'fit': 'ç‰ˆå‹èª¿æ•´'
                }
                category_name = category_names.get(category, category)
                advice += f"\n  {i}. {category_name} (é‡è¦åº¦: {score:.2f})"
        
        return advice

    def extract_detailed_fashion_features(self, image_path):
        """æå–è©³ç´°çš„æ™‚å°šç‰¹å¾µç”¨æ–¼rule_basedæ¯”è¼ƒ"""
        try:
            image = Image.open(image_path).convert('RGB')
            
            # èª¿æ•´åœ–ç‰‡å¤§å°ä»¥æé«˜è™•ç†æ•ˆç‡
            if max(image.size) > 512:
                ratio = 512 / max(image.size)
                new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
                image = image.resize(new_size, Image.Resampling.LANCZOS)
            
            detailed_features = {}
            
            # ç‚ºæ¯å€‹ç‰¹å¾µé¡åˆ¥æå–è©•åˆ†
            for category, prompts in self.fashion_features.items():
                clip_inputs = self.model.clip_processor(
                    text=prompts, images=image, return_tensors="pt", padding=True
                ).to(self.device)
                
                with torch.no_grad():
                    outputs = self.model.clip_model(**clip_inputs)
                    logits_per_image = outputs.logits_per_image
                    probs = logits_per_image.softmax(dim=1)[0]
                
                # ç²å–æœ€é«˜åˆ†çš„ç‰¹å¾µå’Œå…¶åˆ†æ•¸
                max_idx = probs.argmax().item()
                max_score = probs[max_idx].item()
                
                detailed_features[category] = {
                    'top_feature': prompts[max_idx],
                    'score': float(max_score),
                    'all_scores': {prompt: float(score) for prompt, score in zip(prompts, probs)}
                }
            
            return detailed_features
        except Exception as e:
            logging.error(f"âŒ è©³ç´°ç‰¹å¾µæå–å¤±æ•—: {e}")
            return {}

    def create_comparison_image(self, user_image_path, target_image_path, temp_path="temp_comparison.jpg"):
        """å‰µå»ºä¸Šä¸‹æ’åˆ—çš„æ¯”è¼ƒåœ–ç‰‡ä¾›LLaVAåˆ†æ"""
        try:
            # è¼‰å…¥å…©å¼µåœ–ç‰‡
            user_image = Image.open(user_image_path).convert('RGB')
            target_image = Image.open(target_image_path).convert('RGB')
            
            # æ¨™æº–åŒ–åœ–ç‰‡å¤§å°
            target_width = 512
            target_height = 512
            
            user_image = user_image.resize((target_width, target_height), Image.Resampling.LANCZOS)
            target_image = target_image.resize((target_width, target_height), Image.Resampling.LANCZOS)
            
            # å‰µå»ºæ–°çš„åœ–ç‰‡ï¼Œé«˜åº¦æ˜¯å…©å¼µåœ–ç‰‡çš„ç¸½å’ŒåŠ ä¸Šæ¨™ç±¤ç©ºé–“
            label_height = 60
            total_height = (target_height * 2) + (label_height * 2) + 20  # 20æ˜¯é–“è·
            comparison_image = Image.new('RGB', (target_width, total_height), (255, 255, 255))
            
            # å˜—è©¦è¼‰å…¥å­—é«”
            try:
                font = ImageFont.truetype("/System/Library/Fonts/Arial.ttf", 24)
            except:
                try:
                    font = ImageFont.truetype("arial.ttf", 24)
                except:
                    font = ImageFont.load_default()
            
            # å‰µå»ºç¹ªåœ–å°è±¡
            draw = ImageDraw.Draw(comparison_image)
            
            # æ·»åŠ æ¨™ç±¤å’Œåœ–ç‰‡
            y_offset = 10
            
            # ç”¨æˆ¶åœ–ç‰‡æ¨™ç±¤
            draw.text((10, y_offset), "ç”¨æˆ¶ç©¿æ­ (User Outfit)", fill=(0, 0, 0), font=font)
            y_offset += label_height
            
            # ç²˜è²¼ç”¨æˆ¶åœ–ç‰‡
            comparison_image.paste(user_image, (0, y_offset))
            y_offset += target_height + 10
            
            # ç›®æ¨™åœ–ç‰‡æ¨™ç±¤
            draw.text((10, y_offset), "ç›®æ¨™ç©¿æ­ (Target Outfit)", fill=(0, 0, 0), font=font)
            y_offset += label_height
            
            # ç²˜è²¼ç›®æ¨™åœ–ç‰‡
            comparison_image.paste(target_image, (0, y_offset))
            
            # ä¿å­˜æ¯”è¼ƒåœ–ç‰‡
            comparison_image.save(temp_path, quality=95)
            logging.info(f"âœ… æ¯”è¼ƒåœ–ç‰‡å·²å‰µå»º: {temp_path}")
            
            return temp_path
        except Exception as e:
            logging.error(f"âŒ å‰µå»ºæ¯”è¼ƒåœ–ç‰‡å¤±æ•—: {e}")
            return None

    def _generate_llava_advice(self, user_image_path, recommendation):
        """ç”Ÿæˆæ”¹é€²ç‰ˆ LLaVA é›™åœ–ç‰‡æ¯”è¼ƒå»ºè­°"""
        try:
            if 'llava' not in self.loaded_ai_models:
                return "LLaVAæ¨¡å‹æœªè¼‰å…¥"
                
            config = self.loaded_ai_models['llava']
            
            # å‰µå»ºæ¯”è¼ƒåœ–ç‰‡
            comparison_path = self.create_comparison_image(user_image_path, recommendation['path'])
            if not comparison_path:
                return "ç„¡æ³•å‰µå»ºæ¯”è¼ƒåœ–ç‰‡"
            
            try:
                # è¼‰å…¥åˆä½µå¾Œçš„åœ–ç‰‡
                comparison_image = Image.open(comparison_path).convert('RGB')
                
                # å‰µå»ºå°ˆé–€çš„æ¯”è¼ƒæç¤ºè©
                style = recommendation['style']
                similarity = recommendation['similarity']
                
                prompt = f"""è«‹ä»”ç´°åˆ†æé€™å¼µæ¯”è¼ƒåœ–ç‰‡ï¼Œä¸ŠåŠéƒ¨åˆ†æ˜¯ç”¨æˆ¶çš„ç•¶å‰ç©¿æ­ï¼Œä¸‹åŠéƒ¨åˆ†æ˜¯ç›®æ¨™{style}é¢¨æ ¼çš„ç©¿æ­ã€‚

è«‹å¾ä»¥ä¸‹å¹¾å€‹æ–¹é¢é€²è¡Œè©³ç´°æ¯”è¼ƒä¸¦æä¾›å…·é«”çš„æ”¹é€²å»ºè­°ï¼š

1. æœè£é¡å‹æ¯”è¼ƒï¼šåˆ†æå…©è€…åœ¨ä¸Šè¡£ã€ä¸‹è£ã€å¤–å¥—ç­‰æ–¹é¢çš„å·®ç•°
2. é¡è‰²æ­é…æ¯”è¼ƒï¼šæ¯”è¼ƒè‰²å½©é¸æ“‡å’Œæ­é…æ–¹å¼
3. é¢¨æ ¼å…ƒç´ æ¯”è¼ƒï¼šåˆ†æé¢¨æ ¼ç‰¹å¾µçš„å·®ç•°
4. é…é£¾æ¯”è¼ƒï¼šæ¯”è¼ƒé…é£¾çš„é¸æ“‡å’Œæ­é…
5. æ•´é«”æ•ˆæœæ¯”è¼ƒï¼šè©•åƒ¹æ•´é«”å”èª¿æ€§å’Œé¢¨æ ¼çµ±ä¸€æ€§

è«‹æä¾›5-8æ¢å…·é«”çš„æ”¹é€²å»ºè­°ï¼Œè®“ç”¨æˆ¶çš„ç©¿æ­æ›´æ¥è¿‘ç›®æ¨™é¢¨æ ¼ã€‚"""

                # ä½¿ç”¨æ­£ç¢ºçš„LLaVAæ ¼å¼
                conversation = [
                    {
                        "role": "user", 
                        "content": [
                            {"type": "image"},
                            {"type": "text", "text": prompt}
                        ]
                    }
                ]
                
                # æ ¼å¼åŒ–prompt
                formatted_prompt = config['processor'].apply_chat_template(
                    conversation, 
                    add_generation_prompt=True
                )
                
                # è™•ç†è¼¸å…¥
                inputs = config['processor'](
                    comparison_image, 
                    formatted_prompt, 
                    return_tensors="pt"
                ).to(self.device)
                
                # ç”Ÿæˆå›æ‡‰
                with torch.no_grad():
                    outputs = config['model'].generate(
                        **inputs,
                        max_new_tokens=300,
                        temperature=0.7,
                        do_sample=True,
                        top_p=0.9,
                        pad_token_id=config['processor'].tokenizer.eos_token_id
                    )
                
                # è§£ç¢¼å›æ‡‰
                response = config['processor'].decode(outputs[0], skip_special_tokens=True)
                
                # æ¸…ç†å›æ‡‰
                if formatted_prompt in response:
                    response = response.replace(formatted_prompt, "").strip()
                
                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                try:
                    os.remove(comparison_path)
                except:
                    pass
                
                # æ ¼å¼åŒ–å›æ‡‰
                cleaned_response = self._clean_and_format_llava_response(response, style, similarity)
                return self.translate_to_chinese(cleaned_response)
                
            except Exception as e:
                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                try:
                    os.remove(comparison_path)
                except:
                    pass
                raise e
                
        except Exception as e:
            return f"LLaVA é›™åœ–ç‰‡æ¯”è¼ƒåˆ†æå¤±æ•—: {str(e)}"

    def _clean_and_format_llava_response(self, response, style, similarity):
        """æ¸…ç†å’Œæ ¼å¼åŒ–LLaVAçš„å›æ‡‰"""
        if not response or len(response.strip()) < 10:
            return f"å»ºè­°åƒè€ƒ{style}é¢¨æ ¼çš„ç‰¹å¾µé€²è¡Œæ•´é«”èª¿æ•´"
        
        # ç§»é™¤å¤šé¤˜çš„ç©ºç™½å’Œæ¨™é»
        response = re.sub(r'\s+', ' ', response.strip())
        response = re.sub(r'[.]{2,}', '.', response)
        
        # ç¢ºä¿å›æ‡‰æ˜¯çµæ§‹åŒ–çš„
        if not any(marker in response for marker in ['1.', '2.', '1:', '2:', '1ã€', '2ã€']):
            # å¦‚æœæ²’æœ‰ç·¨è™Ÿï¼Œå˜—è©¦æŒ‰å¥å­åˆ†å‰²ä¸¦æ·»åŠ ç·¨è™Ÿ
            sentences = [s.strip() for s in response.split('.') if s.strip() and len(s.strip()) > 10]
            if sentences:
                formatted_response = ""
                for i, sentence in enumerate(sentences[:6], 1):
                    formatted_response += f"{i}. {sentence}. "
                response = formatted_response
        
        # æ·»åŠ ç›¸ä¼¼åº¦ä¿¡æ¯
        similarity_text = f"\n\nğŸ“Š ç•¶å‰ç›¸ä¼¼åº¦: {similarity:.1%}"
        if similarity > 0.8:
            similarity_text += " - å·²ç¶“éå¸¸æ¥è¿‘ç›®æ¨™é¢¨æ ¼"
        elif similarity > 0.6:
            similarity_text += " - æœ‰ä¸€å®šç›¸ä¼¼æ€§ï¼Œå¯ä»¥é€²ä¸€æ­¥å„ªåŒ–"
        else:
            similarity_text += " - éœ€è¦è¼ƒå¤§èª¿æ•´ä¾†é”åˆ°ç›®æ¨™é¢¨æ ¼"
        
        return response + similarity_text

    def _generate_blip2_advice(self, user_image_path, recommendation):
        """ç”Ÿæˆ BLIP-2 å»ºè­°"""
        try:
            config = self.loaded_ai_models['blip2']
            
            user_image = Image.open(user_image_path).convert('RGB')
            target_image = Image.open(recommendation['path']).convert('RGB')
            
            user_prompt = "Describe the outfit in this image, including clothing types, colors, and style."
            target_prompt = f"Describe the outfit in this image, focusing on its {recommendation['style']} style characteristics."
            
            # è™•ç†ç”¨æˆ¶åœ–ç‰‡æè¿°
            user_inputs = config['processor'](
                images=user_image,
                text=user_prompt,
                return_tensors="pt",
                padding=True
            ).to(self.device)
            with torch.no_grad():
                user_outputs = config['model'].generate(
                    **user_inputs,
                    max_new_tokens=100,
                    num_beams=4
                )
            user_desc = config['processor'].decode(user_outputs[0], skip_special_tokens=True)
            
            # è™•ç†ç›®æ¨™åœ–ç‰‡æè¿°
            target_inputs = config['processor'](
                images=target_image,
                text=target_prompt,
                return_tensors="pt",
                padding=True
            ).to(self.device)
            with torch.no_grad():
                target_outputs = config['model'].generate(
                    **target_inputs,
                    max_new_tokens=100,
                    num_beams=4
                )
            target_desc = config['processor'].decode(target_outputs[0], skip_special_tokens=True)
            
            style = recommendation['style']
            similarity = recommendation['similarity']
            
            user_attrs = self.extract_outfit_attributes(user_image_path)
            target_attrs = self.extract_outfit_attributes(recommendation['path'])
            
            prompt = f"""
Compare the following two outfit descriptions to provide specific improvement advice:
User's outfit: {user_desc}
Target {style} style outfit: {target_desc}
User attributes: {user_attrs['parsed_attributes']}
Target attributes: {target_attrs['parsed_attributes']}
Similarity score: {similarity:.3f}

Provide actionable advice to transform the user's outfit to match the target style, structured as:
1. Key differences: List specific differences in clothing items.
2. Clothing suggestions: Specify clothing items to change.
3. Color adjustments: Suggest changes in colors or patterns.
4. Accessory suggestions: Recommend accessories to add or remove.
5. Overall styling: Advise on improving the overall look.

Provide detailed and practical advice in English.
"""
            inputs = config['processor'](
                text=prompt,
                return_tensors="pt",
                padding=True
            ).to(self.device)
            
            with torch.no_grad():
                outputs = config['model'].generate(
                    **inputs,
                    max_new_tokens=200,
                    num_beams=4,
                    temperature=0.8
                )
            
            advice = config['processor'].decode(outputs[0], skip_special_tokens=True)
            advice = advice.replace(prompt, "").strip()
            return self.translate_to_chinese(self._clean_text_output(advice))
        except Exception as e:
            return f"åœ–åƒæè¿°æ¨¡å‹ (BLIP-2) åˆ†æå¤±æ•—: {str(e)}"
    
    def _generate_instructblip_advice(self, user_image_path, recommendation):
        """ç”Ÿæˆ InstructBLIP å»ºè­°"""
        try:
            config = self.loaded_ai_models['instructblip']
            
            user_image = Image.open(user_image_path).convert('RGB')
            target_image = Image.open(recommendation['path']).convert('RGB')
            
            user_prompt = "Describe the outfit in this image, including clothing types, colors, and style."
            target_prompt = f"Describe the outfit in this image, focusing on its {recommendation['style']} style characteristics."
            
            # è™•ç†ç”¨æˆ¶åœ–ç‰‡æè¿°
            user_inputs = config['processor'](
                images=user_image,
                text=user_prompt,
                return_tensors="pt",
                padding=True
            ).to(self.device)
            with torch.no_grad():
                user_outputs = config['model'].generate(
                    **user_inputs,
                    max_new_tokens=100,
                    num_beams=4
                )
            user_desc = config['processor'].decode(user_outputs[0], skip_special_tokens=True)
            
            # è™•ç†ç›®æ¨™åœ–ç‰‡æè¿°
            target_inputs = config['processor'](
                images=target_image,
                text=target_prompt,
                return_tensors="pt",
                padding=True
            ).to(self.device)
            with torch.no_grad():
                target_outputs = config['model'].generate(
                    **target_inputs,
                    max_new_tokens=100,
                    num_beams=4
                )
            target_desc = config['processor'].decode(target_outputs[0], skip_special_tokens=True)
            
            style = recommendation['style']
            similarity = recommendation['similarity']
            
            user_attrs = self.extract_outfit_attributes(user_image_path)
            target_attrs = self.extract_outfit_attributes(recommendation['path'])
            
            prompt = f"""
Compare the following two outfit descriptions to provide specific improvement advice:
User's outfit: {user_desc}
Target {style} style outfit: {target_desc}
User attributes: {user_attrs['parsed_attributes']}
Target attributes: {target_attrs['parsed_attributes']}
Similarity score: {similarity:.3f}

Provide actionable advice to transform the user's outfit to match the target style, structured as:
1. Key differences: List specific differences in clothing items.
2. Clothing suggestions: Specify clothing items to change.
3. Color adjustments: Suggest changes in colors or patterns.
4. Accessory suggestions: Recommend accessories to add or remove.
5. Overall styling: Advise on improving the overall look.

Provide detailed and practical advice in English.
"""
            inputs = config['processor'](
                text=prompt,
                return_tensors="pt",
                padding=True
            ).to(self.device)
            
            with torch.no_grad():
                outputs = config['model'].generate(
                    **inputs,
                    max_new_tokens=200,
                    num_beams=4,
                    temperature=0.8
                )
            
            advice = config['processor'].decode(outputs[0], skip_special_tokens=True)
            advice = advice.replace(prompt, "").strip()
            return self.translate_to_chinese(self._clean_text_output(advice))
        except Exception as e:
            return f"æŒ‡ä»¤åœ–åƒæ¨¡å‹ (InstructBLIP) åˆ†æå¤±æ•—: {str(e)}"

    def analyze_and_recommend_improved(self, image_path, gender, top_k=4, style_preference=None, 
                                     strategy='balanced', ai_models=['rule_based', 'clip', 'llava', 'blip2', 'instructblip']):
        """æ”¹é€²ç‰ˆåˆ†æå’Œæ¨è–¦æµç¨‹ï¼ŒåŒ…å«å¤šæ¨¡æ…‹æ¯”è¼ƒ"""
        logging.info(f"\nğŸ¯ é–‹å§‹æ”¹é€²ç‰ˆç©¿æ­åˆ†æ...")
        logging.info(f"ğŸ“ åœ–ç‰‡: {image_path}")
        logging.info(f"ğŸ‘¤ æ€§åˆ¥: {gender}")
        logging.info(f"ğŸ¨ é¢¨æ ¼åå¥½: {style_preference or 'ç„¡é™åˆ¶'}")
        logging.info(f"ğŸ”§ ç­–ç•¥: {strategy}")
        logging.info(f"ğŸ¤– ä½¿ç”¨å¤šæ¨¡æ…‹æ¨¡å‹: {', '.join(ai_models)}")
        
        if not os.path.exists(image_path):
            return {"error": f"åœ–ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}"}
        
        try:
            # è¼‰å…¥å¤šæ¨¡æ…‹æ¨¡å‹
            self._load_ai_models(ai_models)
            
            # å®šç¾©ç­–ç•¥æ¬Šé‡
            strategy_weights = {
                'pure_visual': {'original': 1.0, 'pca': 0.0, 'mapped': 0.0},
                'visual_focused': {'original': 0.7, 'pca': 0.3, 'mapped': 0.0},
                'balanced': {'original': 0.5, 'pca': 0.3, 'mapped': 0.2},
                'style_aware': {'original': 0.3, 'pca': 0.2, 'mapped': 0.5}
            }
            
            weights = strategy_weights.get(strategy, strategy_weights['balanced'])
            
            # ç”Ÿæˆæ¨è–¦
            similar_outfits = self.find_similar_outfits_improved(
                image_path, gender, top_k, style_preference, weights
            )
            
            if not similar_outfits:
                return {
                    "error": "æ²’æœ‰æ‰¾åˆ°ç›¸ä¼¼çš„ç©¿æ­",
                    "suggestion": "è«‹å˜—è©¦èª¿æ•´æ€§åˆ¥æˆ–é¢¨æ ¼åå¥½è¨­ç½®"
                }
            
            # ç”Ÿæˆå¤šæ¨¡æ…‹æ¯”è¼ƒå»ºè­°
            for outfit in similar_outfits:
                outfit['comparison_advice'] = self.generate_comparison_advice(
                    image_path, outfit, ai_models
                )
            
            # é¢¨æ ¼åˆ†æ
            style_analysis = self._generate_style_analysis(similar_outfits)
            
            # æ”¹é€²å»ºè­°
            improved_advice = self._generate_improved_advice(similar_outfits, strategy)
            
            result = {
                "status": "success",
                "input_image": image_path,
                "gender": gender,
                "style_preference": style_preference,
                "strategy": strategy,
                "strategy_weights": weights,
                "similar_outfits": similar_outfits,
                "style_analysis": style_analysis,
                "improved_advice": improved_advice,
                "ai_models_used": ai_models,
                "summary": {
                    "best_match": similar_outfits[0] if similar_outfits else None,
                    "recommended_style": style_analysis.get("dominant_style"),
                    "visual_similarity": similar_outfits[0]['detailed_similarity']['visual_similarity'] if similar_outfits else 0,
                    "style_similarity": similar_outfits[0]['detailed_similarity'].get('style_similarity', 0) if similar_outfits else 0,
                    "total_recommendations": len(similar_outfits)
                }
            }
            
            logging.info(f"âœ… æ”¹é€²ç‰ˆåˆ†æå®Œæˆï¼")
            return result
        except Exception as e:
            logging.error(f"âŒ åˆ†æå¤±æ•—: {e}")
            return {"error": f"åˆ†æå¤±æ•—: {str(e)}"}
    
    def _generate_style_analysis(self, similar_outfits):
        """ç”Ÿæˆé¢¨æ ¼åˆ†æ"""
        if not similar_outfits:
            return {}
        
        style_distribution = {}
        visual_similarities = []
        style_similarities = []
        
        for outfit in similar_outfits:
            style = outfit['style']
            style_distribution[style] = style_distribution.get(style, 0) + 1
            visual_similarities.append(outfit['detailed_similarity']['visual_similarity'])
            if 'style_similarity' in outfit['detailed_similarity']:
                style_similarities.append(outfit['detailed_similarity']['style_similarity'])
        
        dominant_style = max(style_distribution.items(), key=lambda x: x[1])[0]
        
        return {
            "dominant_style": dominant_style,
            "style_distribution": style_distribution,
            "average_visual_similarity": float(np.mean(visual_similarities)),
            "average_style_similarity": float(np.mean(style_similarities)) if style_similarities else 0,
            "max_visual_similarity": float(max(visual_similarities))
        }
    
    def _generate_improved_advice(self, similar_outfits, strategy):
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        if not similar_outfits:
            return []
        
        advice = []
        strategy_advice = {
            'pure_visual': "åŸºæ–¼ç´”è¦–è¦ºç›¸ä¼¼æ€§çš„æ¨è–¦ï¼Œæ³¨é‡å¤–è§€åŒ¹é…åº¦",
            'visual_focused': "ä»¥è¦–è¦ºç›¸ä¼¼ç‚ºä¸»å°ï¼Œå…¼é¡§ä¸»è¦è¦–è¦ºæˆåˆ†",
            'balanced': "å¹³è¡¡è¦–è¦ºç›¸ä¼¼æ€§å’Œé¢¨æ ¼ä¸€è‡´æ€§çš„æ¨è–¦",
            'style_aware': "å¼·èª¿é¢¨æ ¼ä¸€è‡´æ€§ï¼ŒåŒæ™‚ä¿æŒè¦–è¦ºå”èª¿"
        }
        
        if strategy in strategy_advice:
            advice.append(f"ç­–ç•¥ç‰¹é»: {strategy_advice[strategy]}")
        
        avg_visual = np.mean([r['detailed_similarity']['visual_similarity'] for r in similar_outfits])
        
        if avg_visual > 0.8:
            advice.append("è¦–è¦ºåŒ¹é…åº¦æ¥µä½³ï¼Œå¯ç›´æ¥åƒè€ƒæ¨è–¦ç©¿æ­")
        elif avg_visual > 0.7:
            advice.append("è¦–è¦ºåŒ¹é…åº¦è‰¯å¥½ï¼Œå»ºè­°å­¸ç¿’æ¨è–¦ç©¿æ­çš„æ­é…æŠ€å·§")
        else:
            advice.append("æ¨è–¦ç©¿æ­å¯ä½œç‚ºé¢¨æ ¼è½‰æ›çš„éˆæ„Ÿä¾†æº")
        
        return advice
    
    def generate_improved_display_image(self, results, output_path="improved_recommendation_display.png", max_recommendations=4):
        """ç”ŸæˆåŒ…å«æ¨è–¦åœ–ç‰‡å’Œå¤šæ¨¡æ…‹å»ºè­°çš„å±•ç¤ºåœ–ç‰‡"""
        if "error" in results:
            logging.error(f"âŒ ç„¡æ³•ç”Ÿæˆé¡¯ç¤ºåœ–ç‰‡: {results['error']}")
            return None
        
        logging.info(f"ğŸ¨ ç”Ÿæˆæ”¹é€²ç‰ˆé¡¯ç¤ºåœ–ç‰‡...")
        
        try:
            recommendations = results['similar_outfits'][:max_recommendations]
            num_recs = len(recommendations)
            total_images = num_recs + 1  # +1 for original image
            ai_models = results['ai_models_used']
            
            # è¨ˆç®—å­åœ–é«˜åº¦ï¼šåœ–ç‰‡ + æ¯å€‹æ¨¡å‹çš„å»ºè­°
            fig_height = 6 + 2 * len(ai_models)  # åœ–ç‰‡6è‹±å¯¸ï¼Œæ¯å€‹æ¨¡å‹å»ºè­°2è‹±å¯¸
            fig = plt.figure(figsize=(total_images * 4, fig_height))
            
            # åœ–ç‰‡é¡¯ç¤ºå€åŸŸ
            for i in range(total_images):
                ax = fig.add_subplot(len(ai_models) + 1, total_images, i + 1)
                
                if i == 0:
                    title = "åŸåœ–"
                    image_path = results['input_image']
                else:
                    rec = recommendations[i - 1]
                    title = f"æ¨è–¦ {i}\n{rec['style']}\nç›¸ä¼¼åº¦: {rec['similarity']:.3f}\nè©•åˆ†: {rec['score']:.1f}/10"
                    image_path = rec['path']
                
                self._show_image_subplot(ax, image_path, title)
            
            # æ¯å€‹æ¨¡å‹çš„å»ºè­°å€åŸŸ
            for model_idx, model_key in enumerate(ai_models):
                for rec_idx in range(num_recs):
                    ax = fig.add_subplot(len(ai_models) + 1, total_images, (model_idx + 1) * total_images + rec_idx + 2)
                    advice = recommendations[rec_idx]['comparison_advice']['ai_advice'].get(model_key, "ç„¡å»ºè­°")
                    model_name = self.model_configs[model_key]['name']
                    wrapped_text = textwrap.fill(f"{model_name}ï¼š\n{advice}", width=50)
                    ax.text(0.05, 0.95, wrapped_text, fontsize=8, va='top')
                    ax.axis('off')
            
            plt.tight_layout()
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            logging.info(f"âœ… é¡¯ç¤ºåœ–ç‰‡å·²ç”Ÿæˆ: {output_path}")
            return output_path
        except Exception as e:
            logging.error(f"âŒ ç”Ÿæˆé¡¯ç¤ºåœ–ç‰‡å¤±æ•—: {e}")
            return None
    
    def _show_image_subplot(self, ax, image_path, title):
        """é¡¯ç¤ºå–®å¼µåœ–ç‰‡"""
        try:
            if not os.path.exists(image_path):
                ax.text(0.5, 0.5, f"åœ–ç‰‡ä¸å­˜åœ¨\n{os.path.basename(image_path)}", 
                       ha='center', va='center', bbox=dict(facecolor='lightcoral', alpha=0.7))
            else:
                img = plt.imread(image_path)
                ax.imshow(img)
            
            ax.set_title(title, fontsize=10, fontweight='bold', pad=10)
            ax.axis('off')
        except Exception as e:
            ax.text(0.5, 0.5, f"è¼‰å…¥å¤±æ•—\n{os.path.basename(image_path)}", 
                   ha='center', va='center', bbox=dict(facecolor='lightcoral', alpha=0.7))
            ax.set_title(title, fontsize=10, fontweight='bold', pad=10)
            ax.axis('off')
    
    def print_improved_recommendations(self, results):
        """ç¾åŒ–æ‰“å°æ¨è–¦çµæœ"""
        if "error" in results:
            print(f"âŒ éŒ¯èª¤: {results['error']}")
            return
        
        print(f"\n{'='*80}")
        print(f"ğŸ¯ æ”¹é€²ç‰ˆAIç©¿æ­æ¨è–¦çµæœ")
        print(f"{'='*80}")
        
        print(f"ğŸ“¸ åˆ†æåœ–ç‰‡: {results['input_image']}")
        print(f"ğŸ‘¤ æ€§åˆ¥: {results['gender']}")
        print(f"ğŸ¨ é¢¨æ ¼åå¥½: {results['style_preference'] or 'ç„¡é™åˆ¶'}")
        print(f"ğŸ”§ ä½¿ç”¨ç­–ç•¥: {results['strategy']}")
        print(f"ğŸ¤– å¤šæ¨¡æ…‹æ¨¡å‹: {', '.join([self.model_configs[m]['name'] for m in results['ai_models_used']])}")
        
        weights = results['strategy_weights']
        print(f"âš–ï¸ ç‰¹å¾µæ¬Šé‡: åŸå§‹ç‰¹å¾µ{weights['original']:.1f} + PCAç‰¹å¾µ{weights['pca']:.1f} + æ˜ å°„ç‰¹å¾µ{weights['mapped']:.1f}")
        
        best_match = results['summary']['best_match']
        print(f"\nğŸ¥‡ æœ€ä½³åŒ¹é…:")
        print(f"  é¢¨æ ¼: {best_match['style']}")
        print(f"  ç¶œåˆè©•åˆ†: {best_match['score']:.1f}/10")
        print(f"  è¦–è¦ºç›¸ä¼¼åº¦: {best_match['detailed_similarity']['visual_similarity']:.3f}")
        
        analysis = results['style_analysis']
        print(f"\nğŸ“Š æ•´é«”åˆ†æ:")
        print(f"  ä¸»å°é¢¨æ ¼: {analysis['dominant_style']}")
        print(f"  å¹³å‡è¦–è¦ºç›¸ä¼¼åº¦: {analysis['average_visual_similarity']:.3f}")
        
        print(f"\nğŸ” è©³ç´°æ¨è–¦åˆ—è¡¨:")
        for i, rec in enumerate(results['similar_outfits'], 1):
            print(f"\n  {i}. {rec['style']} - ç¶œåˆè©•åˆ†: {rec['score']:.1f}/10")
            print(f"     è¦–è¦ºç›¸ä¼¼åº¦: {rec['detailed_similarity']['visual_similarity']:.3f}")
            print(f"     åœ–ç‰‡æª”å: {os.path.basename(rec['path'])}")
            print(f"     å¤šæ¨¡æ…‹æ¯”è¼ƒå»ºè­°:")
            for model_key, advice in rec['comparison_advice']['ai_advice'].items():
                model_name = self.model_configs[model_key]['name']
                print(f"       - {model_name}: {advice}")

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='æ”¹é€²ç‰ˆAIç©¿æ­æ¨è–¦ç³»çµ±')
    parser.add_argument('--image', type=str, required=True, help='ç”¨æˆ¶åœ–ç‰‡è·¯å¾‘')
    parser.add_argument('--gender', type=str, required=True, choices=['MEN', 'WOMEN'], help='æ€§åˆ¥')
    parser.add_argument('--top_k', type=int, default=4, help='æ¨è–¦æ•¸é‡')
    parser.add_argument('--style', type=str, help='é¢¨æ ¼åå¥½')
    parser.add_argument('--model', type=str, default='simple_fashion_model_final.pth', help='æ¨¡å‹è·¯å¾‘')
    parser.add_argument('--labels', type=str, default='simple_dataset_labels.json', help='æ¨™ç±¤æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--strategy', type=str, default='balanced', 
                       choices=['pure_visual', 'visual_focused', 'balanced', 'style_aware'], help='æ¨è–¦ç­–ç•¥')
    parser.add_argument('--ai_models', type=str, nargs='+', 
                       choices=['rule_based', 'clip', 'llava', 'blip2', 'instructblip'],
                       default=['rule_based', 'clip', 'llava', 'blip2', 'instructblip'], help='å¤šæ¨¡æ…‹æ¨¡å‹')
    parser.add_argument('--display', type=str, help='ç”Ÿæˆæ¯”è¼ƒåœ–ç‰‡è·¯å¾‘')
    parser.add_argument('--save', type=str, help='ä¿å­˜çµæœåˆ°JSONæ–‡ä»¶')
    
    args = parser.parse_args()
    
    try:
        system = ImprovedFashionRecommendationSystem(args.model, args.labels)
        
        results = system.analyze_and_recommend_improved(
            args.image, args.gender, args.top_k, args.style, args.strategy, args.ai_models
        )
        
        system.print_improved_recommendations(results)
        
        if args.display:
            display_path = system.generate_improved_display_image(results, args.display)
            if display_path:
                print(f"ğŸ–¼ï¸ æ¯”è¼ƒåœ–ç‰‡å·²ç”Ÿæˆ: {display_path}")
        
        if args.save:
            with open(args.save, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {args.save}")
            
    except Exception as e:
        print(f"âŒ ç³»çµ±éŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()