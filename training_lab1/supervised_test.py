#!/usr/bin/env python3
"""
æ™‚å°šAIç›£ç£å­¸ç¿’æ¸¬è©¦ç³»çµ±
ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹é€²è¡Œç©¿æ­åˆ†æå’Œæ¨è–¦
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import torchvision.models as models
import timm
from transformers import ViTModel, ViTConfig
from PIL import Image
import numpy as np
import os
import json
import pickle
from datetime import datetime
from sklearn.neighbors import NearestNeighbors
import argparse
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# ==================== Macå„ªåŒ–è¨­ç½® ====================

def setup_mac_optimization():
    """è¨­ç½®Macå„ªåŒ–"""
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ğŸ ä½¿ç”¨ Metal Performance Shaders (MPS)")
    else:
        device = torch.device("cpu")
        print("ğŸ’» ä½¿ç”¨ CPU")
    return device

# ==================== CudaåŠ é€Ÿè¨­ç½® ====================

def setup_cuda_optimization():
    """è¨­ç½® NVIDIA CUDA ç‰¹å®šçš„å„ªåŒ–"""
    if torch.cuda.is_available():
        print(f"âœ… æª¢æ¸¬åˆ° CUDA æ”¯æŒ: {torch.cuda.get_device_name(0)}")
        device = torch.device("cuda")
        
        # CUDA æ€§èƒ½å„ªåŒ–é¸é …
        torch.backends.cudnn.benchmark = True  # é©ç”¨æ–¼è¼¸å…¥å°ºå¯¸å›ºå®šçš„æ¨¡å‹
        torch.backends.cudnn.deterministic = False  # æé«˜é€Ÿåº¦ä½†çµæœéå®Œå…¨å¯é‡ç¾
    else:
        print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
        device = torch.device("cpu")
    return device

# ==================== æ¨¡å‹æ¶æ§‹å®šç¾©ï¼ˆèˆ‡è¨“ç·´æ™‚ç›¸åŒï¼‰ ====================

class FashionBackbone(nn.Module):
    """å¯é…ç½®çš„Fashion Backboneï¼ˆèˆ‡è¨“ç·´æ™‚ç›¸åŒï¼‰"""
    def __init__(self, backbone_type='mobilenet', pretrained=True):
        super(FashionBackbone, self).__init__()
        self.backbone_type = backbone_type
        
        if backbone_type == 'mobilenet':
            self.backbone = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1 if pretrained else None)
            self.backbone.classifier = nn.Identity()
            self.feature_dim = 960
            
        elif backbone_type == 'resnet18':
            self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)
            self.backbone.fc = nn.Identity()
            self.feature_dim = 512
            
        elif backbone_type == 'resnet50':
            self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1 if pretrained else None)
            self.backbone.fc = nn.Identity()
            self.feature_dim = 2048
            
        elif backbone_type == 'efficientnet_b0':
            self.backbone = timm.create_model('efficientnet_b0', pretrained=pretrained, num_classes=0)
            self.feature_dim = 1280
            
        elif backbone_type == 'efficientnet_b2':
            self.backbone = timm.create_model('efficientnet_b2', pretrained=pretrained, num_classes=0)
            self.feature_dim = 1408
            
        elif backbone_type == 'vit_tiny':
            if pretrained:
                self.backbone = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=0)
            else:
                config = ViTConfig(hidden_size=192, num_hidden_layers=12, num_attention_heads=3)
                self.backbone = ViTModel(config)
            self.feature_dim = 192
            
        elif backbone_type == 'vit_small':
            if pretrained:
                self.backbone = timm.create_model('vit_small_patch16_224', pretrained=True, num_classes=0)
            else:
                config = ViTConfig(hidden_size=384, num_hidden_layers=12, num_attention_heads=6)
                self.backbone = ViTModel(config)
            self.feature_dim = 384
            
        elif backbone_type == 'fashion_resnet':
            self.backbone = self._create_fashion_resnet(pretrained)
            self.feature_dim = 512
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„backboneé¡å‹: {backbone_type}")
    
    def _create_fashion_resnet(self, pretrained):
        """å‰µå»ºé‡å°æ™‚å°šåœ–ç‰‡å„ªåŒ–çš„ResNet"""
        base_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)
        base_model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        base_model.maxpool = nn.Identity()
        base_model.fc = nn.Identity()
        return base_model
    
    def forward(self, x):
        if 'vit' in self.backbone_type and hasattr(self.backbone, 'last_hidden_state'):
            outputs = self.backbone(x)
            if hasattr(outputs, 'last_hidden_state'):
                return outputs.last_hidden_state[:, 0, :]
            else:
                return outputs
        else:
            return self.backbone(x)
    
    def get_feature_dim(self):
        return self.feature_dim

class EnhancedStyleClassifier(nn.Module):
    """å¢å¼·é¢¨æ ¼åˆ†é¡å™¨ï¼ˆèˆ‡è¨“ç·´æ™‚ç›¸åŒï¼‰"""
    def __init__(self, backbone_type='mobilenet', num_styles=12, num_genders=2, feature_dim=1024):
        super(EnhancedStyleClassifier, self).__init__()
        
        self.backbone = FashionBackbone(
            backbone_type=backbone_type, 
            pretrained=True
        )
        
        backbone_features = self.backbone.get_feature_dim()
        self.feature_dim = feature_dim
        
        # ç‰¹å¾µæŠ•å½±å±¤
        self.feature_projection = nn.Sequential(
            nn.Linear(backbone_features, feature_dim),
            nn.LayerNorm(feature_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # é¢¨æ ¼åˆ†é¡é ­
        self.style_classifier = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_styles)
        )
        
        # æ€§åˆ¥åˆ†é¡é ­
        self.gender_classifier = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_genders)
        )
        
        # æŠ•å½±é ­ï¼ˆç”¨æ–¼å°æ¯”å­¸ç¿’ï¼‰
        self.projection_head = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.LayerNorm(128)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, (nn.BatchNorm1d, nn.LayerNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
        
    def forward(self, x):
        batch_size = x.size(0)
        if batch_size == 1 and self.training:
            self.eval()
            backbone_features = self.backbone(x)
            features = self.feature_projection(backbone_features)
            
            projected_features = self.projection_head(features)
            projected_features = F.normalize(projected_features, dim=1)
            
            style_logits = self.style_classifier(features)
            gender_logits = self.gender_classifier(features)
            
            self.train()
        else:
            backbone_features = self.backbone(x)
            features = self.feature_projection(backbone_features)
            
            projected_features = self.projection_head(features)
            projected_features = F.normalize(projected_features, dim=1)
            
            style_logits = self.style_classifier(features)
            gender_logits = self.gender_classifier(features)
        
        return {
            'features': features,
            'projected_features': projected_features,
            'style_logits': style_logits,
            'gender_logits': gender_logits
        }

# ==================== æ¨¡å‹è¼‰å…¥å·¥å…· ====================

def load_trained_model(model_path, device, backbone_type='mobilenet'):
    """è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹"""
    print(f"ğŸ“‚ è¼‰å…¥æ¨¡å‹: {model_path}")
    
    # é¢¨æ ¼å’Œæ€§åˆ¥é¡åˆ¥
    style_categories = [
        'Artsy', 'Athleisure', 'BRITISH', 'CASUAL', 'GOTH', 'Japanese',
        'Kawaii', 'Korean', 'Preppy', 'STREET', 'Vintage'
    ]
    gender_categories = ['MEN', 'WOMEN']
    
    # å‰µå»ºæ¨¡å‹
    model = EnhancedStyleClassifier(
        backbone_type=backbone_type,
        num_styles=len(style_categories),
        num_genders=len(gender_categories),
        feature_dim=1024
    )
    
    try:
        # è¼‰å…¥æ¬Šé‡
        checkpoint = torch.load(model_path, map_location=device)
        
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"âœ… è¼‰å…¥checkpointï¼Œepoch: {checkpoint.get('epoch', 'æœªçŸ¥')}")
        else:
            model.load_state_dict(checkpoint)
            print("âœ… è¼‰å…¥state_dict")
        
        model.to(device)
        model.eval()
        
        return model, style_categories, gender_categories
        
    except Exception as e:
        print(f"âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
        print("\nğŸ” æª¢æŸ¥æ¨¡å‹æª”æ¡ˆè³‡è¨Šï¼š")
        try:
            checkpoint = torch.load(model_path, map_location='cpu')
            if isinstance(checkpoint, dict):
                print(f"   Checkpoint keys: {list(checkpoint.keys())}")
                if 'model_state_dict' in checkpoint:
                    print(f"   Model keys (first 5): {list(checkpoint['model_state_dict'].keys())[:5]}")
            else:
                print(f"   State dict keys (first 5): {list(checkpoint.keys())[:5]}")
        except Exception as inspect_error:
            print(f"   ç„¡æ³•æª¢æŸ¥æª”æ¡ˆ: {inspect_error}")
        raise

# ==================== æ¨ç†åŠŸèƒ½ ====================

def predict_image(model, image_path, style_categories, gender_categories, device):
    """é æ¸¬å–®å¼µåœ–ç‰‡"""
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    try:
        # è¼‰å…¥å’Œé è™•ç†åœ–ç‰‡
        image = Image.open(image_path).convert('RGB')
        input_tensor = transform(image).unsqueeze(0).to(device)
        
        # æ¨ç†
        with torch.no_grad():
            outputs = model(input_tensor)
            
            # é¢¨æ ¼é æ¸¬
            style_probs = F.softmax(outputs['style_logits'], dim=1)
            style_probs_np = style_probs.cpu().numpy()[0]
            
            # æ€§åˆ¥é æ¸¬
            gender_probs = F.softmax(outputs['gender_logits'], dim=1)
            gender_probs_np = gender_probs.cpu().numpy()[0]
            
            # ç²å–æœ€ä½³é æ¸¬
            top_style_idx = np.argmax(style_probs_np)
            top_gender_idx = np.argmax(gender_probs_np)
            
            # è¨ˆç®—ä¿¡å¿ƒåº¦è©•åˆ†
            style_confidence = style_probs_np[top_style_idx]
            gender_confidence = gender_probs_np[top_gender_idx]
            
            # ç¶œåˆè©•åˆ†
            overall_score = (style_confidence * 0.7 + gender_confidence * 0.3) * 10
            
            result = {
                'predicted_style': style_categories[top_style_idx],
                'predicted_gender': gender_categories[top_gender_idx],
                'style_confidence': float(style_confidence),
                'gender_confidence': float(gender_confidence),
                'overall_score': float(overall_score),
                'style_probabilities': {
                    style_categories[i]: float(prob) 
                    for i, prob in enumerate(style_probs_np)
                },
                'gender_probabilities': {
                    gender_categories[i]: float(prob) 
                    for i, prob in enumerate(gender_probs_np)
                },
                'features': outputs['features'].cpu().numpy()[0],
                'projected_features': outputs['projected_features'].cpu().numpy()[0]
            }
            
            return result
            
    except Exception as e:
        print(f"âŒ é æ¸¬å¤±æ•—: {e}")
        raise

def extract_features_from_dataset(model, dataset_root, device, max_images_per_class=100):
    """å¾è¨“ç·´æ•¸æ“šé›†æå–ç‰¹å¾µç”¨æ–¼ç›¸ä¼¼åº¦æœç´¢"""
    print(f"ğŸ”¨ å¾æ•¸æ“šé›†æå–ç‰¹å¾µ: {dataset_root}")
    
    style_categories = [
        'Artsy', 'Athleisure', 'BRITISH', 'CASUAL', 'GOTH', 'Japanese',
        'Kawaii', 'Korean', 'Preppy', 'STREET', 'Vintage'
    ]
    gender_categories = ['MEN', 'WOMEN']
    
    
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    feature_database = {
        'features': [],
        'projected_features': [],
        'image_paths': [],
        'styles': [],
        'genders': []
    }
    
    total_images = 0
    
    # æƒææ•¸æ“šé›†
    for style in style_categories:
        for gender in gender_categories:
            folder_name = f"{style}_{gender}"
            folder_path = os.path.join(dataset_root, folder_name)
            
            if not os.path.exists(folder_path):
                continue
            
            print(f"   ğŸ“ è™•ç†: {folder_name}")
            
            img_files = [f for f in os.listdir(folder_path) 
                        if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
            
            img_files = img_files[:max_images_per_class]
            
            for img_name in img_files:
                img_path = os.path.join(folder_path, img_name)
                
                try:
                    image = Image.open(img_path).convert('RGB')
                    input_tensor = transform(image).unsqueeze(0).to(device)
                    
                    with torch.no_grad():
                        outputs = model(input_tensor)
                        features = outputs['features'].cpu().numpy()[0]
                        projected_features = outputs['projected_features'].cpu().numpy()[0]
                    
                    feature_database['features'].append(features)
                    feature_database['projected_features'].append(projected_features)
                    feature_database['image_paths'].append(img_path)
                    feature_database['styles'].append(style)
                    feature_database['genders'].append(gender)
                    
                    total_images += 1
                    
                    if total_images % 50 == 0:
                        print(f"     å·²è™•ç† {total_images} å¼µåœ–ç‰‡...")
                        
                except Exception as e:
                    print(f"âš ï¸ è·³éåœ–ç‰‡ {img_path}: {e}")
                    continue
    
    if total_images == 0:
        raise ValueError("æ•¸æ“šé›†ä¸­æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„åœ–ç‰‡æ–‡ä»¶")
    
    feature_database['features'] = np.array(feature_database['features'])
    feature_database['projected_features'] = np.array(feature_database['projected_features'])
    
    print(f"âœ… ç‰¹å¾µæå–å®Œæˆï¼Œå…± {total_images} å¼µåœ–ç‰‡")
    return feature_database

def find_similar_images(user_features, feature_database, top_k=10):
    """æ‰¾åˆ°æœ€ç›¸ä¼¼çš„åœ–ç‰‡"""
    print(f"ğŸ” æœç´¢æœ€ç›¸ä¼¼çš„ {top_k} å¼µåœ–ç‰‡...")
    
    searcher = NearestNeighbors(
        n_neighbors=min(top_k * 2, len(feature_database['projected_features'])),
        metric='cosine',
        algorithm='brute'
    )
    
    searcher.fit(feature_database['projected_features'])
    
    distances, indices = searcher.kneighbors(
        user_features['projected_features'].reshape(1, -1)
    )
    
    similar_images = []
    for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
        if i >= top_k:
            break
            
        similar_info = {
            'rank': i + 1,
            'similarity_score': float(1 - distance),
            'image_path': feature_database['image_paths'][idx],
            'style': feature_database['styles'][idx],
            'gender': feature_database['genders'][idx],
            'distance': float(distance)
        }
        
        similar_images.append(similar_info)
    
    return similar_images

def create_recommendation_report(user_prediction, similar_images, output_dir="./recommendations"):
    """å‰µå»ºæ¨è–¦å ±å‘Š"""
    os.makedirs(output_dir, exist_ok=True)
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'user_prediction': {
            'style': user_prediction['predicted_style'],
            'gender': user_prediction['predicted_gender'],
            'style_confidence': user_prediction['style_confidence'],
            'gender_confidence': user_prediction['gender_confidence'],
            'overall_score': user_prediction['overall_score']
        },
        'similar_images': similar_images,
        'style_analysis': {}
    }
    
    # åˆ†æç›¸ä¼¼åœ–ç‰‡çš„é¢¨æ ¼åˆ†ä½ˆ
    style_distribution = defaultdict(int)
    for img in similar_images:
        style_distribution[img['style']] += 1
    
    report['style_analysis']['style_distribution'] = dict(style_distribution)
    report['style_analysis']['most_common_style'] = max(style_distribution.items(), key=lambda x: x[1])[0] if style_distribution else None
    
    # ä¿å­˜å ±å‘Š
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = os.path.join(output_dir, f"recommendation_report_{timestamp}.json")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“Š æ¨è–¦å ±å‘Šå·²ä¿å­˜: {report_path}")
    
    return report, report_path

def print_analysis_results(user_prediction, similar_images):
    """æ‰“å°åˆ†æçµæœ"""
    print(f"\n{'='*80}")
    print(f"ğŸ‘— ç©¿æ­åˆ†æçµæœ")
    print(f"{'='*80}")
    
    # ç”¨æˆ¶é æ¸¬çµæœ
    print(f"ğŸ¨ é æ¸¬é¢¨æ ¼: {user_prediction['predicted_style']} ({user_prediction['style_confidence']:.1%})")
    print(f"ğŸ‘¤ é æ¸¬æ€§åˆ¥: {user_prediction['predicted_gender']} ({user_prediction['gender_confidence']:.1%})")
    print(f"â­ ç¶œåˆè©•åˆ†: {user_prediction['overall_score']:.1f}/10")
    
    # é¢¨æ ¼æ©Ÿç‡åˆ†ä½ˆï¼ˆé¡¯ç¤ºå‰5åï¼‰
    style_probs = sorted(user_prediction['style_probabilities'].items(), key=lambda x: x[1], reverse=True)
    print(f"\nğŸ“Š é¢¨æ ¼æ©Ÿç‡åˆ†ä½ˆ (å‰5å):")
    for style, prob in style_probs[:5]:
        print(f"  {style}: {prob:.1%}")
    
    # ç›¸ä¼¼åœ–ç‰‡
    print(f"\nğŸ” æœ€ç›¸ä¼¼çš„ç©¿æ­ (å‰{min(len(similar_images), 5)}å):")
    for img in similar_images[:5]:
        print(f"  {img['rank']}. {os.path.basename(img['image_path'])}")
        print(f"     é¢¨æ ¼: {img['style']}, æ€§åˆ¥: {img['gender']}")
        print(f"     ç›¸ä¼¼åº¦: {img['similarity_score']:.3f}")
    
    # é¢¨æ ¼å»ºè­°
    style_distribution = defaultdict(int)
    for img in similar_images:
        style_distribution[img['style']] += 1
    
    if style_distribution:
        print(f"\nğŸ’¡ é¢¨æ ¼å»ºè­°:")
        most_common = max(style_distribution.items(), key=lambda x: x[1])
        if most_common[0] != user_prediction['predicted_style']:
            print(f"  â€¢ è€ƒæ…®å˜—è©¦ {most_common[0]} é¢¨æ ¼ï¼Œåœ¨ç›¸ä¼¼ç©¿æ­ä¸­å‡ºç¾ {most_common[1]} æ¬¡")
        
        print(f"  â€¢ ç›¸ä¼¼ç©¿æ­é¢¨æ ¼åˆ†ä½ˆ:")
        for style, count in sorted(style_distribution.items(), key=lambda x: x[1], reverse=True):
            print(f"    - {style}: {count} å¼µ")
    
    print(f"\n{'='*80}")

# ==================== ä¸»ç¨‹åº ====================

def main():
    parser = argparse.ArgumentParser(description='æ™‚å°šé¢¨æ ¼åˆ†é¡æ¸¬è©¦ç³»çµ±')
    parser.add_argument('--model', type=str, required=True,
                       help='è¨“ç·´å¥½çš„æ¨¡å‹è·¯å¾‘')
    parser.add_argument('--image', type=str, required=True,
                       help='è¦åˆ†æçš„åœ–ç‰‡è·¯å¾‘')
    parser.add_argument('--dataset', type=str, default='../dataset',
                       help='è¨“ç·´æ•¸æ“šé›†æ ¹ç›®éŒ„ï¼ˆç”¨æ–¼ç›¸ä¼¼åº¦æœç´¢ï¼‰')
    parser.add_argument('--backbone', type=str, default='mobilenet',
                       choices=['mobilenet', 'resnet18', 'resnet50', 'efficientnet_b0', 
                               'efficientnet_b2', 'vit_tiny', 'vit_small', 'fashion_resnet'],
                       help='Backboneæ¶æ§‹é¡å‹')
    parser.add_argument('--platform', type=str, default='auto',
                        choices=['mps', 'cuda', 'cpu', 'auto'],
                        help='æ‰€ä½¿ç”¨çš„ç¡¬é«”è£ç½®')
    parser.add_argument('--output-dir', type=str, default='./recommendations',
                       help='çµæœè¼¸å‡ºç›®éŒ„')
    parser.add_argument('--top-k', type=int, default=10,
                       help='è¿”å›æœ€ç›¸ä¼¼çš„Kå¼µåœ–ç‰‡')
    parser.add_argument('--cache-features', type=str, default='test_features_cache.pkl',
                       help='ç‰¹å¾µç·©å­˜æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--rebuild-cache', action='store_true',
                       help='é‡æ–°å»ºç«‹ç‰¹å¾µç·©å­˜')
    
    args = parser.parse_args()
    
    print("ğŸ¤– æ™‚å°šé¢¨æ ¼åˆ†é¡æ¸¬è©¦ç³»çµ±")
    print("=" * 50)
    
    # æª¢æŸ¥æ–‡ä»¶
    if not os.path.exists(args.model):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model}")
        return
    
    if not os.path.exists(args.image):
        print(f"âŒ åœ–ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {args.image}")
        return
    
    try:
        # è¨­å‚™è¨­ç½®
        device = torch.device("cpu")
        if args.platform == "auto":
            device = setup_cuda_optimization() # prefer cuda first
            if device == torch.device("cpu"):
                device = setup_mac_optimization()
        else:
            if args.platform == "cuda":
                device = setup_cuda_optimization()
            elif args.platform == "mps":
                device = setup_mac_optimization()
        
        # è¼‰å…¥æ¨¡å‹
        model, style_categories, gender_categories = load_trained_model(
            args.model, device, args.backbone
        )
        
        # é æ¸¬ç”¨æˆ¶åœ–ç‰‡
        print(f"ğŸ” åˆ†æåœ–ç‰‡: {args.image}")
        user_prediction = predict_image(
            model, args.image, style_categories, gender_categories, device
        )
        
        # è¼‰å…¥æˆ–å»ºç«‹ç‰¹å¾µæ•¸æ“šåº«
        if args.rebuild_cache or not os.path.exists(args.cache_features):
            print("ğŸ”¨ å»ºç«‹ç‰¹å¾µæ•¸æ“šåº«...")
            feature_database = extract_features_from_dataset(
                model, args.dataset, device
            )
            
            # ä¿å­˜ç·©å­˜
            with open(args.cache_features, 'wb') as f:
                pickle.dump(feature_database, f)
            print(f"ğŸ’¾ ç‰¹å¾µç·©å­˜å·²ä¿å­˜: {args.cache_features}")
        else:
            print(f"ğŸ“ è¼‰å…¥ç‰¹å¾µç·©å­˜: {args.cache_features}")
            with open(args.cache_features, 'rb') as f:
                feature_database = pickle.load(f)
        
        # æœç´¢ç›¸ä¼¼åœ–ç‰‡
        similar_images = find_similar_images(
            user_prediction, feature_database, args.top_k
        )
        
        # ç”Ÿæˆå ±å‘Š
        report, report_path = create_recommendation_report(
            user_prediction, similar_images, args.output_dir
        )
        
        # é¡¯ç¤ºçµæœ
        print_analysis_results(user_prediction, similar_images)
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼å ±å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
    except Exception as e:
        print(f"âŒ ç³»çµ±é‹è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 