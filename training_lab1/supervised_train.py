#!/usr/bin/env python3
"""
æ™‚å°šAIç›£ç£å­¸ç¿’è¨“ç·´ç³»çµ±
æ”¯æ´å¤šç¨®backboneæ¶æ§‹çš„ç©¿æ­é¢¨æ ¼åˆ†é¡è¨“ç·´
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
import torchvision.models as models
import timm
from transformers import ViTModel, ViTConfig
import numpy as np
from PIL import Image
import os
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
from collections import defaultdict
import json
import time
import gc
import psutil
import multiprocessing as mp
import argparse
from datetime import datetime

# ==================== Macå„ªåŒ–è¨­ç½® ====================

def setup_mac_optimization():
    """è¨­ç½®Macç‰¹å®šçš„å„ªåŒ–"""
    if mp.get_start_method(allow_none=True) != 'spawn':
        mp.set_start_method('spawn', force=True)
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        print("âœ… æª¢æ¸¬åˆ° Metal Performance Shaders (MPS) æ”¯æŒ")
        device = torch.device("mps")
    else:
        print("âš ï¸  MPS ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
        device = torch.device("cpu")
    
    torch.set_num_threads(min(8, mp.cpu_count()))
    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
    
    return device

# ==================== è¼¸å‡ºç®¡ç† ====================

def create_output_directory(base_dir="result"):
    """å‰µå»ºè¼¸å‡ºç›®éŒ„ï¼Œæ ¼å¼ç‚º result/[yyyymmddhhmmss]/"""
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    output_dir = os.path.join(base_dir, timestamp)
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {output_dir}")
    return output_dir

def save_config_to_file(config, output_dir):
    """ä¿å­˜è¨“ç·´é…ç½®åˆ°æ–‡ä»¶"""
    config_dict = {
        'config_type': config.config_type,
        'backbone_type': config.backbone_type,
        'num_styles': config.num_styles,
        'num_genders': config.num_genders,
        'feature_dim': config.feature_dim,
        'batch_size': config.batch_size,
        'num_epochs': config.num_epochs,
        'learning_rate': config.learning_rate,
        'max_samples_per_class': config.max_samples_per_class,
        'num_workers': config.num_workers,
        'max_memory_mb': config.max_memory_mb,
        'style_categories': config.style_categories,
        'gender_categories': config.gender_categories,
        'timestamp': datetime.now().isoformat()
    }
    
    config_path = os.path.join(output_dir, 'training_config.json')
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ è¨“ç·´é…ç½®å·²ä¿å­˜: {config_path}")

def save_training_history_plot(train_history, output_dir):
    """ä¿å­˜è¨“ç·´æ­·å²åœ–è¡¨"""
    if not train_history['epoch']:
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Training History', fontsize=16)
    
    # æå¤±åœ–
    axes[0, 0].plot(train_history['epoch'], train_history['loss'], 'b-', label='Loss')
    axes[0, 0].set_title('Training Loss')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # æº–ç¢ºç‡åœ–
    axes[0, 1].plot(train_history['epoch'], train_history['style_acc'], 'r-', label='Style Accuracy')
    axes[0, 1].plot(train_history['epoch'], train_history['gender_acc'], 'g-', label='Gender Accuracy')
    axes[0, 1].set_title('Training Accuracy')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Accuracy')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # å­¸ç¿’ç‡åœ–
    axes[1, 0].plot(train_history['epoch'], train_history['lr'], 'm-', label='Learning Rate')
    axes[1, 0].set_title('Learning Rate')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Learning Rate')
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    
    # è¨˜æ†¶é«”ä½¿ç”¨åœ–
    axes[1, 1].plot(train_history['epoch'], train_history['memory_usage'], 'c-', label='Memory Usage')
    axes[1, 1].set_title('Memory Usage')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Memory (MB)')
    axes[1, 1].legend()
    axes[1, 1].grid(True)
    
    plt.tight_layout()
    
    plot_path = os.path.join(output_dir, 'training_history.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š è¨“ç·´æ­·å²åœ–è¡¨å·²ä¿å­˜: {plot_path}")

# ==================== é…ç½®ç®¡ç† ====================

class TrainingConfig:
    """è¨“ç·´é…ç½®é¡"""
    def __init__(self, config_type="balanced", backbone_type="mobilenet"):
        self.config_type = config_type
        self.backbone_type = backbone_type
        
        # åŸºç¤é…ç½®
        self.num_styles = 12
        self.num_genders = 2
        self.feature_dim = 1024
        
        # é¢¨æ ¼å’Œæ€§åˆ¥é¡åˆ¥
        self.style_categories = [
            'Athleisure', 'BRITISH', 'CASUAL', 'GOTH', 'Kawaii', 
            'Korean', 'MINIMALIST', 'Preppy', 'STREET', 'Streetwear', 
            'Vintage', 'Y2K'
        ]
        self.gender_categories = ['MEN', 'WOMEN']
        
        # æ ¹æ“šé…ç½®é¡å‹è¨­ç½®åƒæ•¸
        if config_type == "minimal":
            self._set_minimal_config()
        elif config_type == "performance":
            self._set_performance_config()
        else:  # balanced
            self._set_balanced_config()
    
    def _set_minimal_config(self):
        """æœ€å°é…ç½® - é©åˆè¨˜æ†¶é«”æœ‰é™çš„æƒ…æ³"""
        self.batch_size = 2
        self.num_epochs = 10
        self.learning_rate = 0.001
        self.max_samples_per_class = 100
        self.num_workers = 0
        self.max_memory_mb = 4000
        
    def _set_balanced_config(self):
        """å¹³è¡¡é…ç½® - æ¨è–¦è¨­ç½®"""
        self.batch_size = 8
        self.num_epochs = 30
        self.learning_rate = 0.001
        self.max_samples_per_class = 500
        self.num_workers = 2
        self.max_memory_mb = 8000
        
    def _set_performance_config(self):
        """æ€§èƒ½é…ç½® - é©åˆé«˜æ€§èƒ½Mac"""
        self.batch_size = 16
        self.num_epochs = 50
        self.learning_rate = 0.0005
        self.max_samples_per_class = 1000
        self.num_workers = 4
        self.max_memory_mb = 16000

# ==================== Backboneå®šç¾© ====================

class FashionBackbone(nn.Module):
    """å¯é…ç½®çš„Fashion Backbone"""
    def __init__(self, backbone_type='mobilenet', pretrained=True):
        super(FashionBackbone, self).__init__()
        self.backbone_type = backbone_type
        
        if backbone_type == 'mobilenet':
            self.backbone = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1 if pretrained else None)
            self.backbone.classifier = nn.Identity()
            self.feature_dim = 960
            
        elif backbone_type == 'resnet18':
            self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)
            self.backbone.fc = nn.Identity()
            self.feature_dim = 512
            
        elif backbone_type == 'resnet50':
            self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1 if pretrained else None)
            self.backbone.fc = nn.Identity()
            self.feature_dim = 2048
            
        elif backbone_type == 'efficientnet_b0':
            self.backbone = timm.create_model('efficientnet_b0', pretrained=pretrained, num_classes=0)
            self.feature_dim = 1280
            
        elif backbone_type == 'efficientnet_b2':
            self.backbone = timm.create_model('efficientnet_b2', pretrained=pretrained, num_classes=0)
            self.feature_dim = 1408
            
        elif backbone_type == 'vit_tiny':
            if pretrained:
                self.backbone = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=0)
            else:
                config = ViTConfig(hidden_size=192, num_hidden_layers=12, num_attention_heads=3)
                self.backbone = ViTModel(config)
            self.feature_dim = 192
            
        elif backbone_type == 'vit_small':
            if pretrained:
                self.backbone = timm.create_model('vit_small_patch16_224', pretrained=True, num_classes=0)
            else:
                config = ViTConfig(hidden_size=384, num_hidden_layers=12, num_attention_heads=6)
                self.backbone = ViTModel(config)
            self.feature_dim = 384
            
        elif backbone_type == 'fashion_resnet':
            self.backbone = self._create_fashion_resnet(pretrained)
            self.feature_dim = 512
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„backboneé¡å‹: {backbone_type}")
    
    def _create_fashion_resnet(self, pretrained):
        """å‰µå»ºé‡å°æ™‚å°šåœ–ç‰‡å„ªåŒ–çš„ResNet"""
        base_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)
        base_model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        base_model.maxpool = nn.Identity()
        base_model.fc = nn.Identity()
        return base_model
    
    def forward(self, x):
        if 'vit' in self.backbone_type and hasattr(self.backbone, 'last_hidden_state'):
            outputs = self.backbone(x)
            if hasattr(outputs, 'last_hidden_state'):
                return outputs.last_hidden_state[:, 0, :]
            else:
                return outputs
        else:
            return self.backbone(x)
    
    def get_feature_dim(self):
        return self.feature_dim

# ==================== æ•¸æ“šé›† ====================

class OptimizedOutfitDataset(Dataset):
    def __init__(self, root_dir, config, transform=None, mode='train'):
        self.root_dir = root_dir
        self.config = config
        self.transform = transform
        self.mode = mode
        
        self.style_to_idx = {style: idx for idx, style in enumerate(config.style_categories)}
        self.gender_to_idx = {gender: idx for idx, gender in enumerate(config.gender_categories)}
        
        self.samples = []
        self._load_samples()
        
        print(f"ğŸ“Š è¼‰å…¥äº† {len(self.samples)} å€‹æ¨£æœ¬")
        
    def _load_samples(self):
        class_counts = defaultdict(int)
        
        for style in self.config.style_categories:
            for gender in self.config.gender_categories:
                folder_name = f"{style}_{gender}"
                folder_path = os.path.join(self.root_dir, folder_name)
                
                if os.path.exists(folder_path):
                    img_files = [f for f in os.listdir(folder_path) 
                               if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                    
                    img_files = img_files[:self.config.max_samples_per_class]
                    
                    for img_name in img_files:
                        img_path = os.path.join(folder_path, img_name)
                        
                        if os.path.getsize(img_path) < 1024:
                            continue
                            
                        if self._is_valid_image(img_path):
                            self.samples.append({
                                'path': img_path,
                                'style': style,
                                'gender': gender,
                                'style_idx': self.style_to_idx[style],
                                'gender_idx': self.gender_to_idx[gender]
                            })
                            class_counts[f"{style}_{gender}"] += 1
        
        print("ğŸ“ˆ å„é¡åˆ¥æ¨£æœ¬æ•¸é‡:")
        for class_name, count in sorted(class_counts.items()):
            print(f"  {class_name}: {count}")
    
    def _is_valid_image(self, img_path):
        """å¿«é€Ÿæª¢æŸ¥åœ–ç‰‡æ˜¯å¦æœ‰æ•ˆ"""
        try:
            with Image.open(img_path) as img:
                img.verify()
                return True
        except Exception:
            return False
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        try:
            image = Image.open(sample['path']).convert('RGB')
            if min(image.size) < 32:
                raise ValueError("åœ–ç‰‡å¤ªå°")
        except Exception as e:
            print(f"âš ï¸  è¼‰å…¥åœ–ç‰‡å¤±æ•—: {sample['path']}")
            image = Image.new('RGB', (224, 224), color=(128, 128, 128))
        
        if self.transform:
            image = self.transform(image)
            
        return {
            'image': image,
            'style_idx': sample['style_idx'],
            'gender_idx': sample['gender_idx'],
            'style': sample['style'],
            'gender': sample['gender'],
            'path': sample['path']
        }

# ==================== æ¨¡å‹å®šç¾© ====================

class EnhancedStyleClassifier(nn.Module):
    def __init__(self, config):
        super(EnhancedStyleClassifier, self).__init__()
        
        self.backbone = FashionBackbone(
            backbone_type=config.backbone_type, 
            pretrained=True
        )
        
        backbone_features = self.backbone.get_feature_dim()
        self.feature_dim = config.feature_dim
        
        # ç‰¹å¾µæŠ•å½±å±¤
        self.feature_projection = nn.Sequential(
            nn.Linear(backbone_features, config.feature_dim),
            nn.LayerNorm(config.feature_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # é¢¨æ ¼åˆ†é¡é ­
        self.style_classifier = nn.Sequential(
            nn.Linear(config.feature_dim, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, config.num_styles)
        )
        
        # æ€§åˆ¥åˆ†é¡é ­
        self.gender_classifier = nn.Sequential(
            nn.Linear(config.feature_dim, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, config.num_genders)
        )
        
        # æŠ•å½±é ­ï¼ˆç”¨æ–¼å°æ¯”å­¸ç¿’ï¼‰
        self.projection_head = nn.Sequential(
            nn.Linear(config.feature_dim, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.LayerNorm(128)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, (nn.BatchNorm1d, nn.LayerNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
        
    def forward(self, x):
        batch_size = x.size(0)
        if batch_size == 1 and self.training:
            self.eval()
            backbone_features = self.backbone(x)
            features = self.feature_projection(backbone_features)
            
            projected_features = self.projection_head(features)
            projected_features = F.normalize(projected_features, dim=1)
            
            style_logits = self.style_classifier(features)
            gender_logits = self.gender_classifier(features)
            
            self.train()
        else:
            backbone_features = self.backbone(x)
            features = self.feature_projection(backbone_features)
            
            projected_features = self.projection_head(features)
            projected_features = F.normalize(projected_features, dim=1)
            
            style_logits = self.style_classifier(features)
            gender_logits = self.gender_classifier(features)
        
        return {
            'features': features,
            'projected_features': projected_features,
            'style_logits': style_logits,
            'gender_logits': gender_logits
        }

# ==================== æå¤±å‡½æ•¸ ====================

class MemoryEfficientContrastiveLoss(nn.Module):
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, features, labels):
        batch_size = features.shape[0]
        if batch_size < 2:
            return torch.tensor(0.0, device=features.device)
        
        chunk_size = min(32, batch_size)
        total_loss = 0.0
        num_chunks = 0
        
        for i in range(0, batch_size, chunk_size):
            end_i = min(i + chunk_size, batch_size)
            chunk_features = features[i:end_i]
            chunk_labels = labels[i:end_i]
            
            similarity_matrix = torch.matmul(chunk_features, features.T) / self.temperature
            label_mask = (chunk_labels.unsqueeze(1) == labels.unsqueeze(0)).float()
            mask = torch.eye(len(chunk_labels), batch_size, device=features.device)
            label_mask = label_mask * (1 - mask)
            
            exp_sim = torch.exp(similarity_matrix)
            log_prob = similarity_matrix - torch.log(exp_sim.sum(dim=1, keepdim=True))
            
            positive_mask = label_mask
            if positive_mask.sum() > 0:
                loss = -(log_prob * positive_mask).sum() / positive_mask.sum()
                total_loss += loss
                num_chunks += 1
        
        return total_loss / max(num_chunks, 1)

# ==================== è¨“ç·´å™¨ ====================

class MacOptimizedTrainer:
    def __init__(self, model, config, device, output_dir):
        self.model = model
        self.config = config
        self.device = device
        self.output_dir = output_dir
        
        # æå¤±å‡½æ•¸
        self.style_criterion = nn.CrossEntropyLoss()
        self.gender_criterion = nn.CrossEntropyLoss()
        self.contrastive_criterion = MemoryEfficientContrastiveLoss()
        
        # å„ªåŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=config.learning_rate,
            weight_decay=0.01
        )
        
        # å­¸ç¿’ç‡èª¿åº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=config.num_epochs
        )
        
        # è¨“ç·´è¨˜éŒ„
        self.train_history = {
            'epoch': [],
            'loss': [],
            'style_acc': [],
            'gender_acc': [],
            'lr': [],
            'memory_usage': []
        }
        
        # å‰µå»ºè¨“ç·´æ—¥èªŒæ–‡ä»¶
        self.log_file = os.path.join(output_dir, 'training_log.txt')
        with open(self.log_file, 'w', encoding='utf-8') as f:
            f.write(f"è¨“ç·´é–‹å§‹æ™‚é–“: {datetime.now().isoformat()}\n")
            f.write(f"é…ç½®: {config.config_type}\n")
            f.write(f"Backbone: {config.backbone_type}\n")
            f.write("="*50 + "\n")
    
    def log_message(self, message):
        """è¨˜éŒ„è¨“ç·´æ—¥èªŒ"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_msg = f"[{timestamp}] {message}"
        print(log_msg)
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_msg + "\n")
    
    def get_memory_usage(self):
        """ç²å–è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024  # MB
    
    def train_epoch(self, dataloader, epoch):
        """è¨“ç·´ä¸€å€‹epoch"""
        self.model.train()
        total_loss = 0.0
        style_correct = 0
        gender_correct = 0
        total_samples = 0
        
        for batch_idx, batch in enumerate(dataloader):
            images = batch['image'].to(self.device)
            style_labels = batch['style_idx'].to(self.device)
            gender_labels = batch['gender_idx'].to(self.device)
            
            # æª¢æŸ¥æ‰¹æ¬¡å¤§å°
            if images.size(0) == 1:
                continue
            
            self.optimizer.zero_grad()
            
            # å‰å‘å‚³æ’­
            outputs = self.model(images)
            
            # è¨ˆç®—æå¤±
            style_loss = self.style_criterion(outputs['style_logits'], style_labels)
            gender_loss = self.gender_criterion(outputs['gender_logits'], gender_labels)
            
            # å°æ¯”æå¤±ï¼ˆçµ„åˆé¢¨æ ¼å’Œæ€§åˆ¥æ¨™ç±¤ï¼‰
            combined_labels = style_labels * self.config.num_genders + gender_labels
            contrastive_loss = self.contrastive_criterion(
                outputs['projected_features'], 
                combined_labels
            )
            
            # ç¸½æå¤±
            total_batch_loss = style_loss + gender_loss + 0.1 * contrastive_loss
            
            # åå‘å‚³æ’­
            total_batch_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # çµ±è¨ˆ
            total_loss += total_batch_loss.item()
            
            # è¨ˆç®—æº–ç¢ºç‡
            style_pred = outputs['style_logits'].argmax(dim=1)
            gender_pred = outputs['gender_logits'].argmax(dim=1)
            
            style_correct += (style_pred == style_labels).sum().item()
            gender_correct += (gender_pred == gender_labels).sum().item()
            total_samples += style_labels.size(0)
            
            # è¨˜æ†¶é«”ç®¡ç†
            if batch_idx % 10 == 0:
                if hasattr(torch.cuda, 'empty_cache'):
                    torch.cuda.empty_cache()
                gc.collect()
            
            # é€²åº¦é¡¯ç¤º
            if batch_idx % 20 == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                memory_usage = self.get_memory_usage()
                
                self.log_message(
                    f"Epoch {epoch}/{self.config.num_epochs}, "
                    f"Batch {batch_idx}/{len(dataloader)}, "
                    f"Loss: {total_batch_loss.item():.4f}, "
                    f"LR: {current_lr:.6f}, "
                    f"Memory: {memory_usage:.1f}MB"
                )
        
        # Epochçµ±è¨ˆ
        avg_loss = total_loss / len(dataloader)
        style_acc = style_correct / total_samples
        gender_acc = gender_correct / total_samples
        current_lr = self.optimizer.param_groups[0]['lr']
        memory_usage = self.get_memory_usage()
        
        # è¨˜éŒ„æ­·å²
        self.train_history['epoch'].append(epoch)
        self.train_history['loss'].append(avg_loss)
        self.train_history['style_acc'].append(style_acc)
        self.train_history['gender_acc'].append(gender_acc)
        self.train_history['lr'].append(current_lr)
        self.train_history['memory_usage'].append(memory_usage)
        
        self.log_message(
            f"Epoch {epoch} å®Œæˆ - "
            f"å¹³å‡æå¤±: {avg_loss:.4f}, "
            f"é¢¨æ ¼æº–ç¢ºç‡: {style_acc:.4f}, "
            f"æ€§åˆ¥æº–ç¢ºç‡: {gender_acc:.4f}"
        )
        
        return avg_loss
    
    def save_checkpoint(self, epoch, loss, filename):
        """ä¿å­˜æª¢æŸ¥é»"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'loss': loss,
            'config': {
                'backbone_type': self.config.backbone_type,
                'num_styles': self.config.num_styles,
                'num_genders': self.config.num_genders,
                'feature_dim': self.config.feature_dim
            }
        }
        
        filepath = os.path.join(self.output_dir, filename)
        torch.save(checkpoint, filepath)
        self.log_message(f"æª¢æŸ¥é»å·²ä¿å­˜: {filepath}")

def train_model(data_root, config_type="balanced", backbone_type="mobilenet", resume_from=None):
    """ä¸»è¨“ç·´å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹è¨“ç·´æ™‚å°šé¢¨æ ¼åˆ†é¡æ¨¡å‹")
    print("=" * 50)
    
    # è¨­å‚™è¨­ç½®
    device = setup_mac_optimization()
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir = create_output_directory()
    
    # å‰µå»ºé…ç½®
    config = TrainingConfig(config_type, backbone_type)
    save_config_to_file(config, output_dir)
    
    print(f"ğŸ“Š è¨“ç·´é…ç½®:")
    print(f"  é…ç½®é¡å‹: {config.config_type}")
    print(f"  Backbone: {config.backbone_type}")
    print(f"  æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"  è¨“ç·´è¼ªæ•¸: {config.num_epochs}")
    print(f"  å­¸ç¿’ç‡: {config.learning_rate}")
    
    # æ•¸æ“šé è™•ç†
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(10),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    # è¼‰å…¥æ•¸æ“šé›†
    print(f"ğŸ“‚ è¼‰å…¥æ•¸æ“šé›†: {data_root}")
    dataset = OptimizedOutfitDataset(data_root, config, transform=transform)
    
    dataloader = DataLoader(
        dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        pin_memory=False,
        drop_last=True
    )
    
    # å‰µå»ºæ¨¡å‹
    print(f"ğŸ”§ å‰µå»ºæ¨¡å‹: {config.backbone_type}")
    model = EnhancedStyleClassifier(config).to(device)
    
    # å‰µå»ºè¨“ç·´å™¨
    trainer = MacOptimizedTrainer(model, config, device, output_dir)
    
    # æ¢å¾©è¨“ç·´ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    start_epoch = 1
    best_loss = float('inf')
    
    if resume_from and os.path.exists(resume_from):
        print(f"ğŸ“¥ æ¢å¾©è¨“ç·´: {resume_from}")
        checkpoint = torch.load(resume_from, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        trainer.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        best_loss = checkpoint['loss']
        trainer.log_message(f"å¾ epoch {start_epoch-1} æ¢å¾©è¨“ç·´")
    
    # è¨“ç·´å¾ªç’°
    trainer.log_message("é–‹å§‹è¨“ç·´å¾ªç’°")
    
    try:
        for epoch in range(start_epoch, config.num_epochs + 1):
            epoch_loss = trainer.train_epoch(dataloader, epoch)
            
            # å­¸ç¿’ç‡èª¿åº¦
            trainer.scheduler.step()
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if epoch_loss < best_loss:
                best_loss = epoch_loss
                trainer.save_checkpoint(
                    epoch, epoch_loss, 
                    f'best_model_{backbone_type}_{config_type}.pth'
                )
            
            # å®šæœŸä¿å­˜æª¢æŸ¥é»
            if epoch % 5 == 0:
                trainer.save_checkpoint(
                    epoch, epoch_loss,
                    f'checkpoint_epoch_{epoch}.pth'
                )
            
            # è¨˜æ†¶é«”æ¸…ç†
            if hasattr(torch.cuda, 'empty_cache'):
                torch.cuda.empty_cache()
            gc.collect()
    
    except KeyboardInterrupt:
        trainer.log_message("è¨“ç·´è¢«ä¸­æ–·")
        trainer.save_checkpoint(
            epoch, epoch_loss,
            f'interrupted_model_{backbone_type}_{config_type}.pth'
        )
    
    # ä¿å­˜æœ€çµ‚æ¨¡å‹
    trainer.save_checkpoint(
        config.num_epochs, best_loss,
        f'final_model_{backbone_type}_{config_type}.pth'
    )
    
    # ä¿å­˜è¨“ç·´æ­·å²
    save_training_history_plot(trainer.train_history, output_dir)
    
    # ä¿å­˜è¨“ç·´æ­·å²JSON
    history_path = os.path.join(output_dir, 'training_history.json')
    with open(history_path, 'w') as f:
        json.dump(trainer.train_history, f, indent=2)
    
    trainer.log_message("è¨“ç·´å®Œæˆï¼")
    print(f"\nâœ… è¨“ç·´å®Œæˆï¼")
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {output_dir}")
    print(f"ğŸ† æœ€ä½³æå¤±: {best_loss:.4f}")

def main():
    parser = argparse.ArgumentParser(description='æ™‚å°šé¢¨æ ¼åˆ†é¡è¨“ç·´ç³»çµ±')
    parser.add_argument('--data', type=str, default='../dataset',
                       help='æ•¸æ“šé›†æ ¹ç›®éŒ„')
    parser.add_argument('--config', type=str, default='balanced',
                       choices=['minimal', 'balanced', 'performance'],
                       help='è¨“ç·´é…ç½®é¡å‹')
    parser.add_argument('--backbone', type=str, default='mobilenet',
                       choices=['mobilenet', 'resnet18', 'resnet50', 'efficientnet_b0', 
                               'efficientnet_b2', 'vit_tiny', 'vit_small', 'fashion_resnet'],
                       help='Backboneæ¶æ§‹')
    parser.add_argument('--resume', type=str, default=None,
                       help='æ¢å¾©è¨“ç·´çš„æª¢æŸ¥é»è·¯å¾‘')
    
    args = parser.parse_args()
    
    train_model(
        data_root=args.data,
        config_type=args.config,
        backbone_type=args.backbone,
        resume_from=args.resume
    )

if __name__ == "__main__":
    main() 