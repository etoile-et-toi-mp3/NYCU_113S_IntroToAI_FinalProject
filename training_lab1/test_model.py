#!/usr/bin/env python3
"""
Mac è¨“ç·´æ¨¡å‹æ¸¬è©¦è…³æœ¬
ç”¨æ–¼æ¸¬è©¦ .pth æ¨¡å‹æ–‡ä»¶å° test.jpg çš„é æ¸¬çµæœ
"""

import torch
import torch.nn.functional as F
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import os
import argparse
import json
from datetime import datetime

# å°å…¥æ¨¡å‹é¡
from test_mac_optimized import LightweightStyleClassifier, setup_mac_optimization

class ModelTester:
    def __init__(self, model_path, device=None):
        """
        åˆå§‹åŒ–æ¨¡å‹æ¸¬è©¦å™¨
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾‘ (.pth)
            device: è¨ˆç®—è¨­å‚™ï¼ŒNoneå‰‡è‡ªå‹•é¸æ“‡
        """
        self.model_path = model_path
        
        # è¨­å‚™è¨­ç½®
        if device is None:
            self.device = setup_mac_optimization()
        else:
            self.device = device
        
        # é¢¨æ ¼é¡åˆ¥
        self.style_categories = [
            'Athleisure', 'BRITISH', 'CASUAL', 'GOTH', 'Kawaii', 
            'Korean', 'MINIMALIST', 'Preppy', 'STREET', 'Streetwear', 
            'Vintage', 'Y2K'
        ]
        
        self.gender_categories = ['MEN', 'WOMEN']
        
        # åœ–ç‰‡é è™•ç†
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # è¼‰å…¥æ¨¡å‹
        self.model = self._load_model()
        
    def _load_model(self):
        """è¼‰å…¥æ¨¡å‹"""
        try:
            print(f"ğŸ“‚ è¼‰å…¥æ¨¡å‹: {self.model_path}")
            
            # åˆå§‹åŒ–æ¨¡å‹
            model = LightweightStyleClassifier(
                num_styles=len(self.style_categories),
                num_genders=len(self.gender_categories),
                feature_dim=1024
            )
            
            # è¼‰å…¥æ¬Šé‡
            if self.model_path.endswith('.pth'):
                state_dict = torch.load(self.model_path, map_location=self.device)
                
                # å¦‚æœæ˜¯æª¢æŸ¥é»æ–‡ä»¶ï¼Œæå–æ¨¡å‹ç‹€æ…‹
                if 'model_state_dict' in state_dict:
                    model.load_state_dict(state_dict['model_state_dict'])
                    print("âœ… å¾æª¢æŸ¥é»è¼‰å…¥æ¨¡å‹")
                    
                    # é¡¯ç¤ºæª¢æŸ¥é»ä¿¡æ¯
                    if 'epoch' in state_dict:
                        print(f"   Epoch: {state_dict['epoch'] + 1}")
                    if 'loss' in state_dict:
                        print(f"   æå¤±: {state_dict['loss']:.4f}")
                else:
                    model.load_state_dict(state_dict)
                    print("âœ… è¼‰å…¥æ¨¡å‹æ¬Šé‡")
            else:
                raise ValueError("ä¸æ”¯æ´çš„æ¨¡å‹æ–‡ä»¶æ ¼å¼")
            
            model.to(self.device)
            model.eval()
            
            # è¨ˆç®—æ¨¡å‹åƒæ•¸
            total_params = sum(p.numel() for p in model.parameters())
            print(f"ğŸ“ˆ æ¨¡å‹åƒæ•¸æ•¸é‡: {total_params:,}")
            
            return model
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
            raise
    
    def preprocess_image(self, image_path):
        """é è™•ç†åœ–ç‰‡"""
        try:
            # è¼‰å…¥åœ–ç‰‡
            image = Image.open(image_path).convert('RGB')
            print(f"ğŸ“· è¼‰å…¥åœ–ç‰‡: {image_path}")
            print(f"   åŸå§‹å°ºå¯¸: {image.size}")
            
            # é è™•ç†
            input_tensor = self.transform(image).unsqueeze(0)
            print(f"   è™•ç†å¾Œå°ºå¯¸: {input_tensor.shape}")
            
            return input_tensor.to(self.device), image
            
        except Exception as e:
            print(f"âŒ åœ–ç‰‡é è™•ç†å¤±æ•—: {e}")
            raise
    
    def predict(self, image_path, top_k=5):
        """
        é æ¸¬åœ–ç‰‡çš„é¢¨æ ¼å’Œæ€§åˆ¥
        
        Args:
            image_path: åœ–ç‰‡è·¯å¾‘
            top_k: è¿”å›å‰kå€‹é æ¸¬çµæœ
            
        Returns:
            dict: é æ¸¬çµæœ
        """
        try:
            # é è™•ç†åœ–ç‰‡
            input_tensor, original_image = self.preprocess_image(image_path)
            
            print(f"\nğŸ”® é–‹å§‹é æ¸¬...")
            print(f"   ä½¿ç”¨è¨­å‚™: {self.device}")
            
            # æ¨¡å‹æ¨è«–
            with torch.no_grad():
                outputs = self.model(input_tensor)
                
                # é¢¨æ ¼é æ¸¬
                style_logits = outputs['style_logits']
                style_probs = F.softmax(style_logits, dim=1)
                style_probs_np = style_probs.cpu().numpy()[0]
                
                # æ€§åˆ¥é æ¸¬
                gender_logits = outputs['gender_logits']
                gender_probs = F.softmax(gender_logits, dim=1)
                gender_probs_np = gender_probs.cpu().numpy()[0]
                
                # ç²å–top-ké¢¨æ ¼é æ¸¬
                style_top_indices = np.argsort(style_probs_np)[::-1][:top_k]
                style_predictions = []
                
                for i, idx in enumerate(style_top_indices):
                    style_predictions.append({
                        'rank': i + 1,
                        'style': self.style_categories[idx],
                        'confidence': float(style_probs_np[idx]),
                        'percentage': float(style_probs_np[idx] * 100)
                    })
                
                # ç²å–æ€§åˆ¥é æ¸¬
                gender_top_indices = np.argsort(gender_probs_np)[::-1]
                gender_predictions = []
                
                for i, idx in enumerate(gender_top_indices):
                    gender_predictions.append({
                        'rank': i + 1,
                        'gender': self.gender_categories[idx],
                        'confidence': float(gender_probs_np[idx]),
                        'percentage': float(gender_probs_np[idx] * 100)
                    })
                
                # çµ„åˆçµæœ
                result = {
                    'image_path': image_path,
                    'image_size': original_image.size,
                    'device': str(self.device),
                    'model_path': self.model_path,
                    'timestamp': datetime.now().isoformat(),
                    'style_predictions': style_predictions,
                    'gender_predictions': gender_predictions,
                    'top_style': style_predictions[0]['style'],
                    'top_style_confidence': style_predictions[0]['confidence'],
                    'top_gender': gender_predictions[0]['gender'],
                    'top_gender_confidence': gender_predictions[0]['confidence']
                }
                
                return result
                
        except Exception as e:
            print(f"âŒ é æ¸¬å¤±æ•—: {e}")
            raise
    
    def print_results(self, result, detailed=True):
        """æ‰“å°é æ¸¬çµæœ"""
        print(f"\n{'='*60}")
        print(f"ğŸ¯ é æ¸¬çµæœ")
        print(f"{'='*60}")
        
        print(f"ğŸ“· åœ–ç‰‡: {result['image_path']}")
        print(f"ğŸ“ å°ºå¯¸: {result['image_size']}")
        print(f"ğŸ”§ è¨­å‚™: {result['device']}")
        print(f"ğŸ“… æ™‚é–“: {result['timestamp']}")
        
        print(f"\nğŸ¨ é¢¨æ ¼é æ¸¬:")
        print(f"   æœ€å¯èƒ½: {result['top_style']} ({result['top_style_confidence']:.1%})")
        
        if detailed:
            print(f"\n   è©³ç´°æ’å:")
            for pred in result['style_predictions']:
                confidence_bar = "â–ˆ" * int(pred['percentage'] / 5)
                print(f"   {pred['rank']}. {pred['style']:<12} {pred['percentage']:5.1f}% {confidence_bar}")
        
        print(f"\nğŸ‘¤ æ€§åˆ¥é æ¸¬:")
        print(f"   æœ€å¯èƒ½: {result['top_gender']} ({result['top_gender_confidence']:.1%})")
        
        if detailed:
            print(f"\n   è©³ç´°æ’å:")
            for pred in result['gender_predictions']:
                confidence_bar = "â–ˆ" * int(pred['percentage'] / 5)
                print(f"   {pred['rank']}. {pred['gender']:<6} {pred['percentage']:5.1f}% {confidence_bar}")
        
        print(f"\n{'='*60}")
    
    def save_results(self, result, output_path):
        """ä¿å­˜çµæœåˆ°JSONæ–‡ä»¶"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜çµæœå¤±æ•—: {e}")

def main():
    parser = argparse.ArgumentParser(description='æ¸¬è©¦è¨“ç·´å¥½çš„æ¨¡å‹')
    parser.add_argument('--model', type=str, required=True,
                       help='æ¨¡å‹æ–‡ä»¶è·¯å¾‘ (.pth)')
    parser.add_argument('--image', type=str, default='test.jpg',
                       help='æ¸¬è©¦åœ–ç‰‡è·¯å¾‘ (default: test.jpg)')
    parser.add_argument('--top-k', type=int, default=5,
                       help='é¡¯ç¤ºå‰kå€‹é æ¸¬çµæœ (default: 5)')
    parser.add_argument('--output', type=str, default=None,
                       help='ä¿å­˜çµæœçš„JSONæ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--simple', action='store_true',
                       help='ç°¡åŒ–è¼¸å‡ºï¼Œåªé¡¯ç¤ºæœ€ä½³é æ¸¬')
    
    args = parser.parse_args()
    
    print("ğŸ Mac æ¨¡å‹æ¸¬è©¦å™¨")
    print("=" * 50)
    
    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.model):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model}")
        return
    
    if not os.path.exists(args.image):
        print(f"âŒ åœ–ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {args.image}")
        return
    
    try:
        # åˆå§‹åŒ–æ¸¬è©¦å™¨
        tester = ModelTester(args.model)
        
        # é€²è¡Œé æ¸¬
        result = tester.predict(args.image, top_k=args.top_k)
        
        # é¡¯ç¤ºçµæœ
        tester.print_results(result, detailed=not args.simple)
        
        # ä¿å­˜çµæœ
        if args.output:
            tester.save_results(result, args.output)
        
        # ç°¡å–®ç¸½çµ
        print(f"\nğŸ“‹ ç¸½çµ:")
        print(f"   é¢¨æ ¼: {result['top_style']} ({result['top_style_confidence']:.1%})")
        print(f"   æ€§åˆ¥: {result['top_gender']} ({result['top_gender_confidence']:.1%})")
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

def quick_test(model_path, image_path="test.jpg"):
    """å¿«é€Ÿæ¸¬è©¦å‡½æ•¸"""
    try:
        tester = ModelTester(model_path)
        result = tester.predict(image_path)
        
        print(f"ğŸ¯ å¿«é€Ÿæ¸¬è©¦çµæœ:")
        print(f"   åœ–ç‰‡: {image_path}")
        print(f"   é¢¨æ ¼: {result['top_style']} ({result['top_style_confidence']:.1%})")
        print(f"   æ€§åˆ¥: {result['top_gender']} ({result['top_gender_confidence']:.1%})")
        
        return result
    except Exception as e:
        print(f"âŒ å¿«é€Ÿæ¸¬è©¦å¤±æ•—: {e}")
        return None

if __name__ == "__main__":
    main() 